
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch张量 · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="78.html" />
    
    
    <link rel="prev" href="76.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    PyTorch 1.4 教程&文档
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    入门
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="4.html">
            
                <a href="4.html">
            
                    
                    使用 PyTorch 进行深度学习：60 分钟的闪电战
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="blitz/tensor_tutorial.html">
            
                <a href="blitz/tensor_tutorial.html">
            
                    
                    什么是PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="blitz/autograd_tutorial.html">
            
                <a href="blitz/autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="blitz/neural_networks_tutorial.html">
            
                <a href="blitz/neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="blitz/cifar10_tutorial.html">
            
                <a href="blitz/cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="blitz/data_parallel_tutorial.html">
            
                <a href="blitz/data_parallel_tutorial.html">
            
                    
                    可选：数据并行
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="5.html">
            
                <a href="5.html">
            
                    
                    编写自定义数据集，数据加载器和转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="6.html">
            
                <a href="6.html">
            
                    
                    使用 TensorBoard 可视化模型，数据和训练
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    图片
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="8.html">
            
                <a href="8.html">
            
                    
                    TorchVision 对象检测微调教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="9.html">
            
                <a href="9.html">
            
                    
                    转移学习的计算机视觉教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="10.html">
            
                <a href="10.html">
            
                    
                    空间变压器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="11.html">
            
                <a href="11.html">
            
                    
                    使用 PyTorch 进行神经传递
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="12.html">
            
                <a href="12.html">
            
                    
                    对抗示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="13.html">
            
                <a href="13.html">
            
                    
                    DCGAN 教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    音频
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="15.html">
            
                <a href="15.html">
            
                    
                    torchaudio 教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" >
            
                <span>
            
                    
                    文本
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="17.html">
            
                <a href="17.html">
            
                    
                    NLP From Scratch: 使用char-RNN对姓氏进行分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="18.html">
            
                <a href="18.html">
            
                    
                    NLP From Scratch: 生成名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="19.html">
            
                <a href="19.html">
            
                    
                    NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="20.html">
            
                <a href="20.html">
            
                    
                    使用 TorchText 进行文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="21.html">
            
                <a href="21.html">
            
                    
                    使用 TorchText 进行语言翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="22.html">
            
                <a href="22.html">
            
                    
                    使用 nn.Transformer 和 TorchText 进行序列到序列建模
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" >
            
                <span>
            
                    
                    命名为 Tensor(实验性）
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="24.html">
            
                <a href="24.html">
            
                    
                    (实验性）PyTorch 中的命名张量简介
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" >
            
                <span>
            
                    
                    强化学习
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="26.html">
            
                <a href="26.html">
            
                    
                    强化学习(DQN）教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" >
            
                <span>
            
                    
                    在生产中部署 PyTorch 模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="28.html">
            
                <a href="28.html">
            
                    
                    通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="29.html">
            
                <a href="29.html">
            
                    
                    TorchScript 简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="30.html">
            
                <a href="30.html">
            
                    
                    在 C ++中加载 TorchScript 模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.4" data-path="31.html">
            
                <a href="31.html">
            
                    
                    (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" >
            
                <span>
            
                    
                    并行和分布式训练
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="33.html">
            
                <a href="33.html">
            
                    
                    单机模型并行最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="34.html">
            
                <a href="34.html">
            
                    
                    分布式数据并行入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="35.html">
            
                <a href="35.html">
            
                    
                    用 PyTorch 编写分布式应用程序
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.4" data-path="36.html">
            
                <a href="36.html">
            
                    
                    分布式 RPC 框架入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.5" data-path="37.html">
            
                <a href="37.html">
            
                    
                    (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.10" >
            
                <span>
            
                    
                    扩展 PyTorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="39.html">
            
                <a href="39.html">
            
                    
                    使用自定义 C ++运算符扩展 TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.2" data-path="40.html">
            
                <a href="40.html">
            
                    
                    使用自定义 C ++类扩展 TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.3" data-path="41.html">
            
                <a href="41.html">
            
                    
                    使用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.4" data-path="42.html">
            
                <a href="42.html">
            
                    
                    自定义 C ++和 CUDA 扩展
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" >
            
                <span>
            
                    
                    模型优化
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.11.1" data-path="44.html">
            
                <a href="44.html">
            
                    
                    LSTM Word 语言模型上的(实验）动态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.2" data-path="45.html">
            
                <a href="45.html">
            
                    
                    (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.3" data-path="46.html">
            
                <a href="46.html">
            
                    
                    (实验性）计算机视觉教程的量化转移学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.4" data-path="47.html">
            
                <a href="47.html">
            
                    
                    (实验）BERT 上的动态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.5" data-path="48.html">
            
                <a href="48.html">
            
                    
                    修剪教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.12" >
            
                <span>
            
                    
                    PyTorch 用其他语言
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.12.1" data-path="50.html">
            
                <a href="50.html">
            
                    
                    使用 PyTorch C ++前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.13" >
            
                <span>
            
                    
                    PyTorch 基础知识
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.13.1" data-path="52.html">
            
                <a href="52.html">
            
                    
                    通过示例学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13.2" data-path="53.html">
            
                <a href="53.html">
            
                    
                    torch.nn 到底是什么？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.14" >
            
                <span>
            
                    
                    文件
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" >
            
                <span>
            
                    
                    笔记
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="56.html">
            
                <a href="56.html">
            
                    
                    自动毕业力学
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="57.html">
            
                <a href="57.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.3" data-path="58.html">
            
                <a href="58.html">
            
                    
                    CPU 线程和 TorchScript 推断
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4" data-path="59.html">
            
                <a href="59.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5" data-path="60.html">
            
                <a href="60.html">
            
                    
                    分布式 Autograd 设计
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.6" data-path="61.html">
            
                <a href="61.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.7" data-path="62.html">
            
                <a href="62.html">
            
                    
                    经常问的问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.8" data-path="63.html">
            
                <a href="63.html">
            
                    
                    大规模部署的功能
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.9" data-path="64.html">
            
                <a href="64.html">
            
                    
                    并行处理最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.10" data-path="65.html">
            
                <a href="65.html">
            
                    
                    重现性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.11" data-path="66.html">
            
                <a href="66.html">
            
                    
                    远程参考协议
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.12" data-path="67.html">
            
                <a href="67.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.13" data-path="68.html">
            
                <a href="68.html">
            
                    
                    Windows 常见问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.14" data-path="69.html">
            
                <a href="69.html">
            
                    
                    XLA 设备上的 PyTorch
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" >
            
                <span>
            
                    
                    语言绑定
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.16.1" data-path="71.html">
            
                <a href="71.html">
            
                    
                    PyTorch C ++ API
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.2" data-path="72.html">
            
                <a href="72.html">
            
                    
                    PyTorch Java API
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.17" >
            
                <span>
            
                    
                    Python API
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.17.1" data-path="74.html">
            
                <a href="74.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.2" data-path="75.html">
            
                <a href="75.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.3" data-path="76.html">
            
                <a href="76.html">
            
                    
                    torch功能
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.17.4" data-path="77.html">
            
                <a href="77.html">
            
                    
                    torch张量
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.5" data-path="78.html">
            
                <a href="78.html">
            
                    
                    张量属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.6" data-path="79.html">
            
                <a href="79.html">
            
                    
                    自动差分包-Torch.Autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.7" data-path="80.html">
            
                <a href="80.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.8" data-path="81.html">
            
                <a href="81.html">
            
                    
                    分布式通讯包-Torch.Distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.9" data-path="82.html">
            
                <a href="82.html">
            
                    
                    概率分布-torch分布
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.10" data-path="83.html">
            
                <a href="83.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.11" data-path="84.html">
            
                <a href="84.html">
            
                    
                    torch脚本
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.12" data-path="85.html">
            
                <a href="85.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.13" data-path="86.html">
            
                <a href="86.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.14" data-path="87.html">
            
                <a href="87.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.15" data-path="88.html">
            
                <a href="88.html">
            
                    
                    量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.16" data-path="89.html">
            
                <a href="89.html">
            
                    
                    分布式 RPC 框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.17" data-path="90.html">
            
                <a href="90.html">
            
                    
                    torch随机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.18" data-path="91.html">
            
                <a href="91.html">
            
                    
                    torch稀疏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.19" data-path="92.html">
            
                <a href="92.html">
            
                    
                    torch存储
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.20" data-path="93.html">
            
                <a href="93.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.21" data-path="94.html">
            
                <a href="94.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.22" data-path="95.html">
            
                <a href="95.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.23" data-path="96.html">
            
                <a href="96.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.24" data-path="97.html">
            
                <a href="97.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.25" data-path="98.html">
            
                <a href="98.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.26" data-path="99.html">
            
                <a href="99.html">
            
                    
                    torch.utils.tensorboard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.27" data-path="100.html">
            
                <a href="100.html">
            
                    
                    类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.28" data-path="101.html">
            
                <a href="101.html">
            
                    
                    命名张量
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.29" data-path="102.html">
            
                <a href="102.html">
            
                    
                    命名为 Tensors 操作员范围
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.30" data-path="103.html">
            
                <a href="103.html">
            
                    
                    糟糕！
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.18" >
            
                <span>
            
                    
                    torchvision参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.18.1" data-path="105.html">
            
                <a href="105.html">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.19" >
            
                <span>
            
                    
                    音频参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.19.1" data-path="107.html">
            
                <a href="107.html">
            
                    
                    torchaudio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.20" >
            
                <span>
            
                    
                    torchtext参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.20.1" data-path="109.html">
            
                <a href="109.html">
            
                    
                    torchtext
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.21" >
            
                <span>
            
                    
                    社区
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.21.1" data-path="111.html">
            
                <a href="111.html">
            
                    
                    PyTorch 贡献指南
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.2" data-path="112.html">
            
                <a href="112.html">
            
                    
                    PyTorch 治理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.3" data-path="113.html">
            
                <a href="113.html">
            
                    
                    PyTorch 治理| 感兴趣的人
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch张量</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torch&#x5F20;&#x91CF;">torch&#x5F20;&#x91CF;</h1>
<blockquote>
<p>&#x539F;&#x6587;&#xFF1A; <a href="https://pytorch.org/docs/stable/tensors.html" target="_blank">https://pytorch.org/docs/stable/tensors.html</a></p>
</blockquote>
<p><a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> &#x662F;&#x5305;&#x542B;&#x5355;&#x4E2A;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x5143;&#x7D20;&#x7684;&#x591A;&#x7EF4;&#x77E9;&#x9635;&#x3002;</p>
<p>Torch &#x5B9A;&#x4E49;&#x4E86; 9 &#x79CD; CPU &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x548C; 9 &#x79CD; GPU &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#xFF1A;</p>
<table>
<thead>
<tr>
<th>&#x6570;&#x636E;&#x7C7B;&#x578B;</th>
<th>dtype</th>
<th>CPU &#x5F20;&#x91CF;</th>
<th>GPU &#x5F20;&#x91CF;</th>
</tr>
</thead>
<tbody>
<tr>
<td>32 &#x4F4D;&#x6D6E;&#x70B9;</td>
<td><code>torch.float32</code>&#x6216;<code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64 &#x4F4D;&#x6D6E;&#x70B9;</td>
<td><code>torch.float64</code>&#x6216;<code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16 &#x4F4D;&#x6D6E;&#x70B9;</td>
<td><code>torch.float16</code>&#x6216;<code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>8 &#x4F4D;&#x6574;&#x6570;(&#x65E0;&#x7B26;&#x53F7;&#xFF09;</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8 &#x4F4D;&#x6574;&#x6570;(&#x6709;&#x7B26;&#x53F7;&#xFF09;</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16 &#x4F4D;&#x6574;&#x6570;(&#x6709;&#x7B26;&#x53F7;&#xFF09;</td>
<td><code>torch.int16</code>&#x6216;<code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32 &#x4F4D;&#x6574;&#x6570;(&#x6709;&#x7B26;&#x53F7;&#xFF09;</td>
<td><code>torch.int32</code>&#x6216;<code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64 &#x4F4D;&#x6574;&#x6570;(&#x6709;&#x7B26;&#x53F7;&#xFF09;</td>
<td><code>torch.int64</code>&#x6216;<code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>&#x5E03;&#x5C14;&#x578B;</td>
<td><code>torch.bool</code></td>
<td><a href="#torch.BoolTensor" title="torch.BoolTensor"><code>torch.BoolTensor</code></a></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
</tbody>
</table>
<p><a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> &#x662F;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(<code>torch.FloatTensor</code>&#xFF09;&#x7684;&#x522B;&#x540D;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x4F7F;&#x7528; <a href="torch.html#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x6784;&#x9020;&#x51FD;&#x6570;&#x4ECE; Python <code>list</code>&#x6216;&#x5E8F;&#x5217;&#x6784;&#x9020;&#x5F20;&#x91CF;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
&gt;&gt;&gt; torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
</code></pre><p>&#x8B66;&#x544A;</p>
<p><a href="torch.html#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x59CB;&#x7EC8;&#x590D;&#x5236;<code>data</code>&#x3002; &#x5982;&#x679C;&#x60A8;&#x5177;&#x6709;&#x5F20;&#x91CF;<code>data</code>&#xFF0C;&#x800C;&#x53EA;&#x60F3;&#x66F4;&#x6539;&#x5176;<code>requires_grad</code>&#x6807;&#x5FD7;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>requires_grad_()</code></a> &#x6216; <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>detach()</code></a> &#x4EE5;&#x907F;&#x514D;&#x590D;&#x5236;&#x3002; &#x5982;&#x679C;&#x60A8;&#x6709;&#x4E00;&#x4E2A; numpy &#x6570;&#x7EC4;&#x5E76;&#x4E14;&#x60F3;&#x8981;&#x907F;&#x514D;&#x590D;&#x5236;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="torch.html#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a> &#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x5C06; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C;/&#x6216; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x4F20;&#x9012;&#x7ED9;&#x6784;&#x9020;&#x51FD;&#x6570;&#x6216;&#x5F20;&#x91CF;&#x521B;&#x5EFA;&#x64CD;&#x4F5C;&#x6765;&#x6784;&#x9020;&#x7279;&#x5B9A;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
&gt;&gt;&gt; cuda0 = torch.device(&apos;cuda:0&apos;)
&gt;&gt;&gt; torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&apos;cuda:0&apos;)
</code></pre><p>&#x5F20;&#x91CF;&#x7684;&#x5185;&#x5BB9;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; Python &#x7684;&#x7D22;&#x5F15;&#x548C;&#x5207;&#x7247;&#x7B26;&#x53F7;&#x6765;&#x8BBF;&#x95EE;&#x548C;&#x4FEE;&#x6539;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])
&gt;&gt;&gt; print(x[1][2])
tensor(6)
&gt;&gt;&gt; x[0][1] = 8
&gt;&gt;&gt; print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])
</code></pre><p>&#x4F7F;&#x7528; <a href="#torch.Tensor.item" title="torch.Tensor.item"><code>torch.Tensor.item()</code></a> &#x4ECE;&#x5F20;&#x91CF;&#x4E2D;&#x83B7;&#x53D6;&#x5305;&#x542B;&#x5355;&#x4E2A;&#x503C;&#x7684; Python &#x6570;&#x5B57;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1]])
&gt;&gt;&gt; x
tensor([[ 1]])
&gt;&gt;&gt; x.item()
1
&gt;&gt;&gt; x = torch.tensor(2.5)
&gt;&gt;&gt; x
tensor(2.5000)
&gt;&gt;&gt; x.item()
2.5
</code></pre><p>&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>requires_grad=True</code>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x4FBF; <a href="autograd.html#module-torch.autograd" title="torch.autograd"><code>torch.autograd</code></a> &#x5BF9;&#x5176;&#x8FDB;&#x884C;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x4EE5;&#x8FDB;&#x884C;&#x81EA;&#x52A8;&#x5FAE;&#x5206;&#x3002;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
&gt;&gt;&gt; out = x.pow(2).sum()
&gt;&gt;&gt; out.backward()
&gt;&gt;&gt; x.grad
tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])
</code></pre><p>&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x90FD;&#x6709;&#x4E00;&#x4E2A;&#x5173;&#x8054;&#x7684;<code>torch.Storage</code>&#xFF0C;&#x5B83;&#x4FDD;&#x5B58;&#x5176;&#x6570;&#x636E;&#x3002; &#x5F20;&#x91CF;&#x7C7B;&#x63D0;&#x4F9B;&#x4E86;&#x5B58;&#x50A8;&#x7684;&#x591A;&#x7EF4;<a href="https://en.wikipedia.org/wiki/Stride_of_an_array" target="_blank">&#x8DE8;&#x5EA6;</a>&#x89C6;&#x56FE;&#xFF0C;&#x5E76;&#x5B9A;&#x4E49;&#x4E86;&#x6570;&#x5B57;&#x8FD0;&#x7B97;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x6709;&#x5173; <a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> &#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> &#x5C5E;&#x6027;&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x9605;[ <a href="tensor_attributes.html#tensor-attributes-doc">Tensor Attributes</a> &#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x6539;&#x53D8;&#x5F20;&#x91CF;&#x7684;&#x65B9;&#x6CD5;&#x7528;&#x4E0B;&#x5212;&#x7EBF;&#x540E;&#x7F00;&#x6807;&#x8BB0;&#x3002; &#x4F8B;&#x5982;&#xFF0C;<code>torch.FloatTensor.abs_()</code>&#x5728;&#x539F;&#x4F4D;&#x8BA1;&#x7B97;&#x7EDD;&#x5BF9;&#x503C;&#x5E76;&#x8FD4;&#x56DE;&#x4FEE;&#x6539;&#x540E;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x800C;<code>torch.FloatTensor.abs()</code>&#x5728;&#x65B0;&#x5F20;&#x91CF;&#x4E2D;&#x8BA1;&#x7B97;&#x7ED3;&#x679C;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x8981;&#x66F4;&#x6539;&#x73B0;&#x6709;&#x5F20;&#x91CF;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x548C;/&#x6216; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x8BF7;&#x8003;&#x8651;&#x5728;&#x5F20;&#x91CF;&#x4E0A;&#x4F7F;&#x7528; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x65B9;&#x6CD5;&#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p><a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> &#x7684;&#x5F53;&#x524D;&#x5B9E;&#x73B0;&#x5F15;&#x5165;&#x4E86;&#x5185;&#x5B58;&#x5F00;&#x9500;&#xFF0C;&#x56E0;&#x6B64;&#xFF0C;&#x5728;&#x5177;&#x6709;&#x8BB8;&#x591A;&#x5FAE;&#x5C0F;&#x5F20;&#x91CF;&#x7684;&#x5E94;&#x7528;&#x7A0B;&#x5E8F;&#x4E2D;&#xFF0C;&#x5B83;&#x53EF;&#x80FD;&#x5BFC;&#x81F4;&#x610F;&#x5916;&#x7684;&#x9AD8;&#x5185;&#x5B58;&#x4F7F;&#x7528;&#x7387;&#x3002; &#x5982;&#x679C;&#x662F;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF0C;&#x8BF7;&#x8003;&#x8651;&#x4F7F;&#x7528;&#x4E00;&#x79CD;&#x5927;&#x578B;&#x7ED3;&#x6784;&#x3002;</p>
<hr>
<pre><code>class torch.Tensor
</code></pre><p>&#x6839;&#x636E;&#x60A8;&#x7684;&#x7528;&#x4F8B;&#xFF0C;&#x521B;&#x5EFA;&#x5F20;&#x91CF;&#x7684;&#x4E3B;&#x8981;&#x65B9;&#x6CD5;&#x6709;&#x51E0;&#x79CD;&#x3002;</p>
<ul>
<li><p>&#x8981;&#x4F7F;&#x7528;&#x73B0;&#x6709;&#x6570;&#x636E;&#x521B;&#x5EFA;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="torch.html#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x5177;&#x6709;&#x7279;&#x5B9A;&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528;<code>torch.*</code>&#x5F20;&#x91CF;&#x521B;&#x5EFA;&#x64CD;&#x4F5C;(&#x8BF7;&#x53C2;&#x89C1; <a href="torch.html#tensor-creation-ops">Creation Ops</a>)&#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x4E0E;&#x53E6;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x5927;&#x5C0F;(&#x548C;&#x76F8;&#x4F3C;&#x7C7B;&#x578B;&#xFF09;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528;<code>torch.*_like</code>&#x5F20;&#x91CF;&#x521B;&#x5EFA;&#x64CD;&#x4F5C;(&#x8BF7;&#x53C2;&#x89C1;<a href="torch.html#tensor-creation-ops">&#x521B;&#x5EFA;&#x64CD;&#x4F5C;</a>&#xFF09;&#x3002;</p>
</li>
<li><p>&#x8981;&#x521B;&#x5EFA;&#x4E0E;&#x5176;&#x4ED6;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x4F3C;&#x7C7B;&#x578B;&#x4F46;&#x5927;&#x5C0F;&#x4E0D;&#x540C;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528;<code>tensor.new_*</code>&#x521B;&#x5EFA;&#x64CD;&#x4F5C;&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>new_tensor(data, dtype=None, device=None, requires_grad=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4EE5;<code>data</code>&#x4F5C;&#x4E3A;&#x5F20;&#x91CF;&#x6570;&#x636E;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p><a href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code>new_tensor()</code></a> &#x59CB;&#x7EC8;&#x590D;&#x5236;<code>data</code>&#x3002; &#x5982;&#x679C;&#x60A8;&#x6709;&#x5F20;&#x91CF;<code>data</code>&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x590D;&#x5236;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> &#x6216; <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a> &#x3002; &#x5982;&#x679C;&#x60A8;&#x6709;&#x4E00;&#x4E2A; numpy &#x6570;&#x7EC4;&#x5E76;&#x4E14;&#x60F3;&#x8981;&#x907F;&#x514D;&#x590D;&#x5236;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="torch.html#torch.from_numpy" title="torch.from_numpy"><code>torch.from_numpy()</code></a> &#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p>&#x5F53;&#x6570;&#x636E;&#x662F;&#x5F20;&#x91CF; <em>x</em> &#x65F6;&#xFF0C; <a href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code>new_tensor()</code></a> &#x4ECE;&#x4F20;&#x9012;&#x7684;&#x4EFB;&#x4F55;&#x6570;&#x636E;&#x4E2D;&#x8BFB;&#x51FA;&#x201C;&#x6570;&#x636E;&#x201D;&#xFF0C;&#x5E76;&#x6784;&#x9020;&#x4E00;&#x4E2A;&#x53F6;&#x5B50;&#x53D8;&#x91CF;&#x3002; &#x56E0;&#x6B64;&#xFF0C;<code>tensor.new_tensor(x)</code>&#x7B49;&#x540C;&#x4E8E;<code>x.clone().detach()</code>&#xFF0C;<code>tensor.new_tensor(x, requires_grad=True)</code>&#x7B49;&#x540C;&#x4E8E;<code>x.clone().detach().requires_grad_(True)</code>&#x3002; &#x5EFA;&#x8BAE;&#x4F7F;&#x7528;<code>clone()</code>&#x548C;<code>detach()</code>&#x7684;&#x7B49;&#x6548;&#x9879;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>data</strong> (<em>array_like</em> )&#x2013;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x526F;&#x672C;<code>data</code>&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
</li>
<li><p><strong>require_grad</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x9700;&#x8981;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8)
&gt;&gt;&gt; data = [[0, 1], [2, 3]]
&gt;&gt;&gt; tensor.new_tensor(data)
tensor([[ 0,  1],
        [ 2,  3]], dtype=torch.int8)
</code></pre><hr>
<pre><code>new_full(size, fill_value, dtype=None, device=None, requires_grad=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x7528;<code>fill_value</code>&#x586B;&#x5145;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>fill_value</strong> (<em>&#x6807;&#x91CF;</em>&#xFF09;&#x2013;&#x7528;&#x6765;&#x586B;&#x5145;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x5B57;&#x3002;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x9700;&#x8981;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64)
&gt;&gt;&gt; tensor.new_full((3, 4), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)
</code></pre><hr>
<pre><code>new_empty(size, dtype=None, device=None, requires_grad=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x586B;&#x5145;&#x4E86;&#x672A;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; </p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x9700;&#x8981;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.ones(())
&gt;&gt;&gt; tensor.new_empty((2, 3))
tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],
        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])
</code></pre><hr>
<pre><code>new_ones(size, dtype=None, device=None, requires_grad=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x7528;<code>1</code>&#x586B;&#x5145;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> (<em>python&#xFF1A;int ...</em> )&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#x6216;<code>torch.Size</code>&#x3002;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x662F; None, &#x548C; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x4E00;&#x6837;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x5728;&#x7684;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;: &#x5982;&#x679C;&#x662F; None, &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x4E00;&#x6837;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x9700;&#x8981;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32)
&gt;&gt;&gt; tensor.new_ones((2, 3))
tensor([[ 1,  1,  1],
        [ 1,  1,  1]], dtype=torch.int32)
</code></pre><hr>
<pre><code>new_zeros(size, dtype=None, device=None, requires_grad=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x7528;<code>0</code>&#x586B;&#x5145;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>size</strong> (<em>python:int...</em>) &#x2013; a list, tuple, or <code>torch.Size</code> of integers defining the shape of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; &#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x9700;&#x8981;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64)
&gt;&gt;&gt; tensor.new_zeros((2, 3))
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]], dtype=torch.float64)
</code></pre><pre><code>is_cuda
</code></pre><p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x5B58;&#x50A8;&#x5728; GPU &#x4E0A;&#xFF0C;&#x5219;&#x4E3A;<code>True</code>&#xFF0C;&#x5426;&#x5219;&#x4E3A;<code>False</code>&#x3002;</p>
<pre><code>device
</code></pre><p>&#x5F20;&#x91CF;&#x6240;&#x5728;&#x7684; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x3002;</p>
<pre><code>grad
</code></pre><p>&#x6B64;&#x5C5E;&#x6027;&#x9ED8;&#x8BA4;&#x4E3A;<code>None</code>&#xFF0C;&#x5E76;&#x5728;&#x9996;&#x6B21;&#x8C03;&#x7528; <a href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code>backward()</code></a> &#x8BA1;&#x7B97;<code>self</code>&#x7684;&#x68AF;&#x5EA6;&#x65F6;&#x6210;&#x4E3A;&#x5F20;&#x91CF;&#x3002; &#x7136;&#x540E;&#xFF0C;&#x8BE5;&#x5C5E;&#x6027;&#x5C06;&#x5305;&#x542B;&#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x68AF;&#x5EA6;&#x4EE5;&#x53CA; <a href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code>backward()</code></a>&#x8FD4;&#x56DE;&#x7684;&#x68AF;&#x5EA6;&#x503C;&#xFF0C;&#x7136;&#x540E;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x7D2F;&#x52A0;&#x3002;</p>
<pre><code>ndim
</code></pre><p><a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim()</code></a> &#x7684;&#x522B;&#x540D;</p>
<pre><code>T
</code></pre><p>&#x8FD9;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x662F;&#x5426;&#x98A0;&#x5012;&#x4E86;&#x5417;&#xFF1F;</p>
<p>&#x5982;&#x679C;<code>n</code>&#x662F;<code>x</code>&#x4E2D;&#x7684;&#x5C3A;&#x5BF8;&#x6570;&#xFF0C;&#x5219;<code>x.T</code>&#x7B49;&#x6548;&#x4E8E;<code>x.permute(n-1, n-2, ..., 0)</code>&#x3002;</p>
<hr>
<pre><code>abs() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.abs" title="torch.abs"><code>torch.abs()</code></a></p>
<hr>
<pre><code>abs_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.abs" title="torch.Tensor.abs"><code>abs()</code></a></p>
<hr>
<pre><code>acos() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.acos" title="torch.acos"><code>torch.acos()</code></a></p>
<hr>
<pre><code>acos_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.acos" title="torch.Tensor.acos"><code>acos()</code></a></p>
<hr>
<pre><code>add(value) &#x2192; Tensor
</code></pre><p>add(value = 1&#xFF0C;other&#xFF09;-&gt;&#x5F20;&#x91CF;</p>
<p>&#x53C2;&#x89C1; <a href="torch.html#torch.add" title="torch.add"><code>torch.add()</code></a></p>
<hr>
<pre><code>add_(value) &#x2192; Tensor
</code></pre><p>add_(value = 1&#xFF0C;other&#xFF09;-&gt;&#x5F20;&#x91CF;</p>
<p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.add" title="torch.Tensor.add"><code>add()</code></a></p>
<hr>
<pre><code>addbmm(beta=1, alpha=1, batch1, batch2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addbmm" title="torch.addbmm"><code>torch.addbmm()</code></a></p>
<hr>
<pre><code>addbmm_(beta=1, alpha=1, batch1, batch2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code>addbmm()</code></a></p>
<hr>
<pre><code>addcdiv(value=1, tensor1, tensor2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addcdiv" title="torch.addcdiv"><code>torch.addcdiv()</code></a></p>
<hr>
<pre><code>addcdiv_(value=1, tensor1, tensor2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code>addcdiv()</code></a></p>
<hr>
<pre><code>addcmul(value=1, tensor1, tensor2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addcmul" title="torch.addcmul"><code>torch.addcmul()</code></a></p>
<hr>
<pre><code>addcmul_(value=1, tensor1, tensor2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code>addcmul()</code></a></p>
<hr>
<pre><code>addmm(beta=1, alpha=1, mat1, mat2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addmm" title="torch.addmm"><code>torch.addmm()</code></a></p>
<hr>
<pre><code>addmm_(beta=1, alpha=1, mat1, mat2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code>addmm()</code></a></p>
<hr>
<pre><code>addmv(beta=1, alpha=1, mat, vec) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addmv" title="torch.addmv"><code>torch.addmv()</code></a></p>
<hr>
<pre><code>addmv_(beta=1, alpha=1, mat, vec) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code>addmv()</code></a></p>
<hr>
<pre><code>addr(beta=1, alpha=1, vec1, vec2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.addr" title="torch.addr"><code>torch.addr()</code></a></p>
<hr>
<pre><code>addr_(beta=1, alpha=1, vec1, vec2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.addr" title="torch.Tensor.addr"><code>addr()</code></a></p>
<hr>
<pre><code>allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.allclose" title="torch.allclose"><code>torch.allclose()</code></a></p>
<hr>
<pre><code>angle() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.angle" title="torch.angle"><code>torch.angle()</code></a></p>
<hr>
<pre><code>apply_(callable) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x51FD;&#x6570;<code>callable</code>&#x5E94;&#x7528;&#x4E8E;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#xFF0C;&#x5E76;&#x7528;<code>callable</code>&#x8FD4;&#x56DE;&#x7684;&#x503C;&#x66FF;&#x6362;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x4EC5;&#x9002;&#x7528;&#x4E8E; CPU &#x5F20;&#x91CF;&#xFF0C;&#x4E0D;&#x5E94;&#x5728;&#x9700;&#x8981;&#x9AD8;&#x6027;&#x80FD;&#x7684;&#x4EE3;&#x7801;&#x6BB5;&#x4E2D;&#x4F7F;&#x7528;&#x3002;</p>
<hr>
<pre><code>argmax(dim=None, keepdim=False) &#x2192; LongTensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.argmax" title="torch.argmax"><code>torch.argmax()</code></a></p>
<hr>
<pre><code>argmin(dim=None, keepdim=False) &#x2192; LongTensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.argmin" title="torch.argmin"><code>torch.argmin()</code></a></p>
<hr>
<pre><code>argsort(dim=-1, descending=False) &#x2192; LongTensor
</code></pre><p>&#x53C2;&#x89C1;&#xFF1A;func&#xFF1A; &lt;cite&gt;torch.argsort&lt;/cite&gt;</p>
<hr>
<pre><code>asin() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.asin" title="torch.asin"><code>torch.asin()</code></a></p>
<hr>
<pre><code>asin_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.asin" title="torch.Tensor.asin"><code>asin()</code></a></p>
<hr>
<pre><code>as_strided(size, stride, storage_offset=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.as_strided" title="torch.as_strided"><code>torch.as_strided()</code></a></p>
<hr>
<pre><code>atan() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.atan" title="torch.atan"><code>torch.atan()</code></a></p>
<hr>
<pre><code>atan2(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.atan2" title="torch.atan2"><code>torch.atan2()</code></a></p>
<hr>
<pre><code>atan2_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code>atan2()</code></a></p>
<hr>
<pre><code>atan_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.atan" title="torch.Tensor.atan"><code>atan()</code></a></p>
<hr>
<pre><code>backward(gradient=None, retain_graph=None, create_graph=False)
</code></pre><p>&#x8BA1;&#x7B97;&#x5F53;&#x524D;&#x5F20;&#x91CF;&#x7684;&#x68AF;&#x5EA6; w.r.t. &#x56FE;&#x53F6;&#x3002;</p>
<p>&#x8BE5;&#x56FE;&#x4F7F;&#x7528;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x8FDB;&#x884C;&#x533A;&#x5206;&#x3002; &#x5982;&#x679C;&#x5F20;&#x91CF;&#x662F;&#x975E;&#x6807;&#x91CF;&#x7684;(&#x5373;&#x5176;&#x6570;&#x636E;&#x5177;&#x6709;&#x591A;&#x4E2A;&#x5143;&#x7D20;&#xFF09;&#x5E76;&#x4E14;&#x9700;&#x8981;&#x68AF;&#x5EA6;&#xFF0C;&#x5219;&#x8BE5;&#x51FD;&#x6570;&#x8FD8;&#x9700;&#x8981;&#x6307;&#x5B9A;<code>gradient</code>&#x3002; &#x5B83;&#x5E94;&#x8BE5;&#x662F;&#x5339;&#x914D;&#x7C7B;&#x578B;&#x548C;&#x4F4D;&#x7F6E;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x5305;&#x542B;&#x5FAE;&#x5206;&#x51FD;&#x6570; w.r.t &#x7684;&#x68AF;&#x5EA6;&#x3002; <code>self</code>&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x662F;&#x53F6;&#x5B50;&#x68AF;&#x5EA6;&#x7D2F;&#x52A0;-&#x8C03;&#x7528;&#x5B83;&#x4E4B;&#x524D;&#x53EF;&#x80FD;&#x9700;&#x8981;&#x5C06;&#x5B83;&#x4EEC;&#x5F52;&#x96F6;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>gradient</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>Tensor</em> <em>None</em>&#xFF09;&#x2013; &#x68AF;&#x5EA6; w.r.t. &#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;&#x5B83;&#x662F;&#x5F20;&#x91CF;&#xFF0C;&#x9664;&#x975E;<code>create_graph</code>&#x4E3A; True&#xFF0C;&#x5426;&#x5219;&#x5B83;&#x5C06;&#x81EA;&#x52A8;&#x8F6C;&#x6362;&#x4E3A;&#x4E0D;&#x9700;&#x8981; grad &#x7684;&#x5F20;&#x91CF;&#x3002; &#x65E0;&#x6CD5;&#x4E3A;&#x6807;&#x91CF;&#x5F20;&#x91CF;&#x6216;&#x4E0D;&#x9700;&#x8981;&#x7B49;&#x7EA7;&#x7684;&#x5F20;&#x91CF;&#x6307;&#x5B9A;&#x4EFB;&#x4F55;&#x503C;&#x3002; &#x5982;&#x679C; None &#x503C;&#x53EF;&#x4EE5;&#x63A5;&#x53D7;&#xFF0C;&#x90A3;&#x4E48;&#x6B64;&#x53C2;&#x6570;&#x662F;&#x53EF;&#x9009;&#x7684;&#x3002;</p>
</li>
<li><p><strong>retian_graph</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x5982;&#x679C;<code>False</code>&#xFF0C;&#x5219;&#x7528;&#x4E8E;&#x8BA1;&#x7B97;&#x7B49;&#x7EA7;&#x7684;&#x56FE;&#x5F62;&#x5C06;&#x88AB;&#x91CA;&#x653E;&#x3002; &#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x51E0;&#x4E4E;&#x5728;&#x6240;&#x6709;&#x60C5;&#x51B5;&#x4E0B;&#x90FD;&#x4E0D;&#x9700;&#x8981;&#x5C06;&#x6B64;&#x9009;&#x9879;&#x8BBE;&#x7F6E;&#x4E3A; True&#xFF0C;&#x5E76;&#x4E14;&#x901A;&#x5E38;&#x53EF;&#x4EE5;&#x4EE5;&#x66F4;&#x6709;&#x6548;&#x7684;&#x65B9;&#x5F0F;&#x89E3;&#x51B3;&#x5B83;&#x3002; &#x9ED8;&#x8BA4;&#x4E3A;<code>create_graph</code>&#x7684;&#x503C;&#x3002;</p>
</li>
<li><p><strong>create_graph</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x5982;&#x679C;<code>True</code>&#xFF0C;&#x5219;&#x5C06;&#x6784;&#x9020;&#x5BFC;&#x6570;&#x56FE;&#xFF0C;&#x4ECE;&#x800C;&#x5141;&#x8BB8;&#x8BA1;&#x7B97;&#x9AD8;&#x9636;&#x5BFC;&#x6570;&#x4EA7;&#x54C1;&#x3002; &#x9ED8;&#x8BA4;&#x4E3A;<code>False</code>&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>baddbmm(beta=1, alpha=1, batch1, batch2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.baddbmm" title="torch.baddbmm"><code>torch.baddbmm()</code></a></p>
<hr>
<pre><code>baddbmm_(beta=1, alpha=1, batch1, batch2) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code>baddbmm()</code></a></p>
<hr>
<pre><code>bernoulli(*, generator=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6BCF;&#x4E2A;$result[i]$&#x4ECE;$Bernoulli(self[i])$&#x4E2D;&#x72EC;&#x7ACB;&#x91C7;&#x6837;&#x3002; <code>self</code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x6D6E;&#x70B9;<code>dtype</code>&#xFF0C;&#x7ED3;&#x679C;&#x5C06;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;<code>dtype</code>&#x3002;</p>
<p>&#x53C2;&#x89C1; <a href="torch.html#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a></p>
<hr>
<pre><code>bernoulli_(p=0.5, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;$Bernoulli(p)$&#x7684;&#x72EC;&#x7ACB;&#x6837;&#x672C;&#x586B;&#x5145;<code>self</code>&#x7684;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x3002; <code>self</code>&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x6574;&#x6570;<code>dtype</code>&#x3002;</p>
<hr>
<pre><code>bfloat16() &#x2192; Tensor
</code></pre><p><code>self.bfloat16()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.bfloat16)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>bincount(weights=None, minlength=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.bincount" title="torch.bincount"><code>torch.bincount()</code></a></p>
<hr>
<pre><code>bitwise_not() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.bitwise_not" title="torch.bitwise_not"><code>torch.bitwise_not()</code></a></p>
<hr>
<pre><code>bitwise_not_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.bitwise_not" title="torch.Tensor.bitwise_not"><code>bitwise_not()</code></a></p>
<hr>
<pre><code>bitwise_xor() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.bitwise_xor" title="torch.bitwise_xor"><code>torch.bitwise_xor()</code></a></p>
<hr>
<pre><code>bitwise_xor_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.bitwise_xor" title="torch.Tensor.bitwise_xor"><code>bitwise_xor()</code></a></p>
<hr>
<pre><code>bmm(batch2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a></p>
<hr>
<pre><code>bool() &#x2192; Tensor
</code></pre><p><code>self.bool()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.bool)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>byte() &#x2192; Tensor
</code></pre><p><code>self.byte()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.uint8)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>cauchy_(median=0, sigma=1, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;&#x4ECE;&#x67EF;&#x897F;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x6570;&#x5B57;&#x586B;&#x5145;&#x5F20;&#x91CF;&#xFF1A;</p>
<p><script type="math/tex; ">f(x)=\frac {1}{\pi}\frac {\sigma}{(x-median)^2+\sigma^2}</script></p>
<hr>
<pre><code>ceil() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ceil" title="torch.ceil"><code>torch.ceil()</code></a></p>
<hr>
<pre><code>ceil_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code>ceil()</code></a></p>
<hr>
<pre><code>char() &#x2192; Tensor
</code></pre><p><code>self.char()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.int8)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>cholesky(upper=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cholesky" title="torch.cholesky"><code>torch.cholesky()</code></a></p>
<hr>
<pre><code>cholesky_inverse(upper=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code>torch.cholesky_inverse()</code></a></p>
<hr>
<pre><code>cholesky_solve(input2, upper=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cholesky_solve" title="torch.cholesky_solve"><code>torch.cholesky_solve()</code></a></p>
<hr>
<pre><code>chunk(chunks, dim=0) &#x2192; List of Tensors
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a></p>
<hr>
<pre><code>clamp(min, max) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.clamp" title="torch.clamp"><code>torch.clamp()</code></a></p>
<hr>
<pre><code>clamp_(min, max) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code>clamp()</code></a></p>
<hr>
<pre><code>clone() &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x526F;&#x672C;&#x3002; &#x8BE5;&#x526F;&#x672C;&#x7684;&#x5927;&#x5C0F;&#x548C;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x4E0E;<code>self</code>&#x76F8;&#x540C;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x4E0E; <em>copy_</em>()&#x4E0D;&#x540C;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x8BB0;&#x5F55;&#x5728;&#x8BA1;&#x7B97;&#x56FE;&#x4E2D;&#x3002; &#x4F20;&#x64AD;&#x5230;&#x514B;&#x9686;&#x5F20;&#x91CF;&#x7684;&#x6E10;&#x53D8;&#x5C06;&#x4F20;&#x64AD;&#x5230;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>contiguous() &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x4E0E;<code>self</code>&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x7684;&#x8FDE;&#x7EED;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>self</code>&#x5F20;&#x91CF;&#x662F;&#x8FDE;&#x7EED;&#x7684;&#xFF0C;&#x5219;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>copy_(src, non_blocking=False) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5143;&#x7D20;&#x4ECE;<code>src</code>&#x590D;&#x5236;&#x5230;<code>self</code>&#x5F20;&#x91CF;&#x5E76;&#x8FD4;&#x56DE;<code>self</code>&#x3002;</p>
<p><code>src</code>&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x4E0E;<code>self</code>&#x5F20;&#x91CF;&#x4E00;&#x8D77;<a href="&#x6CE8;&#x610F;s/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;</a>&#x3002; &#x5B83;&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x4F4D;&#x4E8E;&#x4E0D;&#x540C;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>src</strong>  (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x4ECE;&#x4E2D;&#x590D;&#x5236;&#x7684;&#x6E90;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>non_blocking</strong>  (<em>bool</em> )&#x2013;&#x5982;&#x679C;<code>True</code>&#x5E76;&#x4E14;&#x6B64;&#x526F;&#x672C;&#x4F4D;&#x4E8E; CPU &#x548C; GPU &#x4E4B;&#x95F4;&#xFF0C;&#x5219;&#x8BE5;&#x526F;&#x672C;&#x53EF;&#x80FD;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x5F02;&#x6B65;&#x53D1;&#x751F;&#x3002; &#x5728;&#x5176;&#x4ED6;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6B64;&#x53C2;&#x6570;&#x65E0;&#x6548;&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>conj() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.conj" title="torch.conj"><code>torch.conj()</code></a></p>
<hr>
<pre><code>cos() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cos" title="torch.cos"><code>torch.cos()</code></a></p>
<hr>
<pre><code>cos_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.cos" title="torch.Tensor.cos"><code>cos()</code></a></p>
<hr>
<pre><code>cosh() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cosh" title="torch.cosh"><code>torch.cosh()</code></a></p>
<hr>
<pre><code>cosh_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code>cosh()</code></a></p>
<hr>
<pre><code>cpu() &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x6B64;&#x5BF9;&#x8C61;&#x5728; CPU &#x5185;&#x5B58;&#x4E2D;&#x7684;&#x526F;&#x672C;&#x3002;</p>
<p>&#x5982;&#x679C;&#x8BE5;&#x5BF9;&#x8C61;&#x5DF2;&#x7ECF;&#x5728; CPU &#x5185;&#x5B58;&#x4E2D;&#x5E76;&#x4E14;&#x5728;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x5219;&#x4E0D;&#x6267;&#x884C;&#x4EFB;&#x4F55;&#x590D;&#x5236;&#x64CD;&#x4F5C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x59CB;&#x5BF9;&#x8C61;&#x3002;</p>
<hr>
<pre><code>cross(other, dim=-1) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cross" title="torch.cross"><code>torch.cross()</code></a></p>
<hr>
<pre><code>cuda(device=None, non_blocking=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x6B64;&#x5BF9;&#x8C61;&#x5728; CUDA &#x5185;&#x5B58;&#x4E2D;&#x7684;&#x526F;&#x672C;&#x3002;</p>
<p>&#x5982;&#x679C;&#x6B64;&#x5BF9;&#x8C61;&#x5DF2;&#x7ECF;&#x5728; CUDA &#x5185;&#x5B58;&#x4E2D;&#x5E76;&#x4E14;&#x5728;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#xFF0C;&#x5219;&#x4E0D;&#x6267;&#x884C;&#x4EFB;&#x4F55;&#x590D;&#x5236;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x59CB;&#x5BF9;&#x8C61;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>) &#x2013; &#x76EE;&#x6807; GPU &#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x4E3A;&#x5F53;&#x524D; CUDA &#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>non_blocking</strong>  (<em>bool</em> ) &#x2013; &#x5982;&#x679C;<code>True</code>&#x5E76;&#x4E14;&#x6E90;&#x4F4D;&#x4E8E;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x4E2D;&#xFF0C;&#x5219;&#x526F;&#x672C;&#x5C06;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x662F;&#x5F02;&#x6B65;&#x7684;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x53C2;&#x6570;&#x65E0;&#x6548;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>cumprod(dim, dtype=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cumprod" title="torch.cumprod"><code>torch.cumprod()</code></a></p>
<hr>
<pre><code>cumsum(dim, dtype=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.cumsum" title="torch.cumsum"><code>torch.cumsum()</code></a></p>
<hr>
<pre><code>data_ptr() &#x2192; int
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5730;&#x5740;&#x3002;</p>
<hr>
<pre><code>dequantize() &#x2192; Tensor
</code></pre><p>&#x7ED9;&#x5B9A;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5BF9;&#x5176;&#x8FDB;&#x884C;&#x53CD;&#x91CF;&#x5316;&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x53CD;&#x91CF;&#x5316;&#x540E;&#x7684;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>det() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.det" title="torch.det"><code>torch.det()</code></a></p>
<hr>
<pre><code>dense_dim() &#x2192; int
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x662F;&#x7A00;&#x758F;&#x7684; COO &#x5F20;&#x91CF;(&#x5373;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5BC6;&#x96C6;&#x5C3A;&#x5BF8;&#x7684;&#x6570;&#x91CF;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code>Tensor.sparse_dim()</code></a> &#x3002;</p>
<hr>
<pre><code>detach()
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;&#x5F53;&#x524D;&#x56FE;&#x5F62;&#x5206;&#x79BB;&#x7684;&#x65B0; Tensor&#x3002;</p>
<p>&#x7ED3;&#x679C;&#x5C06;&#x6C38;&#x8FDC;&#x4E0D;&#x9700;&#x8981;&#x68AF;&#x5EA6;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x8FD4;&#x56DE;&#x7684; Tensor &#x4E0E;&#x539F;&#x59CB; Tensor &#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x3002; &#x53EF;&#x4EE5;&#x770B;&#x5230;&#x5BF9;&#x5B83;&#x4EEC;&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x4E00;&#x4E2A;&#x7684;&#x5C31;&#x5730;&#x4FEE;&#x6539;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x80FD;&#x89E6;&#x53D1;&#x6B63;&#x786E;&#x6027;&#x68C0;&#x67E5;&#x4E2D;&#x7684;&#x9519;&#x8BEF;&#x3002; &#x91CD;&#x8981;&#x8BF4;&#x660E;&#xFF1A;&#x4EE5;&#x524D;&#xFF0C;&#x5C31;&#x5730;&#x5927;&#x5C0F;/&#x6B65;&#x5E45;/&#x5B58;&#x50A8;&#x66F4;&#x6539;(&#x4F8B;&#x5982; <em>resize<strong>/_resize_as</strong>/<em>set<strong>/_transpose</strong>) &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E5F;&#x4F1A;&#x66F4;&#x65B0;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x3002; &#x73B0;&#x5728;&#xFF0C;&#x8FD9;&#x4E9B;&#x5C31;&#x5730;&#x66F4;&#x6539;&#x5C06;&#x4E0D;&#x518D;&#x66F4;&#x65B0;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#xFF0C;&#x800C;&#x5C06;&#x89E6;&#x53D1;&#x9519;&#x8BEF;&#x3002; &#x5BF9;&#x4E8E;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#xFF1A;&#x539F;&#x4F4D;&#x7D22;&#x5F15; / &#x503C;&#x66F4;&#x6539;(&#x4F8B;&#x5982; <em>zero<em>_/_copy</em></em>/_add</em></em>)&#x5C06;&#x4E0D;&#x4F1A;&#x518D;&#x66F4;&#x65B0;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#xFF0C; &#x800C;&#x662F;&#x89E6;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<hr>
<pre><code>detach_()
</code></pre><p>&#x4ECE;&#x521B;&#x5EFA;&#x5B83;&#x7684;&#x56FE;&#x5F62;&#x4E2D;&#x5206;&#x79BB;&#x5F20;&#x91CF;&#xFF0C;&#x4F7F;&#x5176;&#x6210;&#x4E3A;&#x4E00;&#x7247;&#x53F6;&#x5B50;&#x3002; &#x89C6;&#x56FE;&#x4E0D;&#x80FD;&#x5C31;&#x5730;&#x5206;&#x79BB;&#x3002;</p>
<hr>
<pre><code>diag(diagonal=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.diag" title="torch.diag"><code>torch.diag()</code></a></p>
<hr>
<pre><code>diag_embed(offset=0, dim1=-2, dim2=-1) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a></p>
<hr>
<pre><code>diagflat(offset=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.diagflat" title="torch.diagflat"><code>torch.diagflat()</code></a></p>
<hr>
<pre><code>diagonal(offset=0, dim1=0, dim2=1) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a></p>
<hr>
<pre><code>fill_diagonal_(fill_value, wrap=False) &#x2192; Tensor
</code></pre><p>&#x586B;&#x5145;&#x5177;&#x6709;&#x81F3;&#x5C11; 2 &#x7EF4;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x5F53;&gt; 2 &#x53D8;&#x6697;&#x65F6;&#xFF0C;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x76F8;&#x7B49;&#x3002; &#x6B64;&#x51FD;&#x6570;&#x5C31;&#x5730;&#x4FEE;&#x6539;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>fill_value</strong> (<em>&#x6807;&#x91CF;</em>&#xFF09;&#x2013; &#x586B;&#x5145;&#x503C;</p>
</li>
<li><p><strong>wrap</strong> (<em>bool</em> ) &#x2013; &#x5BF9;&#x89D2;&#x7EBF;&#x201C;&#x5305;&#x88F9;&#x201D;&#x5728;&#x9AD8;&#x5217;&#x7684; N &#x5217;&#x4E4B;&#x540E;&#x3002;</p>
</li>
</ul>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; a = torch.zeros(3, 3)
&gt;&gt;&gt; a.fill_diagonal_(5)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.]])
&gt;&gt;&gt; b = torch.zeros(7, 3)
&gt;&gt;&gt; b.fill_diagonal_(5)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
&gt;&gt;&gt; c = torch.zeros(7, 3)
&gt;&gt;&gt; c.fill_diagonal_(5, wrap=True)
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.],
        [0., 0., 0.],
        [5., 0., 0.],
        [0., 5., 0.],
        [0., 0., 5.]])
</code></pre><hr>
<pre><code>digamma() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.digamma" title="torch.digamma"><code>torch.digamma()</code></a></p>
<hr>
<pre><code>digamma_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.digamma" title="torch.Tensor.digamma"><code>digamma()</code></a></p>
<hr>
<pre><code>dim() &#x2192; int
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<hr>
<pre><code>dist(other, p=2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.dist" title="torch.dist"><code>torch.dist()</code></a></p>
<hr>
<pre><code>div(value) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.div" title="torch.div"><code>torch.div()</code></a></p>
<hr>
<pre><code>div_(value) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.div" title="torch.Tensor.div"><code>div()</code></a></p>
<hr>
<pre><code>dot(tensor2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.dot" title="torch.dot"><code>torch.dot()</code></a></p>
<hr>
<pre><code>double() &#x2192; Tensor
</code></pre><p><code>self.double()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.float64)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>eig(eigenvectors=False) -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.eig" title="torch.eig"><code>torch.eig()</code></a></p>
<hr>
<pre><code>element_size() &#x2192; int
</code></pre><p>&#x8FD4;&#x56DE;&#x5355;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5927;&#x5C0F;(&#x4EE5;&#x5B57;&#x8282;&#x4E3A;&#x5355;&#x4F4D;&#xFF09;&#x3002;</p>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([]).element_size()
4
&gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size()
1
</code></pre><hr>
<pre><code>eq(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.eq" title="torch.eq"><code>torch.eq()</code></a></p>
<hr>
<pre><code>eq_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.eq" title="torch.Tensor.eq"><code>eq()</code></a></p>
<hr>
<pre><code>equal(other) &#x2192; bool
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.equal" title="torch.equal"><code>torch.equal()</code></a></p>
<hr>
<pre><code>erf() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.erf" title="torch.erf"><code>torch.erf()</code></a></p>
<hr>
<pre><code>erf_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.erf" title="torch.Tensor.erf"><code>erf()</code></a></p>
<hr>
<pre><code>erfc() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.erfc" title="torch.erfc"><code>torch.erfc()</code></a></p>
<hr>
<pre><code>erfc_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.erfc" title="torch.Tensor.erfc"><code>erfc()</code></a></p>
<hr>
<pre><code>erfinv() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.erfinv" title="torch.erfinv"><code>torch.erfinv()</code></a></p>
<hr>
<pre><code>erfinv_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code>erfinv()</code></a></p>
<hr>
<pre><code>exp() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.exp" title="torch.exp"><code>torch.exp()</code></a></p>
<hr>
<pre><code>exp_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.exp" title="torch.Tensor.exp"><code>exp()</code></a></p>
<hr>
<pre><code>expm1() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.expm1" title="torch.expm1"><code>torch.expm1()</code></a></p>
<hr>
<pre><code>expm1_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.expm1" title="torch.Tensor.expm1"><code>expm1()</code></a></p>
<hr>
<pre><code>expand(*sizes) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x65B0;&#x89C6;&#x56FE;&#xFF0C;&#x5176;&#x4E2D;&#x5355;&#x4F8B;&#x5C3A;&#x5BF8;&#x6269;&#x5C55;&#x4E3A;&#x66F4;&#x5927;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x5C06;-1 &#x4F20;&#x9012;&#x4E3A;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x8868;&#x793A;&#x4E0D;&#x66F4;&#x6539;&#x8BE5;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Tensor &#x4E5F;&#x53EF;&#x4EE5;&#x6269;&#x5C55;&#x5230;&#x66F4;&#x5927;&#x7684;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76;&#x4E14;&#x65B0;&#x5C3A;&#x5BF8;&#x5C06;&#x9644;&#x52A0;&#x5728;&#x524D;&#x9762;&#x3002; &#x5BF9;&#x4E8E;&#x65B0;&#x5C3A;&#x5BF8;&#xFF0C;&#x5C3A;&#x5BF8;&#x4E0D;&#x80FD;&#x8BBE;&#x7F6E;&#x4E3A;-1&#x3002;</p>
<p>&#x6269;&#x5C55;&#x5F20;&#x91CF;&#x4E0D;&#x4F1A;&#x5206;&#x914D;&#x65B0;&#x7684;&#x5185;&#x5B58;&#xFF0C;&#x800C;&#x53EA;&#x4F1A;&#x5728;&#x73B0;&#x6709;&#x5F20;&#x91CF;&#x4E0A;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x65B0;&#x89C6;&#x56FE;&#xFF0C;&#x5176;&#x4E2D;&#x901A;&#x8FC7;&#x5C06;<code>stride</code>&#x8BBE;&#x7F6E;&#x4E3A; 0&#xFF0C;&#x5C06;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x7EF4;&#x6269;&#x5C55;&#x4E3A;&#x66F4;&#x5927;&#x7684;&#x5C3A;&#x5BF8;&#x3002;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x4EFB;&#x4F55;&#x7EF4;&#x90FD;&#x53EF;&#x4EE5;&#x6269;&#x5C55;&#x4E3A; &#x4E0D;&#x5206;&#x914D;&#x65B0;&#x5185;&#x5B58;&#x7684;&#x4EFB;&#x610F;&#x503C;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>*&#x5927;&#x5C0F;</strong>(<em>torch&#x5927;&#x5C0F;</em> <em>&#x6216;</em> <em>python&#xFF1A;int ...</em> )&#x2013;&#x6240;&#x9700;&#x7684;&#x6269;&#x5C55;&#x5927;&#x5C0F;</p>
<p>&#x8B66;&#x544A;</p>
<p>&#x6269;&#x5C55;&#x5F20;&#x91CF;&#x7684;&#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x5143;&#x7D20;&#x53EF;&#x4EE5;&#x5F15;&#x7528;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;&#x3002; &#x7ED3;&#x679C;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;(&#x5C24;&#x5176;&#x662F;&#x77E2;&#x91CF;&#x5316;&#x7684;&#x64CD;&#x4F5C;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x7684;&#x884C;&#x4E3A;&#x3002; &#x5982;&#x679C;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])
&gt;&gt;&gt; x.size()
torch.Size([3, 1])
&gt;&gt;&gt; x.expand(3, 4)
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])
</code></pre><hr>
<pre><code>expand_as(other) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x8BE5;&#x5F20;&#x91CF;&#x6269;&#x5C55;&#x4E3A;&#x4E0E;<code>other</code>&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002; <code>self.expand_as(other)</code>&#x7B49;&#x6548;&#x4E8E;<code>self.expand(other.size())</code>&#x3002;</p>
<p>&#x6709;&#x5173;<code>expand</code>&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.expand" title="torch.Tensor.expand"><code>expand()</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5176;&#x4ED6;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>)&#x2013;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>other</code>&#x76F8;&#x540C;&#x3002;</p>
<hr>
<pre><code>exponential_(lambd=1, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;&#x4ECE;&#x6307;&#x6570;&#x5206;&#x5E03;&#x4E2D;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#xFF1A;</p>
<p><script type="math/tex; ">f(x)=\lambda e^{-\lambda x}</script></p>
<hr>
<pre><code>fft(signal_ndim, normalized=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.fft" title="torch.fft"><code>torch.fft()</code></a></p>
<hr>
<pre><code>fill_(value) &#x2192; Tensor
</code></pre><p>&#x7528;&#x6307;&#x5B9A;&#x503C;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>flatten(input, start_dim=0, end_dim=-1) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.flatten" title="torch.flatten"><code>torch.flatten()</code></a></p>
<hr>
<pre><code>flip(dims) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.flip" title="torch.flip"><code>torch.flip()</code></a></p>
<hr>
<pre><code>float() &#x2192; Tensor
</code></pre><p><code>self.float()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.float32)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>floor() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.floor" title="torch.floor"><code>torch.floor()</code></a></p>
<hr>
<pre><code>floor_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.floor" title="torch.Tensor.floor"><code>floor()</code></a></p>
<hr>
<pre><code>fmod(divisor) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a></p>
<hr>
<pre><code>fmod_(divisor) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code>fmod()</code></a></p>
<hr>
<pre><code>frac() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.frac" title="torch.frac"><code>torch.frac()</code></a></p>
<hr>
<pre><code>frac_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.frac" title="torch.Tensor.frac"><code>frac()</code></a></p>
<hr>
<pre><code>gather(dim, index) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.gather" title="torch.gather"><code>torch.gather()</code></a></p>
<hr>
<pre><code>ge(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ge" title="torch.ge"><code>torch.ge()</code></a></p>
<hr>
<pre><code>ge_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.ge" title="torch.Tensor.ge"><code>ge()</code></a></p>
<hr>
<pre><code>geometric_(p, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;&#x4ECE;&#x51E0;&#x4F55;&#x5206;&#x5E03;&#x4E2D;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#xFF1A;</p>
<p><script type="math/tex; ">f(X=k)=p^{k-1}(1-p)</script></p>
<hr>
<pre><code>geqrf() -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a></p>
<hr>
<pre><code>ger(vec2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ger" title="torch.ger"><code>torch.ger()</code></a></p>
<hr>
<pre><code>get_device() -&gt; Device ordinal (Integer)
</code></pre><p>&#x5BF9;&#x4E8E; CUDA &#x5F20;&#x91CF;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x6240;&#x5728;&#x7684; GPU &#x7684;&#x8BBE;&#x5907;&#x5E8F;&#x53F7;&#x3002; &#x5BF9;&#x4E8E; CPU &#x5F20;&#x91CF;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4, 5, device=&apos;cuda:0&apos;)
&gt;&gt;&gt; x.get_device()
0
&gt;&gt;&gt; x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor
</code></pre><hr>
<pre><code>gt(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.gt" title="torch.gt"><code>torch.gt()</code></a></p>
<hr>
<pre><code>gt_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.gt" title="torch.Tensor.gt"><code>gt()</code></a></p>
<hr>
<pre><code>half() &#x2192; Tensor
</code></pre><p><code>self.half()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.float16)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>hardshrink(lambd=0.5) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="nn.functional.html#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"><code>torch.nn.functional.hardshrink()</code></a></p>
<hr>
<pre><code>histc(bins=100, min=0, max=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.histc" title="torch.histc"><code>torch.histc()</code></a></p>
<hr>
<pre><code>ifft(signal_ndim, normalized=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ifft" title="torch.ifft"><code>torch.ifft()</code></a></p>
<hr>
<pre><code>imag() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.imag" title="torch.imag"><code>torch.imag()</code></a></p>
<hr>
<pre><code>index_add_(dim, index, tensor) &#x2192; Tensor
</code></pre><p>&#x901A;&#x8FC7;&#x6309;<code>index</code>&#x4E2D;&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x6DFB;&#x52A0;&#x7D22;&#x5F15;&#xFF0C;&#x5C06; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x7684;&#x5143;&#x7D20;&#x7D2F;&#x79EF;&#x5230;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x3002; &#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>dim == 0</code>&#x548C;<code>index[i] == j</code>&#xFF0C;&#x5219;&#x5C06;<a href="torch.html#torch.tensor" title="torch.tensor">&#x7684;&#x7B2C;<code>i</code>&#x884C;<code>tensor</code></a> &#x6DFB;&#x52A0;&#x5230;<code>self</code>&#x7684;&#x7B2C;<code>j</code>&#x884C;&#x3002;</p>
<p><a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x7684; <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a> &#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x4E0E;<code>index</code>&#x7684;&#x957F;&#x5EA6;(&#x5FC5;&#x987B;&#x4E3A;&#x77E2;&#x91CF;&#xFF09;&#x7684;&#x5C3A;&#x5BF8;&#x76F8;&#x540C;&#xFF0C;&#x5E76;&#x4E14;&#x6240;&#x6709;&#x5176;&#x4ED6;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x4E0E;<code>self</code> ]&#xFF0C;&#x5426;&#x5219;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x4F7F;&#x7528; CUDA &#x540E;&#x7AEF;&#x65F6;&#xFF0C;&#x6B64;&#x64CD;&#x4F5C;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x884C;&#x4E3A;&#xFF0C;&#x4E0D;&#x5BB9;&#x6613;&#x5173;&#x95ED;&#x3002; &#x6709;&#x5173;&#x80CC;&#x666F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;<a href="&#x6CE8;&#x610F;s/randomness.html">&#x91CD;&#x73B0;&#x6027;</a>&#x7684;&#x6CE8;&#x91CA;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x7D22;&#x5F15;&#x6240;&#x6CBF;&#x7684;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x7684;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8981;&#x6DFB;&#x52A0;&#x7684;&#x503C;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(5, 3)
&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 4, 2])
&gt;&gt;&gt; x.index_add_(0, index, t)
tensor([[  2.,   3.,   4.],
        [  1.,   1.,   1.],
        [  8.,   9.,  10.],
        [  1.,   1.,   1.],
        [  5.,   6.,   7.]])
</code></pre><hr>
<pre><code>index_add(dim, index, tensor) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code>torch.Tensor.index_add_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>index_copy_(dim, index, tensor) &#x2192; Tensor
</code></pre><p>&#x901A;&#x8FC7;&#x6309;<code>index</code>&#x4E2D;&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x9009;&#x62E9;&#x7D22;&#x5F15;&#xFF0C;&#x5C06; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x7684;&#x5143;&#x7D20;&#x590D;&#x5236;&#x5230;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x3002; &#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>dim == 0</code>&#x548C;<code>index[i] == j</code>&#xFF0C;&#x5219;&#x5C06; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x7684;&#x7B2C;<code>i</code>&#x884C;&#x590D;&#x5236;&#x5230;<code>self</code>&#x7684;&#x7B2C;<code>j</code>&#x884C;&#x3002;</p>
<p>The <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a>th dimension of <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> must have the same size as the length of <code>index</code> (which must be a vector), and all other dimensions must match <code>self</code>, or an error will be raised.</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; dimension along which to index</p>
</li>
<li><p><strong>index</strong> (<em>LongTensor</em>) &#x2013; indices of <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> to select from</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8981;&#x590D;&#x5236;&#x7684;&#x503C;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(5, 3)
&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 4, 2])
&gt;&gt;&gt; x.index_copy_(0, index, t)
tensor([[ 1.,  2.,  3.],
        [ 0.,  0.,  0.],
        [ 7.,  8.,  9.],
        [ 0.,  0.,  0.],
        [ 4.,  5.,  6.]])
</code></pre><hr>
<pre><code>index_copy(dim, index, tensor) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code>torch.Tensor.index_copy_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>index_fill_(dim, index, val) &#x2192; Tensor
</code></pre><p>&#x901A;&#x8FC7;&#x6309;<code>index</code>&#x4E2D;&#x7ED9;&#x5B9A;&#x7684;&#x987A;&#x5E8F;&#x9009;&#x62E9;&#x7D22;&#x5F15;&#xFF0C;&#x7528;&#x503C;<code>val</code>&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; dimension along which to index</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013;&#x586B;&#x5199;&#x7684;<code>self</code>&#x5F20;&#x91CF;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>val</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x8981;&#x586B;&#x5145;&#x7684;&#x503C;</p>
</li>
</ul>
<pre><code>&#x4F8B;&#xFF1A;:
</code></pre><pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 2])
&gt;&gt;&gt; x.index_fill_(1, index, -1)
tensor([[-1.,  2., -1.],
        [-1.,  5., -1.],
        [-1.,  8., -1.]])
</code></pre><hr>
<pre><code>index_fill(dim, index, value) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code>torch.Tensor.index_fill_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>index_put_(indices, value, accumulate=False) &#x2192; Tensor
</code></pre><p>&#x4F7F;&#x7528;&#x5728; <a href="#torch.Tensor.indices" title="torch.Tensor.indices"><code>indices</code></a> &#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;(&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#xFF09;&#x5C06;&#x5F20;&#x91CF;<code>value</code>&#x7684;&#x503C;&#x653E;&#x5165;&#x5F20;&#x91CF;<code>self</code>&#x3002; &#x8868;&#x8FBE;&#x5F0F;<code>tensor.index_put_(indices, value)</code>&#x7B49;&#x6548;&#x4E8E;<code>tensor[indices] = value</code>&#x3002; &#x8FD4;&#x56DE;<code>self</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>accumulate</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x5C06; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x4E2D;&#x7684;&#x5143;&#x7D20;&#x6DFB;&#x52A0;&#x5230;<code>self</code>&#x4E2D;&#x3002; &#x5982;&#x679C; accumulate &#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x5728;&#x7D22;&#x5F15;&#x5305;&#x542B;&#x91CD;&#x590D;&#x5143;&#x7D20;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x884C;&#x4E3A;&#x672A;&#x5B9A;&#x4E49;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>&#x7D22;&#x5F15;</strong>(LongTensor &#x7684;<em>&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x7528;&#x4E8E;&#x7D22;&#x5F15;&lt;cite&gt;&#x81EA;&#x8EAB;&lt;/cite&gt;&#x7684;&#x5F20;&#x91CF;&#x3002;</em></p>
</li>
<li><p><strong>&#x503C;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x4E0E;&lt;cite&gt;&#x81EA;&#x8EAB;&lt;/cite&gt;&#x76F8;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>&#x7D2F;&#x79EF;</strong> (<em>bool</em> )&#x2013;&#x662F;&#x5426;&#x7D2F;&#x79EF;</p>
</li>
</ul>
<hr>
<pre><code>index_put(indices, value, accumulate=False) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.index_put_" title="torch.Tensor.index_put_"><code>index_put_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>index_select(dim, index) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.index_select" title="torch.index_select"><code>torch.index_select()</code></a></p>
<hr>
<pre><code>indices() &#x2192; Tensor
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x662F;&#x7A00;&#x758F;&#x7684; COO &#x5F20;&#x91CF;(&#x5373;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x7684;&#x7D22;&#x5F15;&#x5F20;&#x91CF;&#x7684;&#x89C6;&#x56FE;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.values" title="torch.Tensor.values"><code>Tensor.values()</code></a> &#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x53EA;&#x80FD;&#x5728;&#x5408;&#x5E76;&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x4E0A;&#x8C03;&#x7528;&#x6B64;&#x65B9;&#x6CD5;&#x3002; &#x6709;&#x5173;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;<code>Tensor.coalesce()</code>&#x3002;</p>
<hr>
<pre><code>int() &#x2192; Tensor
</code></pre><p><code>self.int()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.int32)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>int_repr() &#x2192; Tensor
</code></pre><p>&#x7ED9;&#x5B9A;&#x91CF;&#x5316;&#x7684; Tensor&#xFF0C;<code>self.int_repr()</code>&#x8FD4;&#x56DE;&#x4EE5; uint8_t &#x4F5C;&#x4E3A;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684; CPU Tensor&#xFF0C;&#x8BE5;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x5B58;&#x50A8;&#x7ED9;&#x5B9A; Tensor &#x7684;&#x57FA;&#x7840; uint8_t &#x503C;&#x3002;</p>
<hr>
<pre><code>inverse() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.inverse" title="torch.inverse"><code>torch.inverse()</code></a></p>
<hr>
<pre><code>irfft(signal_ndim, normalized=False, onesided=True, signal_sizes=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.irfft" title="torch.irfft"><code>torch.irfft()</code></a></p>
<hr>
<pre><code>is_contiguous() &#x2192; bool
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x5F20;&#x91CF;&#x5728;&#x5185;&#x5B58;&#x4E2D;&#x4EE5; C &#x987A;&#x5E8F;&#x8FDE;&#x7EED;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<hr>
<pre><code>is_floating_point() &#x2192; bool
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x662F;&#x6D6E;&#x70B9;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<pre><code>is_leaf
</code></pre><p>&#x6309;&#x7167;&#x60EF;&#x4F8B;&#xFF0C;&#x6240;&#x6709;&#x5177;&#x6709; <a href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code>requires_grad</code></a> &#x5373;<code>False</code>&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x662F;&#x53F6;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x5177;&#x6709; <a href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code>requires_grad</code></a> (&#x5373;<code>True</code>&#xFF09;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5982;&#x679C;&#x5B83;&#x4EEC;&#x662F;&#x7531;&#x7528;&#x6237;&#x521B;&#x5EFA;&#x7684;&#xFF0C;&#x5219;&#x5B83;&#x4EEC;&#x5C06;&#x662F;&#x53F6;&#x5F20;&#x91CF;&#x3002; &#x8FD9;&#x610F;&#x5473;&#x7740;&#x5B83;&#x4EEC;&#x4E0D;&#x662F;&#x8FD0;&#x7B97;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x56E0;&#x6B64;<code>grad_fn</code>&#x4E3A;&#x201C;&#x65E0;&#x201D;&#x3002;</p>
<p>&#x5728;&#x8C03;&#x7528; <a href="autograd.html#torch.Tensor.backward" title="torch.Tensor.backward"><code>backward()</code></a> &#x671F;&#x95F4;&#xFF0C;&#x4EC5;&#x53F6;&#x5B50;&#x5F20;&#x91CF;&#x4F1A;&#x586B;&#x5145;&#x5176; <a href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> &#x3002; &#x8981;&#x4E3A;&#x975E;&#x53F6;&#x5F20;&#x91CF;&#x586B;&#x5145; <a href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> &#xFF0C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528; <a href="autograd.html#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code>retain_grad()</code></a> &#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)
&gt;&gt;&gt; a.is_leaf
True
&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()
&gt;&gt;&gt; b.is_leaf
False
# b was created by the operation that cast a cpu Tensor into a cuda Tensor
&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2
&gt;&gt;&gt; c.is_leaf
False
# c was created by the addition operation
&gt;&gt;&gt; d = torch.rand(10).cuda()
&gt;&gt;&gt; d.is_leaf
True
# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)
&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()
&gt;&gt;&gt; e.is_leaf
True
# e requires gradients and has no operations creating it
&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=&quot;cuda&quot;)
&gt;&gt;&gt; f.is_leaf
True
# f requires grad, has no operation creating it
</code></pre><hr>
<pre><code>is_pinned()
</code></pre><p>&#x5982;&#x679C;&#x8BE5;&#x5F20;&#x91CF;&#x9A7B;&#x7559;&#x5728;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;&#x4E2D;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; true&#x3002;</p>
<hr>
<pre><code>is_set_to(tensor) &#x2192; bool
</code></pre><p>&#x5982;&#x679C;&#x6B64;&#x5BF9;&#x8C61;&#x5F15;&#x7528;&#x4E0E; Torch C API &#x4E2D;&#x76F8;&#x540C;&#x7684;<code>THTensor</code>&#x5BF9;&#x8C61;&#x4F5C;&#x4E3A;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<hr>
<pre><code>is_shared()
</code></pre><p>&#x68C0;&#x67E5;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x5728;&#x5171;&#x4EAB;&#x5185;&#x5B58;&#x4E2D;&#x3002;</p>
<p>CUDA &#x5F20;&#x91CF;&#x59CB;&#x7EC8;&#x4E3A;<code>True</code>&#x3002;</p>
<hr>
<pre><code>is_signed() &#x2192; bool
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x662F;&#x5E26;&#x7B26;&#x53F7;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<pre><code>is_sparse
</code></pre><hr>
<pre><code>item() &#x2192; number
</code></pre><p>&#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x91CF;&#x7684;&#x503C;&#x4F5C;&#x4E3A;&#x6807;&#x51C6; Python &#x6570;&#x3002; &#x8FD9;&#x4EC5;&#x9002;&#x7528;&#x4E8E;&#x5177;&#x6709;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5BF9;&#x4E8E;&#x5176;&#x4ED6;&#x60C5;&#x51B5;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.tolist" title="torch.Tensor.tolist"><code>tolist()</code></a> &#x3002;</p>
<p>&#x6B64;&#x64CD;&#x4F5C;&#x4E0D;&#x53EF;&#x533A;&#x5206;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1.0])
&gt;&gt;&gt; x.item()
1.0
</code></pre><hr>
<pre><code>kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.kthvalue" title="torch.kthvalue"><code>torch.kthvalue()</code></a></p>
<hr>
<pre><code>le(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.le" title="torch.le"><code>torch.le()</code></a></p>
<hr>
<pre><code>le_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.le" title="torch.Tensor.le"><code>le()</code></a></p>
<hr>
<pre><code>lerp(end, weight) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lerp" title="torch.lerp"><code>torch.lerp()</code></a></p>
<hr>
<pre><code>lerp_(end, weight) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code>lerp()</code></a></p>
<hr>
<pre><code>lgamma() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lgamma" title="torch.lgamma"><code>torch.lgamma()</code></a></p>
<hr>
<pre><code>lgamma_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.lgamma" title="torch.Tensor.lgamma"><code>lgamma()</code></a></p>
<hr>
<pre><code>log() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.log" title="torch.log"><code>torch.log()</code></a></p>
<hr>
<pre><code>log_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.log" title="torch.Tensor.log"><code>log()</code></a></p>
<hr>
<pre><code>logdet() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.logdet" title="torch.logdet"><code>torch.logdet()</code></a></p>
<hr>
<pre><code>log10() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.log10" title="torch.log10"><code>torch.log10()</code></a></p>
<hr>
<pre><code>log10_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.log10" title="torch.Tensor.log10"><code>log10()</code></a></p>
<hr>
<pre><code>log1p() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.log1p" title="torch.log1p"><code>torch.log1p()</code></a></p>
<hr>
<pre><code>log1p_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code>log1p()</code></a></p>
<hr>
<pre><code>log2() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.log2" title="torch.log2"><code>torch.log2()</code></a></p>
<hr>
<pre><code>log2_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.log2" title="torch.Tensor.log2"><code>log2()</code></a></p>
<hr>
<pre><code>log_normal_(mean=1, std=2, *, generator=None)
</code></pre><p>&#x7528;&#x5BF9;&#x6570;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x4E2D;&#x7531;&#x7ED9;&#x5B9A;&#x5E73;&#x5747;&#x503C;<img src="img/fba1f12214374ac856c1e4be142c9dff.jpg" alt="">&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;<img src="img/cc84e998f72a1c5a0a5f736ba6a9ff34.jpg" alt="">&#x53C2;&#x6570;&#x5316;&#x7684;&#x6570;&#x5B57;&#x6837;&#x672C;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x3002; &#x8BF7;&#x6CE8;&#x610F;&#xFF0C; <a href="torch.html#torch.mean" title="torch.mean"><code>mean</code></a> &#x548C; <a href="torch.html#torch.std" title="torch.std"><code>std</code></a> &#x662F;&#x57FA;&#x7840;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x8FD4;&#x56DE;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#xFF1A;</p>
<p><script type="math/tex; ">f(x)=\frac {1}{x\sigma \sqrt {2\pi}}e^{-\frac {(lnx - \mu)^2}{2\sigma^2}}</script></p>
<hr>
<pre><code>logsumexp(dim, keepdim=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.logsumexp" title="torch.logsumexp"><code>torch.logsumexp()</code></a></p>
<hr>
<pre><code>logical_not() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.logical_not" title="torch.logical_not"><code>torch.logical_not()</code></a></p>
<hr>
<pre><code>logical_not_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.logical_not" title="torch.Tensor.logical_not"><code>logical_not()</code></a></p>
<hr>
<pre><code>logical_xor() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.logical_xor" title="torch.logical_xor"><code>torch.logical_xor()</code></a></p>
<hr>
<pre><code>logical_xor_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.logical_xor" title="torch.Tensor.logical_xor"><code>logical_xor()</code></a></p>
<hr>
<pre><code>long() &#x2192; Tensor
</code></pre><p><code>self.long()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.int64)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>lstsq(A) -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lstsq" title="torch.lstsq"><code>torch.lstsq()</code></a></p>
<hr>
<pre><code>lt(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lt" title="torch.lt"><code>torch.lt()</code></a></p>
<hr>
<pre><code>lt_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.lt" title="torch.Tensor.lt"><code>lt()</code></a></p>
<hr>
<pre><code>lu(pivot=True, get_infos=False)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lu" title="torch.lu"><code>torch.lu()</code></a></p>
<hr>
<pre><code>lu_solve(LU_data, LU_pivots) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.lu_solve" title="torch.lu_solve"><code>torch.lu_solve()</code></a></p>
<hr>
<pre><code>map_(tensor, callable)
</code></pre><p>&#x5BF9;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x548C;&#x7ED9;&#x5B9A;&#x7684; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x5E94;&#x7528;<code>callable</code>&#xFF0C;&#x5E76;&#x5C06;&#x7ED3;&#x679C;&#x5B58;&#x50A8;&#x5728;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x3002; <code>self</code>&#x5F20;&#x91CF;&#x548C;&#x7ED9;&#x5B9A;&#x7684; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x5FC5;&#x987B;&#x662F;<a href="&#x6CE8;&#x610F;s/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p><code>callable</code>&#x5E94;&#x5177;&#x6709;&#x7B7E;&#x540D;&#xFF1A;</p>
<pre><code>def callable(a, b) -&gt; number
</code></pre><hr>
<pre><code>masked_scatter_(mask, source)
</code></pre><p>&#x5728;<code>mask</code>&#x4E3A; True &#x7684;&#x4F4D;&#x7F6E;&#x5C06;&#x5143;&#x7D20;&#x4ECE;<code>source</code>&#x590D;&#x5236;&#x5230;<code>self</code>&#x5F20;&#x91CF;&#x3002; <code>mask</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="&#x6CE8;&#x610F;s/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E76;&#x5177;&#x6709;&#x57FA;&#x7840;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002; <code>source</code>&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x81F3;&#x5C11;&#x5E94;&#x4E0E;<code>mask</code>&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x4E00;&#x6837;&#x591A;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>&#x63A9;&#x7801;</strong> (<a href="#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>)&#x2013;&#x5E03;&#x5C14;&#x63A9;&#x7801;</p>
</li>
<li><p><strong>&#x6E90;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x4ECE;&#x4E2D;&#x590D;&#x5236;&#x7684;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>&#x6CE8;&#x610F;</p>
<p><code>mask</code>&#x5728;<code>self</code>&#x5F20;&#x91CF;&#x4E0A;&#x8FD0;&#x884C;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7684;<code>source</code>&#x5F20;&#x91CF;&#x4E0A;&#x8FD0;&#x884C;&#x3002;</p>
<hr>
<pre><code>masked_scatter(mask, tensor) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code>torch.Tensor.masked_scatter_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>masked_fill_(mask, value)
</code></pre><p>&#x7528;<code>value</code>&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x5176;&#x4E2D;<code>mask</code>&#x4E3A; True&#x3002; <code>mask</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="&#x6CE8;&#x610F;s/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E76;&#x5177;&#x6709;&#x57FA;&#x7840;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>mask</strong> (<a href="#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>) &#x2013; the boolean mask</p>
</li>
<li><p><strong>&#x503C;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x8981;&#x586B;&#x5199;&#x7684;&#x503C;</p>
</li>
</ul>
<hr>
<pre><code>masked_fill(mask, value) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code>torch.Tensor.masked_fill_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>masked_select(mask) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.masked_select" title="torch.masked_select"><code>torch.masked_select()</code></a></p>
<hr>
<pre><code>matmul(tensor2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a></p>
<hr>
<pre><code>matrix_power(n) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.matrix_power" title="torch.matrix_power"><code>torch.matrix_power()</code></a></p>
<hr>
<pre><code>max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.max" title="torch.max"><code>torch.max()</code></a></p>
<hr>
<pre><code>mean(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mean" title="torch.mean"><code>torch.mean()</code></a></p>
<hr>
<pre><code>median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.median" title="torch.median"><code>torch.median()</code></a></p>
<hr>
<pre><code>min(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.min" title="torch.min"><code>torch.min()</code></a></p>
<hr>
<pre><code>mm(mat2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mm" title="torch.mm"><code>torch.mm()</code></a></p>
<hr>
<pre><code>mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mode" title="torch.mode"><code>torch.mode()</code></a></p>
<hr>
<pre><code>mul(value) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mul" title="torch.mul"><code>torch.mul()</code></a></p>
<hr>
<pre><code>mul_(value)
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.mul" title="torch.Tensor.mul"><code>mul()</code></a></p>
<hr>
<pre><code>multinomial(num_samples, replacement=False, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a></p>
<hr>
<pre><code>mv(vec) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mv" title="torch.mv"><code>torch.mv()</code></a></p>
<hr>
<pre><code>mvlgamma(p) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.mvlgamma" title="torch.mvlgamma"><code>torch.mvlgamma()</code></a></p>
<hr>
<pre><code>mvlgamma_(p) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code>mvlgamma()</code></a></p>
<hr>
<pre><code>narrow(dimension, start, length) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.narrow" title="torch.narrow"><code>torch.narrow()</code></a></p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; x.narrow(0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
&gt;&gt;&gt; x.narrow(1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])
</code></pre><hr>
<pre><code>narrow_copy(dimension, start, length) &#x2192; Tensor
</code></pre><p>&#x4E0E; <a href="#torch.Tensor.narrow" title="torch.Tensor.narrow"><code>Tensor.narrow()</code></a> &#x76F8;&#x540C;&#xFF0C;&#x53EA;&#x662F;&#x8FD4;&#x56DE;&#x526F;&#x672C;&#x800C;&#x4E0D;&#x662F;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x3002; &#x8FD9;&#x4E3B;&#x8981;&#x7528;&#x4E8E;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#xFF0C;&#x5B83;&#x4EEC;&#x6CA1;&#x6709;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#x7684;&#x7A84;&#x65B9;&#x6CD5;&#x3002; &#x7528;<code>dimemsion &amp;gt; self.sparse_dim()</code>&#x8C03;&#x7528;<code>narrow_copy`&#x5C06;&#x8FD4;&#x56DE;&#x7F29;&#x5C0F;&#x4E86;&#x76F8;&#x5173;&#x5BC6;&#x96C6;&#x5C3A;&#x5BF8;&#x7684;&#x526F;&#x672C;&#xFF0C;&#x5E76;&#x76F8;&#x5E94;&#x5730;&#x66F4;&#x65B0;&#x4E86;</code>self.shape``&#x3002;</p>
<hr>
<pre><code>ndimension() &#x2192; int
</code></pre><p>Alias for <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim()</code></a></p>
<hr>
<pre><code>ne(other) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ne" title="torch.ne"><code>torch.ne()</code></a></p>
<hr>
<pre><code>ne_(other) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.ne" title="torch.Tensor.ne"><code>ne()</code></a></p>
<hr>
<pre><code>neg() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.neg" title="torch.neg"><code>torch.neg()</code></a></p>
<hr>
<pre><code>neg_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.neg" title="torch.Tensor.neg"><code>neg()</code></a></p>
<hr>
<pre><code>nelement() &#x2192; int
</code></pre><p><a href="#torch.Tensor.numel" title="torch.Tensor.numel"><code>numel()</code></a> &#x7684;&#x522B;&#x540D;</p>
<hr>
<pre><code>nonzero() &#x2192; LongTensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.nonzero" title="torch.nonzero"><code>torch.nonzero()</code></a></p>
<hr>
<pre><code>norm(p=&apos;fro&apos;, dim=None, keepdim=False, dtype=None)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.norm" title="torch.norm"><code>torch.norm()</code></a></p>
<hr>
<pre><code>normal_(mean=0, std=1, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;&#x7531; <a href="torch.html#torch.mean" title="torch.mean"><code>mean</code></a> &#x548C; <a href="torch.html#torch.std" title="torch.std"><code>std</code></a> &#x53C2;&#x6570;&#x5316;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x5143;&#x7D20;&#x6837;&#x672C;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>numel() &#x2192; int
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.numel" title="torch.numel"><code>torch.numel()</code></a></p>
<hr>
<pre><code>numpy() &#x2192; numpy.ndarray
</code></pre><p>&#x4EE5; NumPy <code>ndarray</code>&#x7684;&#x5F62;&#x5F0F;&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x3002; &#x8BE5;&#x5F20;&#x91CF;&#x548C;&#x8FD4;&#x56DE;&#x7684;<code>ndarray</code>&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002; &#x5BF9;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x66F4;&#x6539;&#x5C06;&#x53CD;&#x6620;&#x5728;<code>ndarray</code>&#x4E2D;&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x3002;</p>
<hr>
<pre><code>orgqr(input2) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.orgqr" title="torch.orgqr"><code>torch.orgqr()</code></a></p>
<hr>
<pre><code>ormqr(input2, input3, left=True, transpose=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.ormqr" title="torch.ormqr"><code>torch.ormqr()</code></a></p>
<hr>
<pre><code>permute(*dims) &#x2192; Tensor
</code></pre><p>&#x7F6E;&#x6362;&#x6B64;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>*dims</strong> (<em>python&#xFF1A;int ...</em> ) &#x2013; &#x6240;&#x9700;&#x7684;&#x7EF4;&#x5EA6;&#x987A;&#x5E8F;</p>
<p>&#x4F8B;:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3, 5)
&gt;&gt;&gt; x.size()
torch.Size([2, 3, 5])
&gt;&gt;&gt; x.permute(2, 0, 1).size()
torch.Size([5, 2, 3])
</code></pre><hr>
<pre><code>pin_memory() &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;&#x590D;&#x5236;&#x5230;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;(&#x5982;&#x679C;&#x5C1A;&#x672A;&#x56FA;&#x5B9A;&#xFF09;&#x3002;</p>
<hr>
<pre><code>pinverse() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.pinverse" title="torch.pinverse"><code>torch.pinverse()</code></a></p>
<hr>
<pre><code>polygamma(n) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.polygamma" title="torch.polygamma"><code>torch.polygamma()</code></a></p>
<hr>
<pre><code>polygamma_(n) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.polygamma" title="torch.Tensor.polygamma"><code>polygamma()</code></a></p>
<hr>
<pre><code>pow(exponent) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.pow" title="torch.pow"><code>torch.pow()</code></a></p>
<hr>
<pre><code>pow_(exponent) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.pow" title="torch.Tensor.pow"><code>pow()</code></a></p>
<hr>
<pre><code>prod(dim=None, keepdim=False, dtype=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.prod" title="torch.prod"><code>torch.prod()</code></a></p>
<hr>
<pre><code>put_(indices, tensor, accumulate=False) &#x2192; Tensor
</code></pre><p>&#x5C06; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x4E2D;&#x7684;&#x5143;&#x7D20;&#x590D;&#x5236;&#x5230;&#x7D22;&#x5F15;&#x6307;&#x5B9A;&#x7684;&#x4F4D;&#x7F6E;&#x3002; &#x4E3A;&#x4E86;&#x5EFA;&#x7ACB;&#x7D22;&#x5F15;&#xFF0C;&#x5C06;<code>self</code>&#x5F20;&#x91CF;&#x89C6;&#x4E3A;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>If <code>accumulate</code> is <code>True</code>, the elements in <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> are added to <code>self</code>. If accumulate is <code>False</code>, the behavior is undefined if indices contain duplicate elements.</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013;&#x81EA;&#x8EAB;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x8981;&#x590D;&#x5236;&#x7684;&#x503C;</p>
</li>
<li><p><strong>accumulate</strong> (<em>bool</em>) &#x2013; whether to accumulate into self</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
                        [6, 7, 8]])
&gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))
tensor([[  4,   9,   5],
        [ 10,   7,   8]])
</code></pre><hr>
<pre><code>qr(some=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.qr" title="torch.qr"><code>torch.qr()</code></a></p>
<hr>
<pre><code>qscheme() &#x2192; torch.qscheme
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A; QTensor &#x7684;&#x91CF;&#x5316;&#x65B9;&#x6848;&#x3002;</p>
<hr>
<pre><code>q_scale() &#x2192; float
</code></pre><p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x901A;&#x8FC7;&#x7EBF;&#x6027;(&#x4EFF;&#x5C04;&#xFF09;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x91CF;&#x5316;&#x5668;(&#xFF09;&#x7684;&#x6BD4;&#x4F8B;&#x5C3A;&#x3002;</p>
<hr>
<pre><code>q_zero_point() &#x2192; int
</code></pre><p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x901A;&#x8FC7;&#x7EBF;&#x6027;(&#x4EFF;&#x5C04;&#xFF09;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x91CF;&#x5316;&#x5668;(&#xFF09;&#x7684; zero_point&#x3002;</p>
<hr>
<pre><code>q_per_channel_scales() &#x2192; Tensor
</code></pre><p>&#x7ED9;&#x5B9A;&#x901A;&#x8FC7;&#x7EBF;&#x6027;(&#x4EFF;&#x5C04;&#xFF09;&#x6BCF;&#x901A;&#x9053;&#x91CF;&#x5316;&#x8FDB;&#x884C;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x91CF;&#x5316;&#x5668;&#x7684;&#x6BD4;&#x4F8B;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5B83;&#x5177;&#x6709;&#x4E0E;&#x5F20;&#x91CF;&#x7684;&#x76F8;&#x5E94;&#x5C3A;&#x5BF8;(&#x6765;&#x81EA; q_per_channel_axis&#xFF09;&#x5339;&#x914D;&#x7684;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x3002;</p>
<hr>
<pre><code>q_per_channel_zero_points() &#x2192; Tensor
</code></pre><p>&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x901A;&#x8FC7;&#x7EBF;&#x6027;(&#x4EFF;&#x5C04;&#xFF09;&#x6BCF;&#x901A;&#x9053;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x91CF;&#x5316;&#x5668;&#x7684; zero_points &#x5F20;&#x91CF;&#x3002; &#x5B83;&#x5177;&#x6709;&#x4E0E;&#x5F20;&#x91CF;&#x7684;&#x76F8;&#x5E94;&#x5C3A;&#x5BF8;(&#x6765;&#x81EA; q_per_channel_axis&#xFF09;&#x5339;&#x914D;&#x7684;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x3002;</p>
<hr>
<pre><code>q_per_channel_axis() &#x2192; int
</code></pre><p>&#x7ED9;&#x5B9A;&#x901A;&#x8FC7;&#x7EBF;&#x6027;(&#x4EFF;&#x5C04;&#xFF09;&#x6BCF;&#x901A;&#x9053;&#x91CF;&#x5316;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8FD4;&#x56DE;&#x5728;&#x5176;&#x4E0A;&#x5E94;&#x7528;&#x6BCF;&#x901A;&#x9053;&#x91CF;&#x5316;&#x7684;&#x5C3A;&#x5BF8;&#x7D22;&#x5F15;&#x3002;</p>
<hr>
<pre><code>random_(from=0, to=None, *, generator=None) &#x2192; Tensor
</code></pre><p>&#x7528;&#x4ECE;<code>[from, to - 1]</code>&#x4E0A;&#x7684;&#x79BB;&#x6563;&#x5747;&#x5300;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x8FD9;&#x4E9B;&#x503C;&#x901A;&#x5E38;&#x4EC5;&#x53D7;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x9650;&#x5236;&#x3002; &#x4F46;&#x662F;&#xFF0C;&#x5BF9;&#x4E8E;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#xFF0C;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#xFF0C;&#x8303;&#x56F4;&#x5C06;&#x4E3A;<code>[0, 2^mantissa]</code>&#x4EE5;&#x786E;&#x4FDD;&#x6BCF;&#x4E2A;&#x503C;&#x90FD;&#x662F;&#x53EF;&#x8868;&#x793A;&#x7684;&#x3002; &#x4F8B;&#x5982;&#xFF0C; &lt;cite&gt;torch.tensor(1&#xFF0C;dtype = torch.double&#xFF09;.random_(&#xFF09;&lt;/cite&gt;&#x5728;<code>[0, 2^53]</code>&#x4E2D;&#x5C06;&#x662F;&#x7EDF;&#x4E00;&#x7684;&#x3002;</p>
<hr>
<pre><code>reciprocal() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.reciprocal" title="torch.reciprocal"><code>torch.reciprocal()</code></a></p>
<hr>
<pre><code>reciprocal_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code>reciprocal()</code></a></p>
<hr>
<pre><code>record_stream(stream)
</code></pre><p>&#x786E;&#x4FDD;&#x5728;<code>stream</code>&#x4E0A;&#x6392;&#x961F;&#x7684;&#x6240;&#x6709;&#x5F53;&#x524D;&#x5DE5;&#x4F5C;&#x5B8C;&#x6210;&#x4E4B;&#x524D;&#xFF0C;&#x5F20;&#x91CF;&#x5B58;&#x50A8;&#x5668;&#x4E0D;&#x4F1A;&#x88AB;&#x5176;&#x4ED6;&#x5F20;&#x91CF;&#x91CD;&#x7528;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x7F13;&#x5B58;&#x5206;&#x914D;&#x5668;&#x4EC5;&#x77E5;&#x9053;&#x5206;&#x914D;&#x5F20;&#x91CF;&#x7684;&#x6D41;&#x3002; &#x7531;&#x4E8E;&#x6709;&#x4E86;&#x8FD9;&#x79CD;&#x8BA4;&#x8BC6;&#xFF0C;&#x5B83;&#x5DF2;&#x7ECF;&#x53EF;&#x4EE5;&#x4EC5;&#x5728;&#x4E00;&#x4E2A;&#x6D41;&#x4E0A;&#x6B63;&#x786E;&#x7BA1;&#x7406;&#x5F20;&#x91CF;&#x7684;&#x751F;&#x547D;&#x5468;&#x671F;&#x3002; &#x4F46;&#x662F;&#xFF0C;&#x5982;&#x679C;&#x5728;&#x4E0E;&#x539F;&#x59CB;&#x6D41;&#x4E0D;&#x540C;&#x7684;&#x6D41;&#x4E0A;&#x4F7F;&#x7528;&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x5206;&#x914D;&#x5668;&#x53EF;&#x80FD;&#x4F1A;&#x610F;&#x5916;&#x5730;&#x91CD;&#x7528;&#x5185;&#x5B58;&#x3002; &#x8C03;&#x7528;&#x6B64;&#x65B9;&#x6CD5;&#x53EF;&#x8BA9;&#x5206;&#x914D;&#x5668;&#x77E5;&#x9053;&#x54EA;&#x4E9B;&#x6D41;&#x4F7F;&#x7528;&#x4E86;&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>register_hook(hook)
</code></pre><p>&#x6CE8;&#x518C;&#x4E00;&#x4E2A;&#x5012;&#x94A9;&#x3002;</p>
<p>&#x6BCF;&#x5F53;&#x8BA1;&#x7B97;&#x76F8;&#x5BF9;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x68AF;&#x5EA6;&#x65F6;&#xFF0C;&#x90FD;&#x4F1A;&#x8C03;&#x7528;&#x8BE5;&#x6302;&#x94A9;&#x3002; &#x6302;&#x94A9;&#x5E94;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x7B7E;&#x540D;&#xFF1A;</p>
<pre><code>hook(grad) -&gt; Tensor or None
</code></pre><p>&#x6302;&#x94A9;&#x4E0D;&#x5E94;&#x4FEE;&#x6539;&#x5176;&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x4F46;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x6E10;&#x53D8;&#xFF0C;&#x8BE5;&#x6E10;&#x53D8;&#x5C06;&#x4EE3;&#x66FF; <a href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> &#x4F7F;&#x7528;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5E26;&#x6709;&#x65B9;&#x6CD5;<code>handle.remove()</code>&#x7684;&#x53E5;&#x67C4;&#xFF0C;&#x8BE5;&#x65B9;&#x6CD5;&#x53EF;&#x5C06;&#x94A9;&#x5B50;&#x4ECE;&#x6A21;&#x5757;&#x4E2D;&#x79FB;&#x9664;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)
&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient
&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))
&gt;&gt;&gt; v.grad

 2
 4
 6
[torch.FloatTensor of size (3,)]

&gt;&gt;&gt; h.remove()  # removes the hook
</code></pre><hr>
<pre><code>remainder(divisor) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.remainder" title="torch.remainder"><code>torch.remainder()</code></a></p>
<hr>
<pre><code>remainder_(divisor) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code>remainder()</code></a></p>
<hr>
<pre><code>real() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.real" title="torch.real"><code>torch.real()</code></a></p>
<hr>
<pre><code>renorm(p, dim, maxnorm) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.renorm" title="torch.renorm"><code>torch.renorm()</code></a></p>
<hr>
<pre><code>renorm_(p, dim, maxnorm) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code>renorm()</code></a></p>
<hr>
<pre><code>repeat(*sizes) &#x2192; Tensor
</code></pre><p>&#x6CBF;&#x6307;&#x5B9A;&#x5C3A;&#x5BF8;&#x91CD;&#x590D;&#x6B64;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x4E0E; <a href="#torch.Tensor.expand" title="torch.Tensor.expand"><code>expand()</code></a> &#x4E0D;&#x540C;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x590D;&#x5236;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p><code>torch.repeat()</code>&#x7684;&#x884C;&#x4E3A;&#x4E0E; <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html" target="_blank">numpy.repeat</a> &#x4E0D;&#x540C;&#xFF0C;&#x4F46;&#x66F4;&#x7C7B;&#x4F3C;&#x4E8E; <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html" target="_blank">numpy.tile</a> &#x3002; &#x5BF9;&#x4E8E;&#x7C7B;&#x4F3C;&#x4E8E; &lt;cite&gt;numpy.repeat&lt;/cite&gt; &#x7684;&#x8FD0;&#x7B97;&#x7B26;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"><code>torch.repeat_interleave()</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5927;&#x5C0F;</strong>(<em>torch&#x5927;&#x5C0F;</em> <em>&#x6216;</em> <em>python&#xFF1A;int ...</em> )&#x2013;&#x5728;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E0A;&#x91CD;&#x590D;&#x6B64;&#x5F20;&#x91CF;&#x7684;&#x6B21;&#x6570;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat(4, 2)
tensor([[ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3]])
&gt;&gt;&gt; x.repeat(4, 2, 1).size()
torch.Size([4, 2, 3])
</code></pre><hr>
<pre><code>repeat_interleave(repeats, dim=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.repeat_interleave" title="torch.repeat_interleave"><code>torch.repeat_interleave()</code></a> &#x3002;</p>
<pre><code>requires_grad
</code></pre><p>&#x5982;&#x679C;&#x9700;&#x8981;&#x4E3A;&#x6B64;&#x5F20;&#x91CF;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#xFF0C;&#x5219;&#x4E3A;<code>True</code>&#xFF0C;&#x5426;&#x5219;&#x4E3A;<code>False</code>&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x9700;&#x8981;&#x4E3A;&#x5F20;&#x91CF;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x7684;&#x4E8B;&#x5B9E;&#x5E76;&#x4E0D;&#x610F;&#x5473;&#x7740;&#x5C06;&#x586B;&#x5145; <a href="autograd.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> &#x5C5E;&#x6027;&#xFF0C;&#x6709;&#x5173;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="autograd.html#torch.Tensor.is_leaf" title="torch.Tensor.is_leaf"><code>is_leaf</code></a> &#x3002;</p>
<hr>
<pre><code>requires_grad_(requires_grad=True) &#x2192; Tensor
</code></pre><p>&#x66F4;&#x6539; autograd &#x662F;&#x5426;&#x5E94;&#x8BB0;&#x5F55;&#x8BE5;&#x5F20;&#x91CF;&#x4E0A;&#x7684;&#x64CD;&#x4F5C;&#xFF1A;&#x9002;&#x5F53;&#x5730;&#x8BBE;&#x7F6E;&#x6B64;&#x5F20;&#x91CF;&#x7684; <a href="autograd.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code>requires_grad</code></a> &#x5C5E;&#x6027;&#x3002; &#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x91CF;&#x3002;</p>
<p><a href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>requires_grad_()</code></a> &#x7684;&#x4E3B;&#x8981;&#x7528;&#x4F8B;&#x662F;&#x544A;&#x8BC9; autograd &#x5728; Tensor <code>tensor</code>&#x4E0A;&#x5F00;&#x59CB;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002; &#x5982;&#x679C;<code>tensor</code>&#x5177;&#x6709;<code>requires_grad=False</code>(&#x56E0;&#x4E3A;&#x5B83;&#x662F;&#x901A;&#x8FC7; DataLoader &#x83B7;&#x5F97;&#x7684;&#xFF0C;&#x6216;&#x8005;&#x9700;&#x8981;&#x8FDB;&#x884C;&#x9884;&#x5904;&#x7406;&#x6216;&#x521D;&#x59CB;&#x5316;&#xFF09;&#xFF0C;&#x5219;<code>tensor.requires_grad_()</code>&#x5C06;&#x5176;&#x8BBE;&#x7F6E;&#x4E3A;&#x4F7F; autograd &#x5C06;&#x5F00;&#x59CB;&#x5728;<code>tensor</code>&#x4E0A;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>require_grad</strong>  (<em>bool</em> )&#x2013;&#x5982;&#x679C; autograd &#x5E94;&#x8BE5;&#x5728;&#x8BE5;&#x5F20;&#x91CF;&#x4E0A;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code>&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; # Let&apos;s say we want to preprocess some saved weights and use
&gt;&gt;&gt; # the result as new weights.
&gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25]
&gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights)
&gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function
&gt;&gt;&gt; weights
tensor([-0.5503,  0.4926, -2.1158, -0.8303])

&gt;&gt;&gt; # Now, start to record operations done to weights
&gt;&gt;&gt; weights.requires_grad_()
&gt;&gt;&gt; out = weights.pow(2).sum()
&gt;&gt;&gt; out.backward()
&gt;&gt;&gt; weights.grad
tensor([-1.1007,  0.9853, -4.2316, -1.6606])
</code></pre><hr>
<pre><code>reshape(*shape) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x4E0E;<code>self</code>&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x548C;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x4F46;&#x5177;&#x6709;&#x6307;&#x5B9A;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>shape</code>&#x4E0E;&#x5F53;&#x524D;&#x5F62;&#x72B6;&#x517C;&#x5BB9;&#xFF0C;&#x5219;&#x6B64;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x89C6;&#x56FE;&#x3002; &#x5F53;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x89C6;&#x56FE;&#x65F6;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> &#x3002;</p>
<p>&#x53C2;&#x89C1; <a href="torch.html#torch.reshape" title="torch.reshape"><code>torch.reshape()</code></a></p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5F62;&#x72B6;</strong> (<em>python&#xFF1A;ints &#x7684;&#x5143;&#x7EC4;</em> <em>&#x6216;</em> <em>python&#xFF1A;int ...</em> )&#x2013;&#x6240;&#x9700;&#x7684;&#x5F62;&#x72B6;</p>
<hr>
<pre><code>reshape_as(other) &#x2192; Tensor
</code></pre><p>&#x4EE5;&#x4E0E;<code>other</code>&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x8FD4;&#x56DE;&#x6B64;&#x5F20;&#x91CF;&#x3002; <code>self.reshape_as(other)</code>&#x7B49;&#x6548;&#x4E8E;<code>self.reshape(other.sizes())</code>&#x3002; &#x5982;&#x679C;<code>other.sizes()</code>&#x4E0E;&#x5F53;&#x524D;&#x5F62;&#x72B6;&#x517C;&#x5BB9;&#xFF0C;&#x5219;&#x6B64;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x89C6;&#x56FE;&#x3002; &#x4F55;&#x65F6;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x89C6;&#x56FE;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> &#x3002;</p>
<p>&#x6709;&#x5173;<code>reshape</code>&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="torch.html#torch.reshape" title="torch.reshape"><code>reshape()</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5176;&#x4ED6;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>)&#x2013;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E0E;<code>other</code>&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<hr>
<pre><code>resize_(*sizes) &#x2192; Tensor
</code></pre><p>&#x5C06;<code>self</code>&#x5F20;&#x91CF;&#x8C03;&#x6574;&#x4E3A;&#x6307;&#x5B9A;&#x5927;&#x5C0F;&#x3002; &#x5982;&#x679C;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x5927;&#x4E8E;&#x5F53;&#x524D;&#x5B58;&#x50A8;&#x5927;&#x5C0F;&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x8C03;&#x6574;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x7684;&#x5927;&#x5C0F;&#x4EE5;&#x9002;&#x5408;&#x65B0;&#x7684;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x3002; &#x5982;&#x679C;&#x5143;&#x7D20;&#x6570;&#x8F83;&#x5C0F;&#xFF0C;&#x5219;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x4E0D;&#x4F1A;&#x66F4;&#x6539;&#x3002; &#x73B0;&#x6709;&#x5143;&#x7D20;&#x5C06;&#x4FDD;&#x7559;&#xFF0C;&#x4F46;&#x4EFB;&#x4F55;&#x65B0;&#x5185;&#x5B58;&#x5747;&#x672A;&#x521D;&#x59CB;&#x5316;&#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p>&#x8FD9;&#x662F;&#x4E00;&#x79CD;&#x5E95;&#x5C42;&#x65B9;&#x6CD5;&#x3002; &#x5C06;&#x5B58;&#x50A8;&#x91CD;&#x65B0;&#x89E3;&#x91CA;&#x4E3A; C &#x8FDE;&#x7EED;&#x7684;&#xFF0C;&#x800C;&#x5FFD;&#x7565;&#x5F53;&#x524D;&#x6B65;&#x5E45;(&#x9664;&#x975E;&#x76EE;&#x6807;&#x5927;&#x5C0F;&#x7B49;&#x4E8E;&#x5F53;&#x524D;&#x5927;&#x5C0F;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5F20;&#x91CF;&#x4FDD;&#x6301;&#x4E0D;&#x53D8;&#xFF09;&#x3002; &#x5BF9;&#x4E8E;&#x5927;&#x591A;&#x6570;&#x76EE;&#x7684;&#xFF0C;&#x60A8;&#x5C06;&#x6539;&#x4E3A;&#x4F7F;&#x7528; <a href="#torch.Tensor.view" title="torch.Tensor.view"><code>view()</code></a> (&#x68C0;&#x67E5;&#x8FDE;&#x7EED;&#x6027;&#xFF09;&#xFF0C;&#x6216;&#x4F7F;&#x7528; <a href="#torch.Tensor.reshape" title="torch.Tensor.reshape"><code>reshape()</code></a> (&#x5982;&#x679C;&#x9700;&#x8981;&#xFF0C;&#x53EF;&#x590D;&#x5236;&#x6570;&#x636E;&#xFF09;&#x3002; &#x8981;&#x4F7F;&#x7528;&#x81EA;&#x5B9A;&#x4E49;&#x6B65;&#x5E45;&#x5C31;&#x5730;&#x66F4;&#x6539;&#x5927;&#x5C0F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.set_" title="torch.Tensor.set_"><code>set_()</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5927;&#x5C0F;</strong>(<em>torch&#x5927;&#x5C0F;</em> <em>&#x6216;</em> <em>python&#xFF1A;int ...</em> )&#x2013;&#x6240;&#x9700;&#x5927;&#x5C0F;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]])
&gt;&gt;&gt; x.resize_(2, 2)
tensor([[ 1,  2],
        [ 3,  4]])
</code></pre><hr>
<pre><code>resize_as_(tensor) &#x2192; Tensor
</code></pre><p>&#x5C06;<code>self</code>&#x5F20;&#x91CF;&#x8C03;&#x6574;&#x4E3A;&#x4E0E;&#x6307;&#x5B9A;&#x7684; <a href="torch.html#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x8FD9;&#x7B49;&#x6548;&#x4E8E;<code>self.resize_(tensor.size())</code>&#x3002;</p>
<hr>
<pre><code>retain_grad()
</code></pre><p>&#x4E3A;&#x975E;&#x53F6;&#x5F20;&#x91CF;&#x542F;&#x7528;.grad &#x5C5E;&#x6027;&#x3002;</p>
<hr>
<pre><code>rfft(signal_ndim, normalized=False, onesided=True) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.rfft" title="torch.rfft"><code>torch.rfft()</code></a></p>
<hr>
<pre><code>roll(shifts, dims) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.roll" title="torch.roll"><code>torch.roll()</code></a></p>
<hr>
<pre><code>rot90(k, dims) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.rot90" title="torch.rot90"><code>torch.rot90()</code></a></p>
<hr>
<pre><code>round() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.round" title="torch.round"><code>torch.round()</code></a></p>
<hr>
<pre><code>round_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.round" title="torch.Tensor.round"><code>round()</code></a></p>
<hr>
<pre><code>rsqrt() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.rsqrt" title="torch.rsqrt"><code>torch.rsqrt()</code></a></p>
<hr>
<pre><code>rsqrt_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code>rsqrt()</code></a></p>
<hr>
<pre><code>scatter(dim, index, source) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code>torch.Tensor.scatter_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>scatter_(dim, index, src) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;<code>src</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x503C;&#x5199;&#x5165;<code>index</code>&#x5F20;&#x91CF;&#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;&#x5904;&#x7684;<code>self</code>&#x4E2D;&#x3002; &#x5BF9;&#x4E8E;<code>src</code>&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x503C;&#xFF0C;&#x5176;&#x8F93;&#x51FA;&#x7D22;&#x5F15;&#x7531;<code>dimension != dim</code>&#x7684;<code>src</code>&#x4E2D;&#x7684;&#x7D22;&#x5F15;&#x4EE5;&#x53CA;<code>dimension = dim</code>&#x7684;<code>index</code>&#x4E2D;&#x7684;&#x76F8;&#x5E94;&#x503C;&#x6307;&#x5B9A;&#x3002;</p>
<p>&#x5BF9;&#x4E8E; 3-D &#x5F20;&#x91CF;&#xFF0C;<code>self</code>&#x66F4;&#x65B0;&#x4E3A;&#xFF1A;</p>
<pre><code>self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2
</code></pre><p>&#x8FD9;&#x662F; <a href="#torch.Tensor.gather" title="torch.Tensor.gather"><code>gather()</code></a> &#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x65B9;&#x5F0F;&#x7684;&#x76F8;&#x53CD;&#x64CD;&#x4F5C;&#x3002;</p>
<p><code>self</code>&#xFF0C;<code>index</code>&#x548C;<code>src</code>(&#x5982;&#x679C;&#x662F;&#x5F20;&#x91CF;&#xFF09;&#x5E94;&#x5177;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x8FD8;&#x8981;&#x6C42;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x5C3A;&#x5BF8;<code>d</code>&#x5747;&#x4E3A;<code>index.size(d) &amp;lt;= src.size(d)</code>&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x5C3A;&#x5BF8;<code>d != dim</code>&#x5747;&#x8981;&#x6C42;<code>index.size(d) &amp;lt;= self.size(d)</code>&#x3002;</p>
<p>&#x6B64;&#x5916;&#xFF0C;&#x5BF9;&#x4E8E; <a href="#torch.Tensor.gather" title="torch.Tensor.gather"><code>gather()</code></a> &#xFF0C;<code>index</code>&#x7684;&#x503C;&#x5FC5;&#x987B;&#x4ECB;&#x4E8E;<code>0</code>&#x548C;<code>self.size(dim) - 1</code>&#x4E4B;&#x95F4;&#xFF0C;&#x4E14;&#x6CBF;&#x6307;&#x5B9A;&#x5C3A;&#x5BF8; <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a> &#x5FC5;&#x987B;&#x662F;&#x552F;&#x4E00;&#x7684;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python&#xFF1A;int</em> ) &#x2013; &#x6CBF;&#x5176;&#x7D22;&#x5F15;&#x7684;&#x8F74;</p>
</li>
<li><p><strong>index</strong> (<em>LongTensor</em> ) &#x2013; &#x8981;&#x6563;&#x5E03;&#x7684;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x53EF;&#x4EE5;&#x4E3A;&#x7A7A;&#x6216; src &#x7684;&#x5927;&#x5C0F;&#x76F8;&#x540C;&#x3002; &#x5982;&#x679C;&#x4E3A;&#x7A7A;&#xFF0C;&#x5219;&#x64CD;&#x4F5C;&#x8FD4;&#x56DE;&#x6807;&#x8BC6;</p>
</li>
<li><p><strong>src</strong>  (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>) &#x2013; &#x8981;&#x6563;&#x5E03;&#x7684;&#x6E90;&#x5143;&#x7D20;&#xFF0C;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;<em>&#x503C;</em></p>
</li>
<li><p><strong>value</strong> (<em>python&#xFF1A;float</em> ) &#x2013; &#x8981;&#x5206;&#x6563;&#x7684;&#x6E90;&#x5143;&#x7D20;&#xFF0C;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;<em>src</em></p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)
&gt;&gt;&gt; x
tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],
        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])
&gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)
tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],
        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],
        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])

&gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)
&gt;&gt;&gt; z
tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.2300]])
</code></pre><hr>
<pre><code>scatter_add_(dim, index, other) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;<code>other</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x503C;&#x6DFB;&#x52A0;&#x5230;<code>index</code>&#x5F20;&#x91CF;&#x4E2D;&#x6307;&#x5B9A;&#x7684;&#x7D22;&#x5F15;&#x5904;&#x7684;<code>self</code>&#x4E2D;&#xFF0C;&#x5176;&#x65B9;&#x5F0F;&#x4E0E; <a href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code>scatter_()</code></a> &#x76F8;&#x4F3C;&#x3002; &#x5BF9;&#x4E8E;<code>other</code>&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x503C;&#xFF0C;&#x5C06;&#x5176;&#x6DFB;&#x52A0;&#x5230;<code>self</code>&#x4E2D;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x8BE5;&#x7D22;&#x5F15;&#x7531;<code>dimension != dim</code>&#x4E2D;&#x7684;<code>other</code>&#x4E2D;&#x7684;&#x7D22;&#x5F15;&#x548C;<code>dimension = dim</code>&#x4E2D;&#x7684;<code>index</code>&#x4E2D;&#x7684;&#x5BF9;&#x5E94;&#x503C;&#x6307;&#x5B9A;&#x3002;</p>
<p>For a 3-D tensor, <code>self</code> is updated as:</p>
<pre><code>self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2
</code></pre><p><code>self</code>&#xFF0C;<code>index</code>&#x548C;<code>other</code>&#x5E94;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5C3A;&#x5BF8;&#x6570;&#x3002; &#x8FD8;&#x8981;&#x6C42;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x5C3A;&#x5BF8;<code>d</code>&#x5747;&#x4E3A;<code>index.size(d) &amp;lt;= other.size(d)</code>&#xFF0C;&#x5E76;&#x4E14;&#x5BF9;&#x4E8E;&#x6240;&#x6709;&#x5C3A;&#x5BF8;<code>d != dim</code>&#x5747;&#x8981;&#x6C42;<code>index.size(d) &amp;lt;= self.size(d)</code>&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the &#x6CE8;&#x610F;s on <a href="&#x6CE8;&#x610F;s/randomness.html">Reproducibility</a> for background.</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; the axis along which to index</p>
</li>
<li><p><strong>index</strong> (<em>LongTensor</em> ) &#x2013; &#x5206;&#x6563;&#x548C;&#x6DFB;&#x52A0;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x53EF;&#x4EE5;&#x4E3A;&#x7A7A;&#x6216; src &#x5927;&#x5C0F;&#x76F8;&#x540C;&#x3002; &#x4E3A;&#x7A7A;&#x65F6;&#xFF0C;&#x8BE5;&#x64CD;&#x4F5C;&#x5C06;&#x8FD4;&#x56DE;&#x6807;&#x8BC6;&#x3002;</p>
</li>
<li><p><strong>other</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>) &#x2013; &#x5206;&#x6563;&#x548C;&#x6DFB;&#x52A0;&#x7684;&#x6E90;&#x5143;&#x7D20;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)
&gt;&gt;&gt; x
tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],
        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])
&gt;&gt;&gt; torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)
tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],
        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],
        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])
</code></pre><hr>
<pre><code>scatter_add(dim, index, source) &#x2192; Tensor
</code></pre><p><a href="#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code>torch.Tensor.scatter_add_()</code></a> &#x7684;&#x66FF;&#x4EE3;&#x7248;&#x672C;</p>
<hr>
<pre><code>select(dim, index) &#x2192; Tensor
</code></pre><p>&#x6CBF;&#x9009;&#x5B9A;&#x7EF4;&#x5EA6;&#x5728;&#x7ED9;&#x5B9A;&#x7D22;&#x5F15;&#x5904;&#x5207;&#x7247;<code>self</code>&#x5F20;&#x91CF;&#x3002; &#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x88AB;&#x79FB;&#x9664;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5207;&#x7247;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x9009;&#x62E9;&#x7684;&#x7D22;&#x5F15;</p>
</li>
</ul>
<p>&#x6CE8;&#x610F;</p>
<p><a href="#torch.Tensor.select" title="torch.Tensor.select"><code>select()</code></a> &#x76F8;&#x5F53;&#x4E8E;&#x5207;&#x7247;&#x3002; &#x4F8B;&#x5982;&#xFF0C;<code>tensor.select(0, index)</code>&#x7B49;&#x6548;&#x4E8E;<code>tensor[index]</code>&#xFF0C;<code>tensor.select(2, index)</code>&#x7B49;&#x6548;&#x4E8E;<code>tensor[:,:,index]</code>&#x3002;</p>
<hr>
<pre><code>set_(source=None, storage_offset=0, size=None, stride=None) &#x2192; Tensor
</code></pre><p>&#x8BBE;&#x7F6E;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x7A7A;&#x95F4;&#xFF0C;&#x5927;&#x5C0F;&#x548C;&#x8DE8;&#x5EA6;&#x3002; &#x5982;&#x679C;<code>source</code>&#x662F;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>self</code>&#x5F20;&#x91CF;&#x5C06;&#x4E0E;<code>source</code>&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x7A7A;&#x95F4;&#x5E76;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x548C;&#x8DE8;&#x5EA6;&#x3002; &#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x4E2D;&#x5143;&#x7D20;&#x7684;&#x53D8;&#x5316;&#x5C06;&#x53CD;&#x6620;&#x5728;&#x53E6;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x4E2D;&#x3002;</p>
<p>&#x5982;&#x679C;<code>source</code>&#x662F;<code>Storage</code>&#xFF0C;&#x5219;&#x8BE5;&#x65B9;&#x6CD5;&#x8BBE;&#x7F6E;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#xFF0C;&#x504F;&#x79FB;&#xFF0C;&#x5927;&#x5C0F;&#x548C;&#x8DE8;&#x5EA6;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>&#x6E90;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>&#x5B58;&#x50A8;&#x5668;</em>&#xFF09;&#x2013;&#x4F7F;&#x7528;&#x7684;&#x5F20;&#x91CF;&#x6216;&#x5B58;&#x50A8;&#x5668;</p>
</li>
<li><p><strong>storage_offset</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5B58;&#x50A8;&#x4E2D;&#x7684;&#x504F;&#x79FB;&#x91CF;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong>(<em>torch&#x5927;&#x5C0F;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6240;&#x9700;&#x5927;&#x5C0F;&#x3002; &#x9ED8;&#x8BA4;&#x4E3A;&#x6E90;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>&#x6B65;&#x5E45;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6240;&#x9700;&#x7684;&#x6B65;&#x5E45;&#x3002; &#x9ED8;&#x8BA4;&#x4E3A; C &#x8FDE;&#x7EED;&#x8DE8;&#x6B65;&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>share_memory_()
</code></pre><p>&#x5C06;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x79FB;&#x52A8;&#x5230;&#x5171;&#x4EAB;&#x5185;&#x5B58;&#x3002;</p>
<p>&#x5982;&#x679C;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x5DF2;&#x7ECF;&#x5728;&#x5171;&#x4EAB;&#x5185;&#x5B58;&#x4E2D;&#x5E76;&#x4E14;&#x7528;&#x4E8E; CUDA &#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x6B64;&#x64CD;&#x4F5C;&#x4E0D;&#x53EF;&#x64CD;&#x4F5C;&#x3002; &#x5171;&#x4EAB;&#x5185;&#x5B58;&#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x65E0;&#x6CD5;&#x8C03;&#x6574;&#x5927;&#x5C0F;&#x3002;</p>
<hr>
<pre><code>short() &#x2192; Tensor
</code></pre><p><code>self.short()</code>&#x7B49;&#x6548;&#x4E8E;<code>self.to(torch.int16)</code>&#x3002; &#x53C2;&#x89C1; <a href="#torch.Tensor.to" title="torch.Tensor.to"><code>to()</code></a> &#x3002;</p>
<hr>
<pre><code>sigmoid() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sigmoid" title="torch.sigmoid"><code>torch.sigmoid()</code></a></p>
<hr>
<pre><code>sigmoid_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code>sigmoid()</code></a></p>
<hr>
<pre><code>sign() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sign" title="torch.sign"><code>torch.sign()</code></a></p>
<hr>
<pre><code>sign_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sign" title="torch.Tensor.sign"><code>sign()</code></a></p>
<hr>
<pre><code>sin() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sin" title="torch.sin"><code>torch.sin()</code></a></p>
<hr>
<pre><code>sin_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sin" title="torch.Tensor.sin"><code>sin()</code></a></p>
<hr>
<pre><code>sinh() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sinh" title="torch.sinh"><code>torch.sinh()</code></a></p>
<hr>
<pre><code>sinh_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code>sinh()</code></a></p>
<hr>
<pre><code>size() &#x2192; torch.Size
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x8FD4;&#x56DE;&#x7684;&#x503C;&#x662F;<code>tuple</code>&#x7684;&#x5B50;&#x7C7B;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.empty(3, 4, 5).size()
torch.Size([3, 4, 5])
</code></pre><hr>
<pre><code>slogdet() -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.slogdet" title="torch.slogdet"><code>torch.slogdet()</code></a></p>
<hr>
<pre><code>solve(A) &#x2192; Tensor, Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.solve" title="torch.solve"><code>torch.solve()</code></a></p>
<hr>
<pre><code>sort(dim=-1, descending=False) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sort" title="torch.sort"><code>torch.sort()</code></a></p>
<hr>
<pre><code>split(split_size, dim=0)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.split" title="torch.split"><code>torch.split()</code></a></p>
<hr>
<pre><code>sparse_mask(input, mask) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684; SparseTensor&#xFF0C;&#x5176; Tensor <code>input</code>&#x4E2D;&#x7684;&#x503C;&#x88AB;<code>mask</code>&#x7684;&#x7D22;&#x5F15;&#x8FC7;&#x6EE4;&#xFF0C;&#x5E76;&#x4E14;&#x503C;&#x88AB;&#x5FFD;&#x7565;&#x3002; <code>input</code>&#x548C;<code>mask</code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>input</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>mask</strong> (<em>SparseTensor</em> )&#x2013;&#x6211;&#x4EEC;&#x6839;&#x636E;&#x5176;&#x7D22;&#x5F15;&#x8FC7;&#x6EE4;<code>input</code>&#x7684; SparseTensor</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; nnz = 5
&gt;&gt;&gt; dims = [5, 5, 2, 2]
&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),
                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)
&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])
&gt;&gt;&gt; size = torch.Size(dims)
&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce()
&gt;&gt;&gt; D = torch.randn(dims)
&gt;&gt;&gt; D.sparse_mask(S)
tensor(indices=tensor([[0, 0, 0, 2],
                       [0, 1, 4, 3]]),
       values=tensor([[[ 1.6550,  0.2397],
                       [-0.1611, -0.0779]],

                      [[ 0.2326, -1.0558],
                       [ 1.4711,  1.9678]],

                      [[-0.5138, -0.0411],
                       [ 1.9417,  0.5158]],

                      [[ 0.0793,  0.0036],
                       [-0.2569, -0.1055]]]),
       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)
</code></pre><hr>
<pre><code>sparse_dim() &#x2192; int
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x662F;&#x7A00;&#x758F;&#x7684; COO &#x5F20;&#x91CF;(&#x5373;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7A00;&#x758F;&#x7EF4;&#x5EA6;&#x7684;&#x6570;&#x91CF;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code>Tensor.dense_dim()</code></a> &#x3002;</p>
<hr>
<pre><code>sqrt() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sqrt" title="torch.sqrt"><code>torch.sqrt()</code></a></p>
<hr>
<pre><code>sqrt_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code>sqrt()</code></a></p>
<hr>
<pre><code>squeeze(dim=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a></p>
<hr>
<pre><code>squeeze_(dim=None) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code>squeeze()</code></a></p>
<hr>
<pre><code>std(dim=None, unbiased=True, keepdim=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.std" title="torch.std"><code>torch.std()</code></a></p>
<hr>
<pre><code>stft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode=&apos;reflect&apos;, normalized=False, onesided=True)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.stft" title="torch.stft"><code>torch.stft()</code></a></p>
<p>&#x8B66;&#x544A;</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x5728;&#x7248;&#x672C; 0.4.1 &#x66F4;&#x6539;&#x4E86;&#x7B7E;&#x540D;&#x3002; &#x4F7F;&#x7528;&#x524D;&#x4E00;&#x4E2A;&#x7B7E;&#x540D;&#x8FDB;&#x884C;&#x8C03;&#x7528;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x6216;&#x8FD4;&#x56DE;&#x9519;&#x8BEF;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<hr>
<pre><code>storage() &#x2192; torch.Storage
</code></pre><p>&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002;</p>
<hr>
<pre><code>storage_offset() &#x2192; int
</code></pre><p>&#x6839;&#x636E;&#x5B58;&#x50A8;&#x5143;&#x7D20;&#x7684;&#x6570;&#x91CF;(&#x4E0D;&#x662F;&#x5B57;&#x8282;&#xFF09;&#xFF0C;&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x4E2D;&#x7684;<code>self</code>&#x5F20;&#x91CF;&#x504F;&#x79FB;&#x91CF;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5])
&gt;&gt;&gt; x.storage_offset()
0
&gt;&gt;&gt; x[3:].storage_offset()
3
</code></pre><hr>
<pre><code>storage_type() &#x2192; type
</code></pre><p>&#x8FD4;&#x56DE;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<hr>
<pre><code>stride(dim) &#x2192; tuple or int
</code></pre><p>&#x8FD4;&#x56DE;<code>self</code>&#x5F20;&#x91CF;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
<p>&#x8DE8;&#x5EA6;&#x662F;&#x5728;&#x6307;&#x5B9A;&#x5C3A;&#x5BF8; <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a> &#x4E2D;&#x4ECE;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x8DF3;&#x81F3;&#x4E0B;&#x4E00;&#x5143;&#x7D20;&#x6240;&#x9700;&#x7684;&#x8DF3;&#x8DC3;&#x3002; &#x5F53;&#x672A;&#x4F20;&#x9012;&#x4EFB;&#x4F55;&#x53C2;&#x6570;&#x65F6;&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6240;&#x6709;&#x8DE8;&#x5EA6;&#x7684;&#x5143;&#x7EC4;&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6574;&#x6570;&#x503C;&#x4F5C;&#x4E3A;&#x7279;&#x5B9A;&#x7EF4;&#x5EA6; <a href="#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a> &#x4E2D;&#x7684;&#x8DE8;&#x5EA6;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>dim</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x9700;&#x8981;&#x8DE8;&#x5EA6;&#x7684;&#x6240;&#x9700;&#x5C3A;&#x5BF8;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
&gt;&gt;&gt; x.stride()
(5, 1)
&gt;&gt;&gt;x.stride(0)
5
&gt;&gt;&gt; x.stride(-1)
1
</code></pre><hr>
<pre><code>sub(value, other) &#x2192; Tensor
</code></pre><p>&#x4ECE;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x51CF;&#x53BB;&#x6807;&#x91CF;&#x6216;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;&#x540C;&#x65F6;&#x6307;&#x5B9A;&#x4E86;<code>value</code>&#x548C;<code>other</code>&#xFF0C;&#x5219;&#x5728;&#x4F7F;&#x7528;&#x524D;<code>other</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x90FD;&#x4F1A;&#x6309;<code>value</code>&#x7F29;&#x653E;&#x3002;</p>
<p>&#x5F53;<code>other</code>&#x662F;&#x5F20;&#x91CF;&#x65F6;&#xFF0C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="&#x6CE8;&#x610F;s/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E76;&#x5177;&#x6709;&#x57FA;&#x7840;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<hr>
<pre><code>sub_(x) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.sub" title="torch.Tensor.sub"><code>sub()</code></a></p>
<hr>
<pre><code>sum(dim=None, keepdim=False, dtype=None) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.sum" title="torch.sum"><code>torch.sum()</code></a></p>
<hr>
<pre><code>sum_to_size(*size) &#x2192; Tensor
</code></pre><p>&#x5C06;<code>this</code>&#x5F20;&#x91CF;&#x4E0E; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x76F8;&#x52A0;&#x3002; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x5FC5;&#x987B;&#x53EF;&#x5E7F;&#x64AD;&#x5230;<code>this</code>&#x5F20;&#x91CF;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5927;&#x5C0F;</strong> (<em>python&#xFF1A;int ...</em> )&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002;</p>
<hr>
<pre><code>svd(some=True, compute_uv=True) -&gt; (Tensor, Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.svd" title="torch.svd"><code>torch.svd()</code></a></p>
<hr>
<pre><code>symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.symeig" title="torch.symeig"><code>torch.symeig()</code></a></p>
<hr>
<pre><code>t() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.t" title="torch.t"><code>torch.t()</code></a></p>
<hr>
<pre><code>t_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.t" title="torch.Tensor.t"><code>t()</code></a></p>
<hr>
<pre><code>to(*args, **kwargs) &#x2192; Tensor
</code></pre><p>&#x6267;&#x884C; Tensor dtype &#x548C;/&#x6216;&#x8BBE;&#x5907;&#x8F6C;&#x6362;&#x3002; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x662F;&#x4ECE;<code>self.to(*args, **kwargs)</code>&#x7684;&#x8BBA;&#x70B9;&#x63A8;&#x8BBA;&#x51FA;&#x6765;&#x7684;&#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>&#x5982;&#x679C;<code>self</code>&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x5177;&#x6709;&#x6B63;&#x786E;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x5219;&#x8FD4;&#x56DE;<code>self</code>&#x3002; &#x5426;&#x5219;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;<code>self</code>&#x4E0E;&#x6240;&#x9700; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x7684;&#x526F;&#x672C;&#x3002;</p>
<p>&#x4EE5;&#x4E0B;&#x662F;&#x8C03;&#x7528;<code>to</code>&#x7684;&#x65B9;&#x6CD5;&#xFF1A;</p>
<hr>
<pre><code>to(dtype, non_blocking=False, copy=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x6307;&#x5B9A;<code>dtype</code>&#x7684;&#x5F20;&#x91CF;</p>
<hr>
<pre><code>to(device=None, dtype=None, non_blocking=False, copy=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x6307;&#x5B9A; <a href="#torch.Tensor.device" title="torch.Tensor.device"><code>device</code></a> &#x548C;(&#x53EF;&#x9009;&#xFF09;<code>dtype</code>&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>dtype</code>&#x4E3A;<code>None</code>&#xFF0C;&#x5219;&#x63A8;&#x65AD;&#x4E3A;<code>self.dtype</code>&#x3002; &#x5F53;<code>non_blocking</code>&#x65F6;&#xFF0C;&#x5982;&#x679C;&#x53EF;&#x80FD;&#xFF0C;&#x5C1D;&#x8BD5;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x8FDB;&#x884C;&#x5F02;&#x6B65;&#x8F6C;&#x6362;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x5C06;&#x5177;&#x6709;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x7684; CPU &#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A; CUDA &#x5F20;&#x91CF;&#x3002; &#x8BBE;&#x7F6E;<code>copy</code>&#x65F6;&#xFF0C;&#x5373;&#x4F7F;&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x5339;&#x914D;&#x6240;&#x9700;&#x7684;&#x8F6C;&#x6362;&#xFF0C;&#x4E5F;&#x4F1A;&#x521B;&#x5EFA;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>to(other, non_blocking=False, copy=False) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;&#x5F20;&#x91CF;<code>other</code>&#x76F8;&#x540C;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x548C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x7684;&#x5F20;&#x91CF;&#x3002; &#x5F53;<code>non_blocking</code>&#x65F6;&#xFF0C;&#x5982;&#x679C;&#x53EF;&#x80FD;&#xFF0C;&#x5C1D;&#x8BD5;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x8FDB;&#x884C;&#x5F02;&#x6B65;&#x8F6C;&#x6362;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;&#x5C06;&#x5177;&#x6709;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x7684; CPU &#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A; CUDA &#x5F20;&#x91CF;&#x3002; &#x8BBE;&#x7F6E;<code>copy</code>&#x65F6;&#xFF0C;&#x5373;&#x4F7F;&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x4E0E;&#x6240;&#x9700;&#x7684;&#x8F6C;&#x6362;&#x5339;&#x914D;&#xFF0C;&#x4E5F;&#x4F1A;&#x521B;&#x5EFA;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu
&gt;&gt;&gt; tensor.to(torch.float64)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64)

&gt;&gt;&gt; cuda0 = torch.device(&apos;cuda:0&apos;)
&gt;&gt;&gt; tensor.to(cuda0)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], device=&apos;cuda:0&apos;)

&gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64, device=&apos;cuda:0&apos;)

&gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0)
&gt;&gt;&gt; tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
        [ 0.3310, -0.0584]], dtype=torch.float64, device=&apos;cuda:0&apos;)
</code></pre><hr>
<pre><code>to_mkldnn() &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>torch.mkldnn</code>&#x5E03;&#x5C40;&#x4E2D;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x526F;&#x672C;&#x3002;</p>
<hr>
<pre><code>take(indices) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.take" title="torch.take"><code>torch.take()</code></a></p>
<hr>
<pre><code>tan() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.tan" title="torch.tan"><code>torch.tan()</code></a></p>
<hr>
<pre><code>tan_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.tan" title="torch.Tensor.tan"><code>tan()</code></a></p>
<hr>
<pre><code>tanh() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.tanh" title="torch.tanh"><code>torch.tanh()</code></a></p>
<hr>
<pre><code>tanh_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code>tanh()</code></a></p>
<hr>
<pre><code>tolist()
</code></pre><p>&#x201D; tolist(&#xFF09;-&gt;&#x5217;&#x8868;&#x6216;&#x7F16;&#x53F7;</p>
<p>&#x5C06;&#x5F20;&#x91CF;&#x4F5C;&#x4E3A;(&#x5D4C;&#x5957;&#x7684;&#xFF09;&#x5217;&#x8868;&#x8FD4;&#x56DE;&#x3002; &#x5BF9;&#x4E8E;&#x6807;&#x91CF;&#xFF0C;&#x5C06;&#x8FD4;&#x56DE;&#x6807;&#x51C6; Python &#x7F16;&#x53F7;&#xFF0C;&#x5C31;&#x50CF; <a href="#torch.Tensor.item" title="torch.Tensor.item"><code>item()</code></a> &#x4E00;&#x6837;&#x3002; &#x5982;&#x6709;&#x5FC5;&#x8981;&#xFF0C;&#x5F20;&#x91CF;&#x4F1A;&#x5148;&#x81EA;&#x52A8;&#x79FB;&#x81F3; CPU&#x3002;</p>
<p>This operation is not differentiable.</p>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 2)
&gt;&gt;&gt; a.tolist()
[[0.012766935862600803, 0.5415473580360413],
 [-0.08909505605697632, 0.7729271650314331]]
&gt;&gt;&gt; a[0,0].tolist()
0.012766935862600803
</code></pre><hr>
<pre><code>topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.topk" title="torch.topk"><code>torch.topk()</code></a></p>
<hr>
<pre><code>to_sparse(sparseDims) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x7A00;&#x758F;&#x526F;&#x672C;&#x3002; PyTorch &#x652F;&#x6301;<a href="sparse.html#sparse-docs">&#x5750;&#x6807;&#x683C;&#x5F0F;</a>&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>sparseDims</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x65B0;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x4E2D;&#x5305;&#x542B;&#x7684;&#x7A00;&#x758F;&#x7EF4;&#x6570;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])
&gt;&gt;&gt; d
tensor([[ 0,  0,  0],
        [ 9,  0, 10],
        [ 0,  0,  0]])
&gt;&gt;&gt; d.to_sparse()
tensor(indices=tensor([[1, 1],
                       [0, 2]]),
       values=tensor([ 9, 10]),
       size=(3, 3), nnz=2, layout=torch.sparse_coo)
&gt;&gt;&gt; d.to_sparse(1)
tensor(indices=tensor([[1]]),
       values=tensor([[ 9,  0, 10]]),
       size=(3, 3), nnz=1, layout=torch.sparse_coo)
</code></pre><hr>
<pre><code>trace() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.trace" title="torch.trace"><code>torch.trace()</code></a></p>
<hr>
<pre><code>transpose(dim0, dim1) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.transpose" title="torch.transpose"><code>torch.transpose()</code></a></p>
<hr>
<pre><code>transpose_(dim0, dim1) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code>transpose()</code></a></p>
<hr>
<pre><code>triangular_solve(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.triangular_solve" title="torch.triangular_solve"><code>torch.triangular_solve()</code></a></p>
<hr>
<pre><code>tril(k=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.tril" title="torch.tril"><code>torch.tril()</code></a></p>
<hr>
<pre><code>tril_(k=0) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.tril" title="torch.Tensor.tril"><code>tril()</code></a></p>
<hr>
<pre><code>triu(k=0) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.triu" title="torch.triu"><code>torch.triu()</code></a></p>
<hr>
<pre><code>triu_(k=0) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.triu" title="torch.Tensor.triu"><code>triu()</code></a></p>
<hr>
<pre><code>trunc() &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a></p>
<hr>
<pre><code>trunc_() &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code>trunc()</code></a></p>
<hr>
<pre><code>type(dtype=None, non_blocking=False, **kwargs) &#x2192; str or Tensor
</code></pre><p>&#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B;<em>dtype</em>; &#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7C7B;&#x578B;&#xFF0C;&#x5426;&#x5219;&#x5C06;&#x8BE5;&#x5BF9;&#x8C61;&#x5F3A;&#x5236;&#x8F6C;&#x6362;&#x4E3A;&#x6307;&#x5B9A;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x5982;&#x679C;&#x5B83;&#x5DF2;&#x7ECF;&#x662F;&#x6B63;&#x786E;&#x7684;&#x7C7B;&#x578B;&#xFF0C;&#x5219;&#x4E0D;&#x6267;&#x884C;&#x4EFB;&#x4F55;&#x590D;&#x5236;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x539F;&#x59CB;&#x5BF9;&#x8C61;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dtype</strong>  (<em>python&#xFF1A;type</em> <em>&#x6216;</em> <em>&#x5B57;&#x7B26;&#x4E32;</em>&#xFF09;&#x2013; &#x6240;&#x9700;&#x7C7B;&#x578B;</p>
</li>
<li><p><strong>non_blocking</strong>  (<em>bool</em> ) &#x2013; &#x5982;&#x679C;<code>True</code>&#xFF0C;&#x5E76;&#x4E14;&#x6E90;&#x4F4D;&#x4E8E;&#x56FA;&#x5B9A;&#x5185;&#x5B58;&#x4E2D;&#xFF0C;&#x800C;&#x76EE;&#x6807;&#x4F4D;&#x4E8E; GPU &#x4E0A;&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#xFF0C;&#x5219;&#x76F8;&#x5BF9;&#x4E8E;&#x4E3B;&#x673A;&#x5F02;&#x6B65;&#x6267;&#x884C;&#x590D;&#x5236;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x53C2;&#x6570;&#x65E0;&#x6548;&#x3002;</p>
</li>
<li><p><strong>**kwargs</strong> &#x2013; &#x4E3A;&#x517C;&#x5BB9;&#x8D77;&#x89C1;&#xFF0C;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x952E;<code>async</code>&#x6765;&#x4EE3;&#x66FF;<code>non_blocking</code>&#x53C2;&#x6570;&#x3002; &#x4E0D;&#x63A8;&#x8350;&#x4F7F;&#x7528;<code>async</code> arg&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>type_as(tensor) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x6B64;&#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x7684;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x5DF2;&#x7ECF;&#x662F;&#x6B63;&#x786E;&#x7684;&#x7C7B;&#x578B;&#xFF0C;&#x5219;&#x8FD9;&#x662F;&#x65E0;&#x64CD;&#x4F5C;&#x7684;&#x3002; &#x76F8;&#x5F53;&#x4E8E;<code>self.type(tensor.type())</code></p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5F20;&#x91CF;</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5177;&#x6709;&#x6240;&#x9700;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;</p>
<hr>
<pre><code>unbind(dim=0) &#x2192; seq
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.unbind" title="torch.unbind"><code>torch.unbind()</code></a></p>
<hr>
<pre><code>unfold(dimension, size, step) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x5305;&#x542B;<code>self</code>&#x5F20;&#x91CF;&#x4E2D;&#x5C3A;&#x5BF8;&#x4E3A;<code>dimension</code>&#x7684;&#x6240;&#x6709;&#x5927;&#x5C0F;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x5207;&#x7247;&#x3002;</p>
<p>&#x4E24;&#x4E2A;&#x5207;&#x7247;&#x4E4B;&#x95F4;&#x7684;&#x6B65;&#x957F;&#x7531;<code>step</code>&#x7ED9;&#x51FA;&#x3002;</p>
<p>&#x5982;&#x679C; <em>sizedim</em> &#x662F;<code>self</code>&#x7684;&#x5C3A;&#x5BF8;<code>dimension</code>&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E2D;<code>dimension</code>&#x7684;&#x5C3A;&#x5BF8;&#x5C06;&#x662F;(<em>sizeim-size</em>&#xFF09;/<em>step</em>+ 1 &#x3002;</p>
<p>&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E2D;&#x9644;&#x52A0;&#x4E86;&#x5C3A;&#x5BF8;&#x4E3A; <a href="#torch.Tensor.size" title="torch.Tensor.size"><code>size</code></a> &#x7684;&#x9644;&#x52A0;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dimension</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x53D1;&#x751F;&#x5C55;&#x5F00;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>size</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5C55;&#x5F00;&#x7684;&#x6BCF;&#x4E2A;&#x5207;&#x7247;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>step</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6BCF;&#x4E2A;&#x5207;&#x7247;&#x4E4B;&#x95F4;&#x7684;&#x6B65;&#x9AA4;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 8)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])
&gt;&gt;&gt; x.unfold(0, 2, 1)
tensor([[ 1.,  2.],
        [ 2.,  3.],
        [ 3.,  4.],
        [ 4.,  5.],
        [ 5.,  6.],
        [ 6.,  7.]])
&gt;&gt;&gt; x.unfold(0, 2, 2)
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.]])
</code></pre><hr>
<pre><code>uniform_(from=0, to=1) &#x2192; Tensor
</code></pre><p>&#x7528;&#x4ECE;&#x8FDE;&#x7EED;&#x5747;&#x5300;&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#xFF1A;</p>
<p><img src="img/14837c97b5055932ef86ad2cd89a50be.jpg" alt=""></p>
<hr>
<pre><code>unique(sorted=True, return_inverse=False, return_counts=False, dim=None)
</code></pre><p>&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x552F;&#x4E00;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x89C1; <a href="torch.html#torch.unique" title="torch.unique"><code>torch.unique()</code></a></p>
<hr>
<pre><code>unique_consecutive(return_inverse=False, return_counts=False, dim=None)
</code></pre><p>&#x4ECE;&#x6BCF;&#x4E2A;&#x8FDE;&#x7EED;&#x7684;&#x7B49;&#x6548;&#x5143;&#x7D20;&#x7EC4;&#x4E2D;&#x9664;&#x53BB;&#x9664;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x5916;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x89C1; <a href="torch.html#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a></p>
<hr>
<pre><code>unsqueeze(dim) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.unsqueeze" title="torch.unsqueeze"><code>torch.unsqueeze()</code></a></p>
<hr>
<pre><code>unsqueeze_(dim) &#x2192; Tensor
</code></pre><p>&#x5C31;&#x5730;&#x7248;&#x672C;&#x7684; <a href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code>unsqueeze()</code></a></p>
<hr>
<pre><code>values() &#x2192; Tensor
</code></pre><p>&#x5982;&#x679C;<code>self</code>&#x662F;&#x7A00;&#x758F;&#x7684; COO &#x5F20;&#x91CF;(&#x5373;<code>torch.sparse_coo</code>&#x5E03;&#x5C40;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5305;&#x542B;&#x503C;&#x5F20;&#x91CF;&#x7684;&#x89C6;&#x56FE;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.indices" title="torch.Tensor.indices"><code>Tensor.indices()</code></a> &#x3002;</p>
<p>&#x6CE8;&#x610F;</p>
<p>This method can only be called on a coalesced sparse tensor. See <code>Tensor.coalesce()</code> for details.</p>
<hr>
<pre><code>var(dim=None, unbiased=True, keepdim=False) &#x2192; Tensor
</code></pre><p>&#x53C2;&#x89C1; <a href="torch.html#torch.var" title="torch.var"><code>torch.var()</code></a></p>
<hr>
<pre><code>view(*shape) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x4E0E;<code>self</code>&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x4F46;&#x5177;&#x6709;&#x4E0D;&#x540C;<code>shape</code>&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#xFF0C;&#x5E76;&#x4E14;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x4F46;&#x53EF;&#x80FD;&#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x8981;&#x67E5;&#x770B;&#x5F20;&#x91CF;&#xFF0C;&#x65B0;&#x89C6;&#x56FE;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x4E0E;&#x5176;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;&#x548C;&#x6B65;&#x5E45;&#x517C;&#x5BB9;&#xFF0C;&#x5373;&#x6BCF;&#x4E2A;&#x65B0;&#x89C6;&#x56FE;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x662F;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;&#x7684;&#x5B50;&#x7A7A;&#x95F4;&#xFF0C;&#x6216;&#x8005;&#x4EC5;&#x8DE8;&#x8D8A;&#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x6761;&#x4EF6;&#x7684;&#x539F;&#x59CB;&#x5C3A;&#x5BF8;<img src="img/8145d811599f1a90bcb673523825c2d2.jpg" alt=""> <img src="img/935124691142e85d6cbbdab3f68cc1df.jpg" alt="">&#x7684;&#x8FDE;&#x7EED;&#x6027;&#x72B6;</p>
<p><img src="img/b828b01a8af080958cc82bcc339c033a.jpg" alt=""></p>
<p>&#x5426;&#x5219;&#xFF0C;&#x9700;&#x8981;&#x5148;&#x8C03;&#x7528; <a href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code>contiguous()</code></a> &#x624D;&#x80FD;&#x67E5;&#x770B;&#x5F20;&#x91CF;&#x3002; &#x53E6;&#x8BF7;&#x53C2;&#x89C1;&#xFF1A; <a href="torch.html#torch.reshape" title="torch.reshape"><code>reshape()</code></a> &#xFF0C;&#x5982;&#x679C;&#x5F62;&#x72B6;&#x517C;&#x5BB9;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x89C6;&#x56FE;&#xFF0C;&#x5426;&#x5219;&#x590D;&#x5236;(&#x76F8;&#x5F53;&#x4E8E;&#x8C03;&#x7528; <a href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code>contiguous()</code></a>)&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>&#x5F62;&#x72B6;</strong>(<em>torch&#x5927;&#x5C0F;</em> <em>&#x6216;</em> <em>python&#xFF1A;int ...</em> )&#x2013;&#x6240;&#x9700;&#x5927;&#x5C0F;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; x.size()
torch.Size([4, 4])
&gt;&gt;&gt; y = x.view(16)
&gt;&gt;&gt; y.size()
torch.Size([16])
&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
&gt;&gt;&gt; z.size()
torch.Size([2, 8])

&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4)
&gt;&gt;&gt; a.size()
torch.Size([1, 2, 3, 4])
&gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
&gt;&gt;&gt; b.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
&gt;&gt;&gt; c.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; torch.equal(b, c)
False
</code></pre><hr>
<pre><code>view_as(other) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x6B64;&#x5F20;&#x91CF;&#x67E5;&#x770B;&#x4E3A;&#x4E0E;<code>other</code>&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002; <code>self.view_as(other)</code>&#x7B49;&#x6548;&#x4E8E;<code>self.view(other.size())</code>&#x3002;</p>
<p>&#x6709;&#x5173;<code>view</code>&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.Tensor.view" title="torch.Tensor.view"><code>view()</code></a> &#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>other</strong> (<a href="#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>) &#x2013; The result tensor has the same size as <code>other</code>.</p>
<hr>
<pre><code>where(condition, y) &#x2192; Tensor
</code></pre><p><code>self.where(condition, y)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.where(condition, self, y)</code>&#x3002; &#x53C2;&#x89C1; <a href="torch.html#torch.where" title="torch.where"><code>torch.where()</code></a></p>
<hr>
<pre><code>zero_() &#x2192; Tensor
</code></pre><p>&#x7528;&#x96F6;&#x586B;&#x5145;<code>self</code>&#x5F20;&#x91CF;&#x3002;</p>
<hr>
<pre><code>class torch.BoolTensor
</code></pre><p>&#x4EE5;&#x4E0B;&#x65B9;&#x6CD5;&#x662F; <a href="#torch.BoolTensor" title="torch.BoolTensor"><code>torch.BoolTensor</code></a> &#x72EC;&#x6709;&#x7684;&#x3002;</p>
<hr>
<pre><code>all()
</code></pre><hr>
<pre><code>all() &#x2192; bool
</code></pre><p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x5747;&#x4E3A; True&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#xFF0C;&#x5426;&#x5219;&#x8FD4;&#x56DE; False&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(1, 2).bool()
&gt;&gt;&gt; a
tensor([[False, True]], dtype=torch.bool)
&gt;&gt;&gt; a.all()
tensor(False, dtype=torch.bool)
</code></pre><hr>
<pre><code>all(dim, keepdim=False, out=None) &#x2192; Tensor
</code></pre><p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x4E2D;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x5747;&#x4E3A; True&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#xFF0C;&#x5426;&#x5219;&#x8FD4;&#x56DE; False&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x4F46;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x7684;&#x5927;&#x5C0F;&#x4E3A; 1&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="torch.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python&#xFF1A;int</em> ) &#x2013; &#x7F29;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>keepdim</strong>  (<em>bool</em> ) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x4FDD;&#x7559;<code>dim</code></p>
</li>
<li><p><strong>out</strong>  (<a href="#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &#x8F93;&#x51FA;&#x7684;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(4, 2).bool()
&gt;&gt;&gt; a
tensor([[True, True],
        [True, False],
        [True, True],
        [True, True]], dtype=torch.bool)
&gt;&gt;&gt; a.all(dim=1)
tensor([ True, False,  True,  True], dtype=torch.bool)
&gt;&gt;&gt; a.all(dim=0)
tensor([ True, False], dtype=torch.bool)
</code></pre><hr>
<pre><code>any()
</code></pre><hr>
<pre><code>any() &#x2192; bool
</code></pre><p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x5143;&#x7D20;&#x4E3A; True&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#xFF0C;&#x5426;&#x5219;&#x4E3A; False&#x3002;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(1, 2).bool()
&gt;&gt;&gt; a
tensor([[False, True]], dtype=torch.bool)
&gt;&gt;&gt; a.any()
tensor(True, dtype=torch.bool)
</code></pre><hr>
<pre><code>any(dim, keepdim=False, out=None) &#x2192; Tensor
</code></pre><p>&#x5982;&#x679C;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x4E2D;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x5143;&#x7D20;&#x4E3A; True&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#xFF0C;&#x5426;&#x5219;&#x4E3A; False&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x4F46;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x7684;&#x5927;&#x5C0F;&#x4E3A; 1&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="torch.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<ul>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; &#x7F29;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; &#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x4FDD;&#x7559;<code>dim</code></p>
</li>
<li><p><strong>out</strong> (<a href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; &#x8F93;&#x51FA;&#x7684;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0
&gt;&gt;&gt; a
tensor([[ True,  True],
        [False,  True],
        [ True,  True],
        [False, False]])
&gt;&gt;&gt; a.any(1)
tensor([ True,  True,  True, False])
&gt;&gt;&gt; a.any(0)
tensor([True, True])
</code></pre><p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2020-03-28 20:32:05
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="76.html" class="navigation navigation-prev " aria-label="Previous page: torch功能">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="78.html" class="navigation navigation-next " aria-label="Next page: 张量属性">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch张量","level":"1.17.4","depth":2,"next":{"title":"张量属性","level":"1.17.5","depth":2,"path":"78.md","ref":"78.md","articles":[]},"previous":{"title":"torch功能","level":"1.17.3","depth":2,"path":"76.md","ref":"76.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.4"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"77.md","mtime":"2020-03-28T12:32:05.460Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-03-30T18:46:55.556Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

