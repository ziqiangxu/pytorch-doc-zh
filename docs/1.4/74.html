
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>torch · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-advanced-emoji/emoji-website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="75.html" />
    
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    PyTorch 1.4 教程&文档
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    入门
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="4.html">
            
                <a href="4.html">
            
                    
                    使用 PyTorch 进行深度学习：60 分钟的闪电战
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="blitz/tensor_tutorial.html">
            
                <a href="blitz/tensor_tutorial.html">
            
                    
                    什么是PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="blitz/autograd_tutorial.html">
            
                <a href="blitz/autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="blitz/neural_networks_tutorial.html">
            
                <a href="blitz/neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="blitz/cifar10_tutorial.html">
            
                <a href="blitz/cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="blitz/data_parallel_tutorial.html">
            
                <a href="blitz/data_parallel_tutorial.html">
            
                    
                    可选：数据并行
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="5.html">
            
                <a href="5.html">
            
                    
                    编写自定义数据集，数据加载器和转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="6.html">
            
                <a href="6.html">
            
                    
                    使用 TensorBoard 可视化模型，数据和训练
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    图片
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="8.html">
            
                <a href="8.html">
            
                    
                    TorchVision 对象检测微调教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="9.html">
            
                <a href="9.html">
            
                    
                    转移学习的计算机视觉教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="10.html">
            
                <a href="10.html">
            
                    
                    空间变压器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="11.html">
            
                <a href="11.html">
            
                    
                    使用 PyTorch 进行神经传递
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="12.html">
            
                <a href="12.html">
            
                    
                    对抗示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="13.html">
            
                <a href="13.html">
            
                    
                    DCGAN 教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    音频
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="15.html">
            
                <a href="15.html">
            
                    
                    torchaudio 教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" >
            
                <span>
            
                    
                    文本
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="17.html">
            
                <a href="17.html">
            
                    
                    NLP From Scratch: 使用char-RNN对姓氏进行分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="18.html">
            
                <a href="18.html">
            
                    
                    NLP From Scratch: 生成名称与字符级RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="19.html">
            
                <a href="19.html">
            
                    
                    NLP From Scratch: 基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="20.html">
            
                <a href="20.html">
            
                    
                    使用 TorchText 进行文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="21.html">
            
                <a href="21.html">
            
                    
                    使用 TorchText 进行语言翻译
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="22.html">
            
                <a href="22.html">
            
                    
                    使用 nn.Transformer 和 TorchText 进行序列到序列建模
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" >
            
                <span>
            
                    
                    命名为 Tensor(实验性）
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="24.html">
            
                <a href="24.html">
            
                    
                    (实验性）PyTorch 中的命名张量简介
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" >
            
                <span>
            
                    
                    强化学习
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="26.html">
            
                <a href="26.html">
            
                    
                    强化学习(DQN）教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" >
            
                <span>
            
                    
                    在生产中部署 PyTorch 模型
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="28.html">
            
                <a href="28.html">
            
                    
                    通过带有 Flask 的 REST API 在 Python 中部署 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="29.html">
            
                <a href="29.html">
            
                    
                    TorchScript 简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="30.html">
            
                <a href="30.html">
            
                    
                    在 C ++中加载 TorchScript 模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.4" data-path="31.html">
            
                <a href="31.html">
            
                    
                    (可选）将模型从 PyTorch 导出到 ONNX 并使用 ONNX Runtime 运行
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" >
            
                <span>
            
                    
                    并行和分布式训练
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="33.html">
            
                <a href="33.html">
            
                    
                    单机模型并行最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="34.html">
            
                <a href="34.html">
            
                    
                    分布式数据并行入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="35.html">
            
                <a href="35.html">
            
                    
                    用 PyTorch 编写分布式应用程序
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.4" data-path="36.html">
            
                <a href="36.html">
            
                    
                    分布式 RPC 框架入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.5" data-path="37.html">
            
                <a href="37.html">
            
                    
                    (高级）带有 Amazon AWS 的 PyTorch 1.0 分布式训练师
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.10" >
            
                <span>
            
                    
                    扩展 PyTorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="39.html">
            
                <a href="39.html">
            
                    
                    使用自定义 C ++运算符扩展 TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.2" data-path="40.html">
            
                <a href="40.html">
            
                    
                    使用自定义 C ++类扩展 TorchScript
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.3" data-path="41.html">
            
                <a href="41.html">
            
                    
                    使用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.4" data-path="42.html">
            
                <a href="42.html">
            
                    
                    自定义 C ++和 CUDA 扩展
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" >
            
                <span>
            
                    
                    模型优化
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.11.1" data-path="44.html">
            
                <a href="44.html">
            
                    
                    LSTM Word 语言模型上的(实验）动态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.2" data-path="45.html">
            
                <a href="45.html">
            
                    
                    (实验性）在 PyTorch 中使用 Eager 模式进行静态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.3" data-path="46.html">
            
                <a href="46.html">
            
                    
                    (实验性）计算机视觉教程的量化转移学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.4" data-path="47.html">
            
                <a href="47.html">
            
                    
                    (实验）BERT 上的动态量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.5" data-path="48.html">
            
                <a href="48.html">
            
                    
                    修剪教程
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.12" >
            
                <span>
            
                    
                    PyTorch 用其他语言
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.12.1" data-path="50.html">
            
                <a href="50.html">
            
                    
                    使用 PyTorch C ++前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.13" >
            
                <span>
            
                    
                    PyTorch 基础知识
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.13.1" data-path="52.html">
            
                <a href="52.html">
            
                    
                    通过示例学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13.2" data-path="53.html">
            
                <a href="53.html">
            
                    
                    torch.nn 到底是什么？
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.14" >
            
                <span>
            
                    
                    文件
            
                </span>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" >
            
                <span>
            
                    
                    笔记
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="56.html">
            
                <a href="56.html">
            
                    
                    自动毕业力学
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="57.html">
            
                <a href="57.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.3" data-path="58.html">
            
                <a href="58.html">
            
                    
                    CPU 线程和 TorchScript 推断
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4" data-path="59.html">
            
                <a href="59.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5" data-path="60.html">
            
                <a href="60.html">
            
                    
                    分布式 Autograd 设计
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.6" data-path="61.html">
            
                <a href="61.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.7" data-path="62.html">
            
                <a href="62.html">
            
                    
                    经常问的问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.8" data-path="63.html">
            
                <a href="63.html">
            
                    
                    大规模部署的功能
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.9" data-path="64.html">
            
                <a href="64.html">
            
                    
                    并行处理最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.10" data-path="65.html">
            
                <a href="65.html">
            
                    
                    重现性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.11" data-path="66.html">
            
                <a href="66.html">
            
                    
                    远程参考协议
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.12" data-path="67.html">
            
                <a href="67.html">
            
                    
                    序列化语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.13" data-path="68.html">
            
                <a href="68.html">
            
                    
                    Windows 常见问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.14" data-path="69.html">
            
                <a href="69.html">
            
                    
                    XLA 设备上的 PyTorch
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" >
            
                <span>
            
                    
                    语言绑定
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.16.1" data-path="71.html">
            
                <a href="71.html">
            
                    
                    PyTorch C ++ API
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.2" data-path="72.html">
            
                <a href="72.html">
            
                    
                    PyTorch Java API
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.17" >
            
                <span>
            
                    
                    Python API
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.17.1" data-path="74.html">
            
                <a href="74.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.2" data-path="75.html">
            
                <a href="75.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.3" data-path="76.html">
            
                <a href="76.html">
            
                    
                    torch功能
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.4" data-path="77.html">
            
                <a href="77.html">
            
                    
                    torch张量
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.5" data-path="78.html">
            
                <a href="78.html">
            
                    
                    张量属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.6" data-path="79.html">
            
                <a href="79.html">
            
                    
                    自动差分包-Torch.Autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.7" data-path="80.html">
            
                <a href="80.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.8" data-path="81.html">
            
                <a href="81.html">
            
                    
                    分布式通讯包-Torch.Distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.9" data-path="82.html">
            
                <a href="82.html">
            
                    
                    概率分布-torch分布
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.10" data-path="83.html">
            
                <a href="83.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.11" data-path="84.html">
            
                <a href="84.html">
            
                    
                    torch脚本
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.12" data-path="85.html">
            
                <a href="85.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.13" data-path="86.html">
            
                <a href="86.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.14" data-path="87.html">
            
                <a href="87.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.15" data-path="88.html">
            
                <a href="88.html">
            
                    
                    量化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.16" data-path="89.html">
            
                <a href="89.html">
            
                    
                    分布式 RPC 框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.17" data-path="90.html">
            
                <a href="90.html">
            
                    
                    torch随机
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.18" data-path="91.html">
            
                <a href="91.html">
            
                    
                    torch稀疏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.19" data-path="92.html">
            
                <a href="92.html">
            
                    
                    torch存储
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.20" data-path="93.html">
            
                <a href="93.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.21" data-path="94.html">
            
                <a href="94.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.22" data-path="95.html">
            
                <a href="95.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.23" data-path="96.html">
            
                <a href="96.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.24" data-path="97.html">
            
                <a href="97.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.25" data-path="98.html">
            
                <a href="98.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.26" data-path="99.html">
            
                <a href="99.html">
            
                    
                    torch.utils.tensorboard
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.27" data-path="100.html">
            
                <a href="100.html">
            
                    
                    类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.28" data-path="101.html">
            
                <a href="101.html">
            
                    
                    命名张量
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.29" data-path="102.html">
            
                <a href="102.html">
            
                    
                    命名为 Tensors 操作员范围
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.30" data-path="103.html">
            
                <a href="103.html">
            
                    
                    糟糕！
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.18" >
            
                <span>
            
                    
                    torchvision参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.18.1" data-path="105.html">
            
                <a href="105.html">
            
                    
                    torchvision
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.19" >
            
                <span>
            
                    
                    音频参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.19.1" data-path="107.html">
            
                <a href="107.html">
            
                    
                    torchaudio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.20" >
            
                <span>
            
                    
                    torchtext参考
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.20.1" data-path="109.html">
            
                <a href="109.html">
            
                    
                    torchtext
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.21" >
            
                <span>
            
                    
                    社区
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.21.1" data-path="111.html">
            
                <a href="111.html">
            
                    
                    PyTorch 贡献指南
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.2" data-path="112.html">
            
                <a href="112.html">
            
                    
                    PyTorch 治理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.3" data-path="113.html">
            
                <a href="113.html">
            
                    
                    PyTorch 治理| 感兴趣的人
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >torch</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="torch">torch</h1>
<blockquote>
<p>&#x539F;&#x6587;&#xFF1A; <a href="https://pytorch.org/docs/stable/torch.html" target="_blank">https://pytorch.org/docs/stable/torch.html</a></p>
</blockquote>
<p>torch&#x7A0B;&#x5E8F;&#x5305;&#x5305;&#x542B;&#x591A;&#x7EF4;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x636E;&#x7ED3;&#x6784;&#xFF0C;&#x5E76;&#x5B9A;&#x4E49;&#x4E86;&#x8FD9;&#x4E9B;&#x6570;&#x636E;&#x7684;&#x6570;&#x5B66;&#x8FD0;&#x7B97;&#x3002; &#x6B64;&#x5916;&#xFF0C;&#x5B83;&#x63D0;&#x4F9B;&#x4E86;&#x8BB8;&#x591A;&#x5B9E;&#x7528;&#x7A0B;&#x5E8F;&#xFF0C;&#x53EF;&#x7528;&#x4E8E;&#x6709;&#x6548;&#x5730;&#x5E8F;&#x5217;&#x5316;&#x5F20;&#x91CF;&#x548C;&#x4EFB;&#x610F;&#x7C7B;&#x578B;&#xFF0C;&#x4EE5;&#x53CA;&#x5176;&#x4ED6;&#x6709;&#x7528;&#x7684;&#x5B9E;&#x7528;&#x7A0B;&#x5E8F;&#x3002;</p>
<p>&#x5B83;&#x5177;&#x6709; CUDA &#x5BF9;&#x5E94;&#x9879;&#xFF0C;&#x4F7F;&#x60A8;&#x80FD;&#x591F;&#x5728;&#x5177;&#x6709;&#x8BA1;&#x7B97;&#x80FD;&#x529B;&gt; = 3.0 &#x7684; NVIDIA GPU &#x4E0A;&#x8FD0;&#x884C;&#x5F20;&#x91CF;&#x8BA1;&#x7B97;&#x3002;</p>
<h2 id="&#x5F20;&#x91CF;">&#x5F20;&#x91CF;</h2>
<hr>
<pre><code>torch.is_tensor(obj)&#xB6;
</code></pre><p>&#x5982;&#x679C; &lt;cite&gt;obj&lt;/cite&gt; &#x662F; PyTorch &#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<p>&#x53C2;&#x6570;</p>
<p><strong>obj</strong> (<em>&#x5BF9;&#x8C61;</em>&#xFF09;&#x2013;&#x8981;&#x6D4B;&#x8BD5;&#x7684;&#x5BF9;&#x8C61;</p>
<hr>
<pre><code>torch.is_storage(obj)&#xB6;
</code></pre><p>&#x5982;&#x679C; &lt;cite&gt;obj&lt;/cite&gt; &#x662F; PyTorch &#x5B58;&#x50A8;&#x5BF9;&#x8C61;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<p>Parameters</p>
<p><strong>obj</strong> (<em>Object</em>) &#x2013; Object to test</p>
<hr>
<pre><code>torch.is_floating_point(input) -&gt; (bool)&#xB6;
</code></pre><p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x662F;&#x6D6E;&#x70B9;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5373;<code>torch.float64</code>&#xFF0C;<code>torch.float32</code>&#x548C;<code>torch.float16</code>&#x4E4B;&#x4E00;&#xFF0C;&#x5219;&#x8FD4;&#x56DE; True&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6D4B;&#x8BD5;&#x7684; PyTorch &#x5F20;&#x91CF;</p>
<hr>
<pre><code>torch.set_default_dtype(d)&#xB6;
</code></pre><p>&#x5C06;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9; dtype &#x8BBE;&#x7F6E;&#x4E3A;<code>d</code>&#x3002; &#x8BE5;&#x7C7B;&#x578B;&#x5C06;&#x7528;&#x4F5C; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4E2D;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9; dtype &#x6700;&#x521D;&#x4E3A;<code>torch.float32</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>d</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)&#x2013;&#x6D6E;&#x70B9; dtype&#xFF0C;&#x4F7F;&#x5176;&#x6210;&#x4E3A;&#x9ED8;&#x8BA4;&#x503C;</p>
<p>&#x4F8B;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor
torch.float64
</code></pre><hr>
<pre><code>torch.get_default_dtype() &#x2192; torch.dtype&#xB6;
</code></pre><p>&#x83B7;&#x53D6;&#x5F53;&#x524D;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x6570; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_dtype(torch.float64)
&gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64
torch.float64
&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this
&gt;&gt;&gt; torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32
</code></pre><hr>
<pre><code>torch.set_default_tensor_type(t)&#xB6;
</code></pre><p>&#x5C06;&#x9ED8;&#x8BA4;&#x7684;<code>torch.Tensor</code>&#x7C7B;&#x578B;&#x8BBE;&#x7F6E;&#x4E3A;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;<code>t</code>&#x3002; &#x8BE5;&#x7C7B;&#x578B;&#x8FD8;&#x5C06;&#x7528;&#x4F5C; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4E2D;&#x7C7B;&#x578B;&#x63A8;&#x65AD;&#x7684;&#x9ED8;&#x8BA4;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x7684;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x6700;&#x521D;&#x4E3A;<code>torch.FloatTensor</code>&#x3002;</p>
<p>Parameters</p>
<p><strong>t</strong>  (<em>python&#xFF1A;type</em> <em>&#x6216;</em> <em>&#x5B57;&#x7B26;&#x4E32;</em>&#xFF09;&#x2013;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x6216;&#x5176;&#x540D;&#x79F0;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
torch.float32
&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)
&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor
torch.float64
</code></pre><hr>
<pre><code>torch.numel(input) &#x2192; int&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x603B;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)
&gt;&gt;&gt; torch.numel(a)
120
&gt;&gt;&gt; a = torch.zeros(4,4)
&gt;&gt;&gt; torch.numel(a)
16
</code></pre><hr>
<pre><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x6253;&#x5370;&#x9009;&#x9879;&#x3002; &#x4ECE; NumPy &#x65E0;&#x803B;&#x5730;&#x62FF;&#x8D70;&#x7684;&#x7269;&#x54C1;</p>
<p>Parameters</p>
<ul>
<li><p><strong>precision</strong> &#x2013;&#x6D6E;&#x70B9;&#x8F93;&#x51FA;&#x7684;&#x7CBE;&#x5EA6;&#x4F4D;&#x6570;(&#x9ED8;&#x8BA4;= 4&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x9608;&#x503C;</strong> &#x2013;&#x89E6;&#x53D1;&#x6C47;&#x603B;&#x800C;&#x4E0D;&#x662F;&#x5B8C;&#x6574;&#x7684; &lt;cite&gt;repr&lt;/cite&gt; &#x7684;&#x6570;&#x7EC4;&#x5143;&#x7D20;&#x603B;&#x6570;(&#x9ED8;&#x8BA4;= 1000&#xFF09;&#x3002;</p>
</li>
<li><p><strong>edgeitems</strong> -&#x6BCF;&#x4E2A;&#x7EF4;&#x7684;&#x5F00;&#x59CB;&#x548C;&#x7ED3;&#x675F;&#x5904;&#x6458;&#x8981;&#x4E2D;&#x6570;&#x7EC4;&#x9879;&#x76EE;&#x7684;&#x6570;&#x91CF;(&#x9ED8;&#x8BA4;= 3&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x7EBF;&#x5BBD;</strong> &#x2013;&#x7528;&#x4E8E;&#x63D2;&#x5165;&#x6362;&#x884C;&#x7B26;&#x7684;&#x6BCF;&#x884C;&#x5B57;&#x7B26;&#x6570;(&#x9ED8;&#x8BA4;= 80&#xFF09;&#x3002; &#x9608;&#x503C;&#x77E9;&#x9635;&#x5C06;&#x5FFD;&#x7565;&#x6B64;&#x53C2;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x914D;&#x7F6E;&#x6587;&#x4EF6;</strong> &#x2013; Sane &#x9ED8;&#x8BA4;&#x7528;&#x4E8E;&#x6F02;&#x4EAE;&#x7684;&#x6253;&#x5370;&#x3002; &#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x4EE5;&#x4E0A;&#x4EFB;&#x4F55;&#x9009;&#x9879;&#x8986;&#x76D6;&#x3002; (&lt;cite&gt;&#x9ED8;&#x8BA4;&lt;/cite&gt;&#xFF0C;&lt;cite&gt;&#x77ED;&lt;/cite&gt;&#xFF0C;&lt;cite&gt;&#x5B8C;&#x6574;&lt;/cite&gt;&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x4E00;&#x79CD;&#xFF09;</p>
</li>
<li><p><strong>sci_mode</strong> &#x2013;&#x542F;&#x7528;(&#x771F;&#xFF09;&#x6216;&#x7981;&#x7528;(&#x5047;&#xFF09;&#x79D1;&#x5B66;&#x8BA1;&#x6570;&#x6CD5;&#x3002; &#x5982;&#x679C;&#x6307;&#x5B9A;&#x4E86; None(&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5219;&#x8BE5;&#x503C;&#x7531; &lt;cite&gt;_Formatter&lt;/cite&gt; &#x5B9A;&#x4E49;</p>
</li>
</ul>
<hr>
<pre><code>torch.set_flush_denormal(mode) &#x2192; bool&#xB6;
</code></pre><p>&#x7981;&#x7528; CPU &#x4E0A;&#x7684;&#x975E;&#x6B63;&#x5E38;&#x6D6E;&#x200B;&#x200B;&#x70B9;&#x6570;&#x3002;</p>
<p>&#x5982;&#x679C;&#x60A8;&#x7684;&#x7CFB;&#x7EDF;&#x652F;&#x6301;&#x5237;&#x65B0;&#x975E;&#x6B63;&#x89C4;&#x6570;&#x5E76;&#x4E14;&#x5DF2;&#x6210;&#x529F;&#x914D;&#x7F6E;&#x5237;&#x65B0;&#x975E;&#x6B63;&#x89C4;&#x6A21;&#x5F0F;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;<code>True</code>&#x3002; <a href="#torch.set_flush_denormal" title="torch.set_flush_denormal"><code>set_flush_denormal()</code></a> &#x4EC5;&#x5728;&#x652F;&#x6301; SSE3 &#x7684; x86 &#x67B6;&#x6784;&#x4E0A;&#x53D7;&#x652F;&#x6301;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x6A21;&#x5F0F;</strong> (<em>bool</em> )&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x542F;&#x7528;&#x51B2;&#x6D17;&#x975E;&#x6B63;&#x5E38;&#x6A21;&#x5F0F;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.set_flush_denormal(True)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor([ 0.], dtype=torch.float64)
&gt;&gt;&gt; torch.set_flush_denormal(False)
True
&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)
</code></pre><h3 id="&#x521B;&#x4F5C;&#x884C;&#x52A8;">&#x521B;&#x4F5C;&#x884C;&#x52A8;</h3>
<p>&#x6CE8;&#x610F;</p>
<p>&#x968F;&#x673A;&#x62BD;&#x6837;&#x521B;&#x5EFA;&#x64CD;&#x4F5C;&#x5217;&#x5728;<a href="#random-sampling">&#x968F;&#x673A;&#x62BD;&#x6837;</a>&#x4E0B;&#xFF0C;&#x5305;&#x62EC;&#xFF1A; <a href="#torch.rand" title="torch.rand"><code>torch.rand()</code></a> <a href="#torch.rand_like" title="torch.rand_like"><code>torch.rand_like()</code></a> <a href="#torch.randn" title="torch.randn"><code>torch.randn()</code></a> <a href="#torch.randn_like" title="torch.randn_like"><code>torch.randn_like()</code></a> <a href="#torch.randint" title="torch.randint"><code>torch.randint()</code></a> <a href="#torch.randint_like" title="torch.randint_like"><code>torch.randint_like()</code></a> <a href="#torch.randperm" title="torch.randperm"><code>torch.randperm()</code></a> &#x60A8;&#x4E5F;&#x53EF;&#x4EE5;&#x5C06; <a href="#torch.empty" title="torch.empty"><code>torch.empty()</code></a> &#x4E0E;<a href="#inplace-random-sampling">&#x8F93;&#x5165;&#x4E00;&#x8D77;&#x4F7F;&#x7528; &#x4F4D;&#x968F;&#x673A;&#x62BD;&#x6837;</a>&#x65B9;&#x6CD5;&#x6765;&#x521B;&#x5EFA; <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> &#xFF0C;&#x5E76;&#x4ECE;&#x66F4;&#x5E7F;&#x6CDB;&#x7684;&#x5206;&#x5E03;&#x8303;&#x56F4;&#x5185;&#x91C7;&#x6837;&#x503C;&#x3002;</p>
<hr>
<pre><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x7528;<code>data</code>&#x6784;&#x9020;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8B66;&#x544A;</p>
<p><a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x59CB;&#x7EC8;&#x590D;&#x5236;<code>data</code>&#x3002; &#x5982;&#x679C;&#x60A8;&#x5177;&#x6709;&#x5F20;&#x91CF;<code>data</code>&#x5E76;&#x5E0C;&#x671B;&#x907F;&#x514D;&#x590D;&#x5236;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="tensors.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> &#x6216; <a href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a> &#x3002; &#x5982;&#x679C;&#x60A8;&#x6709; NumPy <code>ndarray</code>&#x5E76;&#x60F3;&#x907F;&#x514D;&#x590D;&#x5236;&#xFF0C;&#x8BF7;&#x4F7F;&#x7528; <a href="#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a> &#x3002;</p>
<p>Warning</p>
<p>&#x5F53;&#x6570;&#x636E;&#x662F;&#x5F20;&#x91CF; &lt;cite&gt;x&lt;/cite&gt; &#x65F6;&#xFF0C; <a href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> &#x4ECE;&#x4F20;&#x9012;&#x7684;&#x4EFB;&#x4F55;&#x6570;&#x636E;&#x4E2D;&#x8BFB;&#x51FA;&#x201C;&#x6570;&#x636E;&#x201D;&#xFF0C;&#x5E76;&#x6784;&#x9020;&#x4E00;&#x4E2A;&#x53F6;&#x5B50;&#x53D8;&#x91CF;&#x3002; &#x56E0;&#x6B64;&#xFF0C;<code>torch.tensor(x)</code>&#x7B49;&#x540C;&#x4E8E;<code>x.clone().detach()</code>&#xFF0C;<code>torch.tensor(x, requires_grad=True)</code>&#x7B49;&#x540C;&#x4E8E;<code>x.clone().detach().requires_grad_(True)</code>&#x3002; &#x5EFA;&#x8BAE;&#x4F7F;&#x7528;<code>clone()</code>&#x548C;<code>detach()</code>&#x7684;&#x7B49;&#x6548;&#x9879;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6570;&#x636E;</strong> (<em>array_like</em> )&#x2013;&#x5F20;&#x91CF;&#x7684;&#x521D;&#x59CB;&#x6570;&#x636E;&#x3002; &#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy <code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#xFF0C;&#x5219;&#x4ECE;<code>data</code>&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x8BBE;&#x5907;</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BBE;&#x5907;&#x4F5C;&#x4E3A;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)&#x3002; <code>device</code>&#x5C06;&#x662F;&#x7528;&#x4E8E; CPU &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684; CPU&#xFF0C;&#x5E76;&#x4E14;&#x662F;&#x7528;&#x4E8E; CUDA &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;&#x5F53;&#x524D; CUDA &#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>require_grad</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C; autograd &#x5E94;&#x8BE5;&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0A;&#x8BB0;&#x5F55;&#x64CD;&#x4F5C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
<li><p><strong>pin_memory</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x5206;&#x914D;&#x5728;&#x56FA;&#x5B9A;&#x7684;&#x5185;&#x5B58;&#x4E2D;&#x3002; &#x4EC5;&#x9002;&#x7528;&#x4E8E; CPU &#x5F20;&#x91CF;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
                 dtype=torch.float64,
                 device=torch.device(&apos;cuda:0&apos;))  # creates a torch.cuda.DoubleTensor
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&apos;cuda:0&apos;)

&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
tensor(3.1416)

&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
</code></pre><hr>
<pre><code>torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5728;&#x7ED9;&#x5B9A;<code>values</code>&#x548C;&#x7ED9;&#x5B9A;<code>values</code>&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4EE5;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x6784;&#x9020; COO(rdinate&#xFF09;&#x683C;&#x5F0F;&#x7684;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002; &#x7A00;&#x758F;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x662F;&lt;cite&gt;&#x800C;&#x4E0D;&#x662F;&lt;/cite&gt;&#xFF0C;&#x5728;&#x90A3;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x7D22;&#x5F15;&#x4E2D;&#x6709;&#x91CD;&#x590D;&#x7684;&#x5750;&#x6807;&#xFF0C;&#x5E76;&#x4E14;&#x8BE5;&#x7D22;&#x5F15;&#x5904;&#x7684;&#x503C;&#x662F;&#x6240;&#x6709;&#x91CD;&#x590D;&#x503C;&#x6761;&#x76EE;&#x7684;&#x603B;&#x548C;&#xFF1A; <a href="https://pytorch.org/docs/stable/sparse.html" target="_blank">torch.sparse</a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>array_like</em> )&#x2013;&#x5F20;&#x91CF;&#x7684;&#x521D;&#x59CB;&#x6570;&#x636E;&#x3002; &#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy <code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002; &#x5C06;&#x5728;&#x5185;&#x90E8;&#x5F3A;&#x5236;&#x8F6C;&#x6362;&#x4E3A;<code>torch.LongTensor</code>&#x3002; &#x7D22;&#x5F15;&#x662F;&#x77E9;&#x9635;&#x4E2D;&#x975E;&#x96F6;&#x503C;&#x7684;&#x5750;&#x6807;&#xFF0C;&#x56E0;&#x6B64;&#x5E94;&#x4E3A;&#x4E8C;&#x7EF4;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x7EF4;&#x662F;&#x5F20;&#x91CF;&#x7EF4;&#x6570;&#xFF0C;&#x7B2C;&#x4E8C;&#x7EF4;&#x662F;&#x975E;&#x96F6;&#x503C;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x503C;</strong> (<em>array_like</em> )&#x2013;&#x5F20;&#x91CF;&#x7684;&#x521D;&#x59CB;&#x503C;&#x3002; &#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#xFF0C;NumPy <code>ndarray</code>&#xFF0C;&#x6807;&#x91CF;&#x548C;&#x5176;&#x4ED6;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong>(&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#x6216;<code>torch.Size</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B;&#xFF0C;&#x5219;&#x5C06;&#x63A8;&#x65AD;&#x5927;&#x5C0F;&#x4E3A;&#x8DB3;&#x4EE5;&#x5BB9;&#x7EB3;&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4ECE;<code>values</code>&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x8BBE;&#x5907;</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A; None&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BBE;&#x5907;&#x4F5C;&#x4E3A;&#x9ED8;&#x8BA4;&#x5F20;&#x91CF;&#x7C7B;&#x578B;(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)&#x3002; <code>device</code>&#x5C06;&#x662F;&#x7528;&#x4E8E; CPU &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684; CPU&#xFF0C;&#x662F;&#x7528;&#x4E8E; CUDA &#x5F20;&#x91CF;&#x7C7B;&#x578B;&#x7684;&#x5F53;&#x524D; CUDA &#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; i = torch.tensor([[0, 1, 1],
                      [2, 0, 2]])
&gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32)
&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4])
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],
                            dtype=torch.float64,
                            device=torch.device(&apos;cuda:0&apos;))
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device=&apos;cuda:0&apos;, size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1\. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2\. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])
#
# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)
</code></pre><hr>
<pre><code>torch.as_tensor(data, dtype=None, device=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;&#x6570;&#x636E;&#x8F6C;&#x6362;&#x4E3A;&lt;cite&gt;torch&#x3002;&#x5F20;&#x91CF;&lt;/cite&gt;&#x3002; &#x5982;&#x679C;&#x6570;&#x636E;&#x5DF2;&#x7ECF;&#x662F;&#x5177;&#x6709;&#x76F8;&#x540C; &lt;cite&gt;dtype&lt;/cite&gt; &#x548C;&lt;cite&gt;&#x8BBE;&#x5907;&lt;/cite&gt;&#x7684;&lt;cite&gt;&#x5F20;&#x91CF;&lt;/cite&gt;&#xFF0C;&#x5219;&#x4E0D;&#x4F1A;&#x6267;&#x884C;&#x4EFB;&#x4F55;&#x590D;&#x5236;&#xFF0C;&#x5426;&#x5219;&#x5C06;&#x4F7F;&#x7528;&#x65B0;&#x7684;&lt;cite&gt;&#x5F20;&#x91CF;&lt;/cite&gt;&#x3002; &#x5982;&#x679C;&#x6570;&#x636E;&lt;cite&gt;&#x5F20;&#x91CF;&lt;/cite&gt;&#x5177;&#x6709;<code>requires_grad=True</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4FDD;&#x7559;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x8BA1;&#x7B97;&#x56FE;&#x3002; &#x540C;&#x6837;&#xFF0C;&#x5982;&#x679C;&#x6570;&#x636E;&#x662F;&#x5BF9;&#x5E94;&#x7684; &lt;cite&gt;dtype&lt;/cite&gt; &#x7684;<code>ndarray</code>&#xFF0C;&#x5E76;&#x4E14;&lt;cite&gt;&#x8BBE;&#x5907;&lt;/cite&gt;&#x662F; cpu&#xFF0C;&#x5219;&#x4E0D;&#x4F1A;&#x6267;&#x884C;&#x4EFB;&#x4F55;&#x590D;&#x5236;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>data</strong> (<em>array_like</em>) &#x2013; Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])

&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device(&apos;cuda&apos;))
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([1,  2,  3])
</code></pre><hr>
<pre><code>torch.as_strided(input, size, stride, storage_offset=0) &#x2192; Tensor&#xB6;
</code></pre><p>&#x521B;&#x5EFA;&#x5177;&#x6709;&#x6307;&#x5B9A;<code>size</code>&#xFF0C;<code>stride</code>&#x548C;<code>storage_offset</code>&#x7684;&#x73B0;&#x6709;&lt;cite&gt;&#x70AC;&#x7BA1;&lt;/cite&gt; <code>input</code>&#x7684;&#x89C6;&#x56FE;&#x3002;</p>
<p>Warning</p>
<p>&#x521B;&#x5EFA;&#x7684;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x5143;&#x7D20;&#x53EF;&#x4EE5;&#x5F15;&#x7528;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;&#x3002; &#x7ED3;&#x679C;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;(&#x5C24;&#x5176;&#x662F;&#x77E2;&#x91CF;&#x5316;&#x7684;&#x64CD;&#x4F5C;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x7684;&#x884C;&#x4E3A;&#x3002; &#x5982;&#x679C;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>&#x8BB8;&#x591A; PyTorch &#x51FD;&#x6570;&#x53EF;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x89C6;&#x56FE;&#xFF0C;&#x5E76;&#x5728;&#x6B64;&#x51FD;&#x6570;&#x5185;&#x90E8;&#x5B9E;&#x73B0;&#x3002; &#x8FD9;&#x4E9B;&#x529F;&#x80FD;&#xFF0C;&#x4F8B;&#x5982; <a href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"><code>torch.Tensor.expand()</code></a> &#xFF0C;&#x66F4;&#x6613;&#x4E8E;&#x9605;&#x8BFB;&#xFF0C;&#x56E0;&#x6B64;&#x66F4;&#x53EF;&#x53D6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#x6216;</em> <em>python&#xFF1A;ints</em> )&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;</p>
</li>
<li><p><strong>&#x8DE8;&#x5EA6;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#x6216;</em> <em>python&#xFF1A;ints</em> )&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x8DE8;&#x5EA6;</p>
</li>
<li><p><strong>storage_offset</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x4E2D;&#x7684;&#x504F;&#x79FB;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3)
&gt;&gt;&gt; x
tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2))
&gt;&gt;&gt; t
tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])
</code></pre><hr>
<pre><code>torch.from_numpy(ndarray) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4ECE;<code>numpy.ndarray</code>&#x521B;&#x5EFA; <a href="tensors.html#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> &#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x548C;<code>ndarray</code>&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x5185;&#x5B58;&#x3002; &#x5BF9;&#x5F20;&#x91CF;&#x7684;&#x4FEE;&#x6539;&#x5C06;&#x53CD;&#x6620;&#x5728;<code>ndarray</code>&#x4E2D;&#xFF0C;&#x53CD;&#x4E4B;&#x4EA6;&#x7136;&#x3002; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0D;&#x53EF;&#x8C03;&#x6574;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5F53;&#x524D;&#x5B83;&#x63A5;&#x53D7;&#x5177;&#x6709;<code>numpy.float64</code>&#xFF0C;<code>numpy.float32</code>&#xFF0C;<code>numpy.float16</code>&#xFF0C;<code>numpy.int64</code>&#xFF0C;<code>numpy.int32</code>&#xFF0C;<code>numpy.int16</code>&#xFF0C;<code>numpy.int8</code>&#xFF0C;<code>numpy.uint8</code>&#x548C;<code>numpy.bool</code> d &#x7C7B;&#x578B;&#x7684;<code>ndarray</code>&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])
&gt;&gt;&gt; t = torch.from_numpy(a)
&gt;&gt;&gt; t
tensor([ 1,  2,  3])
&gt;&gt;&gt; t[0] = -1
&gt;&gt;&gt; a
array([-1,  2,  3])
</code></pre><hr>
<pre><code>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;&#x6807;&#x91CF;&#x503C; &lt;cite&gt;0&lt;/cite&gt; &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5F62;&#x72B6;&#x7531;&#x53D8;&#x91CF;&#x53C2;&#x6570;<code>size</code>&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> (<em>python&#xFF1A;int ...</em> )&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002; &#x53EF;&#x4EE5;&#x662F;&#x53EF;&#x53D8;&#x6570;&#x91CF;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x5217;&#x8868;&#x6216;&#x5143;&#x7EC4;&#x4E4B;&#x7C7B;&#x7684;&#x96C6;&#x5408;&#x3002;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x7684; Tensor &#x6240;&#x9700;&#x7684;&#x5E03;&#x5C40;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>torch.strided</code>&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.zeros(2, 3)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])

&gt;&gt;&gt; torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
</code></pre><hr>
<pre><code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;&#x6807;&#x91CF;&#x503C; &lt;cite&gt;0&lt;/cite&gt; &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x3002; <code>torch.zeros_like(input)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Warning</p>
<p>&#x4ECE; 0.4 &#x5F00;&#x59CB;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x4E0D;&#x652F;&#x6301;<code>out</code>&#x5173;&#x952E;&#x5B57;&#x3002; &#x4F5C;&#x4E3A;&#x66FF;&#x4EE3;&#xFF0C;&#x65E7;&#x7684;<code>torch.zeros_like(input, out=output)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.zeros(input.size(), out=output)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013; <code>input</code>&#x7684;&#x5927;&#x5C0F;&#x5C06;&#x786E;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x7684; Tensor &#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A;<code>None</code>&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;<code>input</code>&#x7684; dtype&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x5E03;&#x5C40;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A;<code>None</code>&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;<code>input</code>&#x7684;&#x5E03;&#x5C40;&#x3002;</p>
</li>
<li><p><strong>&#x8BBE;&#x5907;</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x8BBE;&#x5907;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x4E3A;<code>None</code>&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;<code>input</code>&#x7684;&#x8BBE;&#x5907;&#x3002;</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
</code></pre><hr>
<pre><code>torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;&#x6807;&#x91CF;&#x503C; &lt;cite&gt;1&lt;/cite&gt; &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5F62;&#x72B6;&#x7531;&#x53D8;&#x91CF;&#x81EA;&#x53D8;&#x91CF;<code>size</code>&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> (<em>python:int...</em>) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ones(2, 3)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])
</code></pre><hr>
<pre><code>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;&#x6807;&#x91CF;&#x503C; &lt;cite&gt;1&lt;/cite&gt; &#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x3002; <code>torch.ones_like(input)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Warning</p>
<p>&#x4ECE; 0.4 &#x5F00;&#x59CB;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x4E0D;&#x652F;&#x6301;<code>out</code>&#x5173;&#x952E;&#x5B57;&#x3002; &#x4F5C;&#x4E3A;&#x66FF;&#x4EE3;&#xFF0C;&#x65E7;&#x7684;<code>torch.ones_like(input, out=output)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.ones(input.size(), out=output)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)
&gt;&gt;&gt; torch.ones_like(input)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
</code></pre><hr>
<pre><code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A;<img src="img/fb2784cdfbfdd3f567366d5157f05b62.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x503C;&#x5177;&#x6709;&#x4ECE;&lt;cite&gt;&#x5F00;&#x59CB;&lt;/cite&gt;&#x5F00;&#x59CB;&#x5177;&#x6709;&#x516C;&#x5171;&#x5DEE;<code>step</code>&#x7684;&#x95F4;&#x9694;<code>[start, end)</code>&#x7684;&#x503C;&#x3002;</p>
<p>&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x4E0E;<code>end</code>&#x6BD4;&#x8F83;&#x65F6;&#xFF0C;&#x975E;&#x6574;&#x6570;<code>step</code>&#x4F1A;&#x51FA;&#x73B0;&#x6D6E;&#x70B9;&#x820D;&#x5165;&#x9519;&#x8BEF;&#xFF1B; &#x4E3A;&#x4E86;&#x907F;&#x514D;&#x4E0D;&#x4E00;&#x81F4;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5EFA;&#x8BAE;&#x5728;<code>end</code>&#x4E2D;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;&#x5C0F;&#x7684;&#x3B5;&#x3002;</p>
<p><img src="img/e3f9924ddb1c14b63336e10cb955b09f.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8D77;&#x59CB;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>0</code>&#x3002;</p>
</li>
<li><p><strong>&#x7ED3;&#x675F;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x70B9;&#x96C6;&#x7684;&#x7ED3;&#x675F;&#x503C;</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x6BCF;&#x5BF9;&#x76F8;&#x90BB;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9699;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>1</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;(&#x8BF7;&#x53C2;&#x9605; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)&#x3002; &#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B; &lt;cite&gt;dtype&lt;/cite&gt; &#xFF0C;&#x5219;&#x4ECE;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x53C2;&#x6570;&#x63A8;&#x65AD;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x5982;&#x679C;&lt;cite&gt;&#x5F00;&#x59CB;&lt;/cite&gt;&#xFF0C;&lt;cite&gt;&#x7ED3;&#x675F;&lt;/cite&gt;&#x6216;&lt;cite&gt;&#x505C;&#x6B62;&lt;/cite&gt;&#x4E2D;&#x7684;&#x4EFB;&#x4F55;&#x4E00;&#x4E2A;&#x662F;&#x6D6E;&#x70B9;&#xFF0C;&#x5219;&#x63A8;&#x65AD; &lt;cite&gt;dtype&lt;/cite&gt; &#x4E3A;&#x9ED8;&#x8BA4; dtype&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;[ <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a> &#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06; &lt;cite&gt;dtype&lt;/cite&gt; &#x63A8;&#x65AD;&#x4E3A; &lt;cite&gt;torch.int64&lt;/cite&gt; &#x3002;</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.arange(5)
tensor([ 0,  1,  2,  3,  4])
&gt;&gt;&gt; torch.arange(1, 4)
tensor([ 1,  2,  3])
&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
</code></pre><hr>
<pre><code>torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5728;&#x6B65;&#x9AA4;<code>step</code>&#x4E2D;&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A;<img src="img/c04824f45400f61b164b031c61248a15.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x503C;&#x4ECE;<code>start</code>&#x5230;<code>end</code>&#x3002; &#x9636;&#x8DC3;&#x662F;&#x5F20;&#x91CF;&#x4E2D;&#x4E24;&#x4E2A;&#x503C;&#x4E4B;&#x95F4;&#x7684;&#x5DEE;&#x8DDD;&#x3002;</p>
<p><img src="img/ce7cb189b2d3908fa0a51507da3efd58.jpg" alt=""></p>
<p>Warning</p>
<p>&#x4E0D;&#x63A8;&#x8350;&#x4F7F;&#x7528;&#x6B64;&#x529F;&#x80FD;&#xFF0C;&#x800C;&#x63A8;&#x8350;&#x4F7F;&#x7528; <a href="#torch.arange" title="torch.arange"><code>torch.arange()</code></a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>start</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>0</code>&#x3002;</p>
</li>
<li><p><strong>end</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x70B9;&#x96C6;&#x7684;&#x7ED3;&#x675F;&#x503C;</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x6BCF;&#x5BF9;&#x76F8;&#x90BB;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x95F4;&#x9699;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>1</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). If &lt;cite&gt;dtype&lt;/cite&gt; is not given, infer the data type from the other input arguments. If any of &lt;cite&gt;start&lt;/cite&gt;, &lt;cite&gt;end&lt;/cite&gt;, or &lt;cite&gt;stop&lt;/cite&gt; are floating-point, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be the default dtype, see <a href="#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype()</code></a>. Otherwise, the &lt;cite&gt;dtype&lt;/cite&gt; is inferred to be &lt;cite&gt;torch.int64&lt;/cite&gt;.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.range(1, 4)
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.range(1, 4, 0.5)
tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])
</code></pre><hr>
<pre><code>torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;<code>start</code>&#x548C;<code>end</code>&#x4E4B;&#x95F4;&#x7B49;&#x8DDD;&#x70B9;&#x7684;<code>steps</code>&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x4E3A;<code>steps</code>&#x5927;&#x5C0F;&#x7684; 1-D&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F00;&#x59CB;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x70B9;&#x96C6;&#x7684;&#x8D77;&#x59CB;&#x503C;</p>
</li>
<li><p><strong>end</strong> (<em>python:float</em>) &#x2013; the ending value for the set of points</p>
</li>
<li><p><strong>&#x6B65;&#x9AA4;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5728;<code>start</code>&#x548C;<code>end</code>&#x4E4B;&#x95F4;&#x91C7;&#x6837;&#x7684;&#x70B9;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>100</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)
tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])
&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)
tensor([-10.,  -5.,   0.,   5.,  10.])
&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1)
tensor([-10.])
</code></pre><hr>
<pre><code>torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;<img src="img/6a2c1af1d373a2b11ada878076f5b2a5.jpg" alt="">&#x548C;<img src="img/6dcd2d5bb84df471dd6574611a280f9d.jpg" alt="">&#x4E4B;&#x95F4;&#x7684;&#x5E95;&#x6570;<code>base</code>&#x5BF9;&#x6570;&#x95F4;&#x9694;&#x7684;<code>steps</code>&#x70B9;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>The output tensor is 1-D of size <code>steps</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>start</strong> (<em>python:float</em>) &#x2013; the starting value for the set of points</p>
</li>
<li><p><strong>end</strong> (<em>python:float</em>) &#x2013; the ending value for the set of points</p>
</li>
<li><p><strong>steps</strong> (<em>python:int</em>) &#x2013; number of points to sample between <code>start</code> and <code>end</code>. Default: <code>100</code>.</p>
</li>
<li><p><strong>&#x57FA;&#x6570;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x5BF9;&#x6570;&#x51FD;&#x6570;&#x7684;&#x57FA;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>10.0</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)
tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)
tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])
&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1)
tensor([1.2589])
&gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
</code></pre><hr>
<pre><code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x6709;&#x4E00;&#x4E2A;&#xFF0C;&#x5176;&#x4ED6;&#x4F4D;&#x7F6E;&#x4E3A;&#x96F6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x884C;&#x6570;</p>
</li>
<li><p><strong>m</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x9ED8;&#x8BA4;&#x4E3A;<code>n</code>&#x7684;&#x5217;&#x6570;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>&#x9000;&#x8D27;</p>
<p>&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x6709;&#x4E00;&#x4E2A;&#xFF0C;&#x5176;&#x4ED6;&#x4F4D;&#x7F6E;&#x4E3A;&#x96F6;</p>
<p>&#x8FD4;&#x56DE;&#x7C7B;&#x578B;</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eye(3)
tensor([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.],
        [ 0.,  0.,  1.]])
</code></pre><hr>
<pre><code>torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x672A;&#x521D;&#x59CB;&#x5316;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7531;&#x53D8;&#x91CF;&#x53C2;&#x6570;<code>size</code>&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> (<em>python:int...</em>) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li><p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty(2, 3)
tensor(1.00000e-08 *
       [[ 6.3984,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000]])
</code></pre><hr>
<pre><code>torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x672A;&#x521D;&#x59CB;&#x5316;&#x5F20;&#x91CF;&#x3002; <code>torch.empty_like(input)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
</code></pre><hr>
<pre><code>torch.empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x586B;&#x5145;&#x6709;&#x672A;&#x521D;&#x59CB;&#x5316;&#x6570;&#x636E;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x548C;&#x6B65;&#x5E45;&#x5206;&#x522B;&#x7531;&#x53D8;&#x91CF;&#x53C2;&#x6570;<code>size</code>&#x548C;<code>stride</code>&#x5B9A;&#x4E49;&#x3002; <code>torch.empty_strided(size, stride)</code>&#x7B49;&#x540C;&#x4E8E;<code>torch.empty(size).as_strided(size, stride)</code>&#x3002;</p>
<p>Warning</p>
<p>&#x521B;&#x5EFA;&#x7684;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x5143;&#x7D20;&#x53EF;&#x4EE5;&#x5F15;&#x7528;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;&#x3002; &#x7ED3;&#x679C;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;(&#x5C24;&#x5176;&#x662F;&#x77E2;&#x91CF;&#x5316;&#x7684;&#x64CD;&#x4F5C;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x7684;&#x884C;&#x4E3A;&#x3002; &#x5982;&#x679C;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong>(python&#xFF1A;ints &#x7684;<em>&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;</em></p>
</li>
<li><p><strong>&#x8DE8;&#x5EA6;</strong>(python&#xFF1A;ints &#x7684;<em>&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x8DE8;&#x5EA6;</em></p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
<li><p><strong>pin_memory</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2))
&gt;&gt;&gt; a
tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],
        [0.0000e+00, 0.0000e+00, 3.0705e-41]])
&gt;&gt;&gt; a.stride()
(1, 2)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])
</code></pre><hr>
<pre><code>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5927;&#x5C0F;&#x4E3A;<code>size</code>&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x586B;&#x5145;&#x4E86;<code>fill_value</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5927;&#x5C0F;</strong> (<em>python&#xFF1A;int ...</em> )&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5217;&#x8868;&#xFF0C;&#x5143;&#x7EC4;&#x6216;<code>torch.Size</code>&#x3002;</p>
</li>
<li><p><strong>fill_value</strong> &#x2013;&#x7528;&#x6765;&#x586B;&#x5145;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x5B57;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
        [ 3.1416,  3.1416,  3.1416]])
</code></pre><hr>
<pre><code>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;&#x586B;&#x5145;&#x6709;<code>fill_value</code>&#x7684;<code>input</code>&#x5927;&#x5C0F;&#x76F8;&#x540C;&#x7684;&#x5F20;&#x91CF;&#x3002; <code>torch.full_like(input, fill_value)</code>&#x7B49;&#x540C;&#x4E8E;<code>torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>fill_value</strong> &#x2013; the number to fill the output tensor with.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr>
<pre><code>torch.quantize_per_tensor(input, scale, zero_point, dtype) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A;&#x5177;&#x6709;&#x7ED9;&#x5B9A;&#x6BD4;&#x4F8B;&#x548C;&#x96F6;&#x70B9;&#x7684;&#x91CF;&#x5316;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x91CF;&#x5316;</p>
</li>
<li><p><strong>&#x6807;&#x5EA6;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x9002;&#x7528;&#x4E8E;&#x91CF;&#x5316;&#x516C;&#x5F0F;&#x7684;&#x6807;&#x5EA6;</p>
</li>
<li><p><strong>zero_point</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x6620;&#x5C04;&#x4E3A;&#x6D6E;&#x70B9;&#x96F6;&#x7684;&#x6574;&#x6570;&#x503C;&#x504F;&#x79FB;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x5FC5;&#x987B;&#x662F;&#x91CF;&#x5316;&#x7684; dtypes &#x4E4B;&#x4E00;&#xFF1A;<code>torch.quint8</code>&#xFF0C;<code>torch.qint8</code>&#x548C;<code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x65B0;&#x91CF;&#x5316;&#x7684;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)
tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
&gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
tensor([ 0, 10, 20, 30], dtype=torch.uint8)
</code></pre><hr>
<pre><code>torch.quantize_per_channel(input, scales, zero_points, axis, dtype) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;&#x6D6E;&#x70B9;&#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A;&#x5177;&#x6709;&#x7ED9;&#x5B9A;&#x6BD4;&#x4F8B;&#x548C;&#x96F6;&#x70B9;&#x7684;&#x6BCF;&#x901A;&#x9053;&#x91CF;&#x5316;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; float tensor to quantize</p>
</li>
<li><p><strong>&#x79E4;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x4F7F;&#x7528;&#x7684;&#x4E00;&#x7EF4;&#x6D6E;&#x6807;&#x79E4;&#xFF0C;&#x5C3A;&#x5BF8;&#x5E94;&#x5339;&#x914D;<code>input.size(axis)</code></p>
</li>
<li><p><strong>zero_points</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x4F7F;&#x7528;&#x7684;&#x6574;&#x6570; 1D &#x5F20;&#x91CF;&#x504F;&#x79FB;&#x91CF;&#xFF0C;&#x5927;&#x5C0F;&#x5E94;&#x4E0E;<code>input.size(axis)</code>&#x76F8;&#x5339;&#x914D;</p>
</li>
<li><p><strong>&#x8F74;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5E94;&#x7528;&#x6BCF;&#x4E2A;&#x901A;&#x9053;&#x91CF;&#x5316;&#x7684;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>) &#x2013; the desired data type of returned tensor. Has to be one of the quantized dtypes: <code>torch.quint8</code>, <code>torch.qint8</code>, <code>torch.qint32</code></p>
</li>
</ul>
<p>Returns</p>
<p>A newly quantized tensor</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])
&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)
tensor([[-1.,  0.],
        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_channel_affine,
       scale=tensor([0.1000, 0.0100], dtype=torch.float64),
       zero_point=tensor([10,  0]), axis=0)
&gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
tensor([[  0,  10],
        [100, 200]], dtype=torch.uint8)
</code></pre><h3 id="&#x7D22;&#x5F15;&#xFF0C;&#x5207;&#x7247;&#xFF0C;&#x8054;&#x63A5;&#xFF0C;&#x64CD;&#x4F5C;&#x53D8;&#x66F4;">&#x7D22;&#x5F15;&#xFF0C;&#x5207;&#x7247;&#xFF0C;&#x8054;&#x63A5;&#xFF0C;&#x64CD;&#x4F5C;&#x53D8;&#x66F4;</h3>
<hr>
<pre><code>torch.cat(tensors, dim=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x4E0A;&#x8FDE;&#x63A5;<code>seq</code>&#x5F20;&#x91CF;&#x7684;&#x7ED9;&#x5B9A;&#x5E8F;&#x5217;&#x3002; &#x6240;&#x6709;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;(&#x5728;&#x8FDE;&#x63A5;&#x7EF4;&#x4E2D;&#x9664;&#x5916;&#xFF09;&#x6216;&#x4E3A;&#x7A7A;&#x3002;</p>
<p><a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> &#x53EF;&#x4EE5;&#x770B;&#x4F5C;&#x662F; <a href="#torch.split" title="torch.split"><code>torch.split()</code></a> &#x548C; <a href="#torch.chunk" title="torch.chunk"><code>torch.chunk()</code></a> &#x7684;&#x9006;&#x8FD0;&#x7B97;&#x3002;</p>
<p>&#x901A;&#x8FC7;&#x793A;&#x4F8B;&#x53EF;&#x4EE5;&#x66F4;&#x597D;&#x5730;&#x7406;&#x89E3; <a href="#torch.cat" title="torch.cat"><code>torch.cat()</code></a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong>(&#x5F20;&#x91CF;<em>&#x5E8F;&#x5217;</em>&#xFF09;&#x2013;&#x540C;&#x4E00;&#x7C7B;&#x578B;&#x7684;&#x4EFB;&#x4F55; python &#x5F20;&#x91CF;&#x5E8F;&#x5217;&#x3002; &#x63D0;&#x4F9B;&#x7684;&#x975E;&#x7A7A;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4F46;&#x732B;&#x7684;&#x5C3A;&#x5BF8;&#x9664;&#x5916;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5F20;&#x91CF;&#x7EA7;&#x8054;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])
</code></pre><hr>
<pre><code>torch.chunk(input, chunks, dim=0) &#x2192; List of Tensors&#xB6;
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;&#x62C6;&#x5206;&#x4E3A;&#x7279;&#x5B9A;&#x6570;&#x91CF;&#x7684;&#x5757;&#x3002;</p>
<p>&#x5982;&#x679C;&#x6CBF;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x7684;&#x5F20;&#x91CF;&#x5927;&#x5C0F;&#x4E0D;&#x80FD;&#x88AB;<code>chunks</code>&#x6574;&#x9664;&#xFF0C;&#x5219;&#x6700;&#x540E;&#x4E00;&#x5757;&#x5C06;&#x8F83;&#x5C0F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x5206;&#x5272;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5757;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x8FD4;&#x56DE;&#x7684;&#x5757;&#x6570;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6CBF;&#x5176;&#x5F20;&#x91CF;&#x5206;&#x88C2;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
</ul>
<hr>
<pre><code>torch.gather(input, dim, index, out=None, sparse_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6CBF;&lt;cite&gt;&#x660F;&#x6697;&lt;/cite&gt;&#x6307;&#x5B9A;&#x7684;&#x8F74;&#x6536;&#x96C6;&#x503C;&#x3002;</p>
<p>&#x5BF9;&#x4E8E; 3-D &#x5F20;&#x91CF;&#xFF0C;&#x8F93;&#x51FA;&#x6307;&#x5B9A;&#x4E3A;&#xFF1A;</p>
<pre><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
</code></pre><p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;<img src="img/a89ebd4ba33f6e8d4b992302696fcb6b.jpg" alt="">&#x548C;<code>dim = i</code>&#x7684; n &#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>index</code>&#x5FC5;&#x987B;&#x662F;&#x5927;&#x5C0F;&#x4E3A;<img src="img/0998ac39fb5e46e656f2261d0af181b6.jpg" alt="">&#x7684;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">-&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/5890f3c556cb9391345ace3ce2c49ff9.jpg" alt="">&#x548C;<code>out</code>&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F; &#x5927;&#x5C0F;&#x4E3A;<code>index</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6E90;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6CBF;&#x5176;&#x7D22;&#x5F15;&#x7684;&#x8F74;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013;&#x8981;&#x6536;&#x96C6;&#x7684;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x76EE;&#x6807;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>sparse_grad</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;<code>True</code>&#xFF0C;&#x5219;&#x68AF;&#x5EA6; w.r.t. <code>input</code>&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x7A00;&#x758F;&#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]])
&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))
tensor([[ 1,  1],
        [ 4,  3]])
</code></pre><hr>
<pre><code>torch.index_select(input, dim, index, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x4F7F;&#x7528;<code>index</code> LongTensor &#x4E2D;&#x7684;<code>index</code>&#x4E2D;&#x7684;&#x6761;&#x76EE;&#x6CBF;&#x7EF4;&#x5EA6;<code>dim</code>&#x7D22;&#x5F15;<code>input</code>&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E0E;&#x539F;&#x59CB;&#x5F20;&#x91CF;(<code>input</code>&#xFF09;&#x76F8;&#x540C;&#x7684;&#x7EF4;&#x6570;&#x3002; <code>dim</code>&#x7684;&#x5C3A;&#x5BF8;&#x4E0E;<code>index</code>&#x7684;&#x957F;&#x5EA6;&#x76F8;&#x540C;&#xFF1B; &#x5176;&#x4ED6;&#x5C3A;&#x5BF8;&#x4E0E;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&#x5C3A;&#x5BF8;&#x76F8;&#x540C;&#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0D;&#x4E0E;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x4F7F;&#x7528;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;&#x7A7A;&#x95F4;<strong>&#x800C;&#x4E0D;&#x662F;</strong>&#x3002; &#x5982;&#x679C;<code>out</code>&#x7684;&#x5F62;&#x72B6;&#x4E0E;&#x9884;&#x671F;&#x7684;&#x5F62;&#x72B6;&#x4E0D;&#x540C;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x9ED8;&#x9ED8;&#x5730;&#x5C06;&#x5176;&#x66F4;&#x6539;&#x4E3A;&#x6B63;&#x786E;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x5E76;&#x5728;&#x5FC5;&#x8981;&#x65F6;&#x91CD;&#x65B0;&#x5206;&#x914D;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6211;&#x4EEC;&#x7D22;&#x5F15;&#x7684;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013;&#x5305;&#x542B;&#x8981;&#x7D22;&#x5F15;&#x7684;&#x7D22;&#x5F15;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-0.4664,  0.2647, -0.1228, -1.1068],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; indices = torch.tensor([0, 2])
&gt;&gt;&gt; torch.index_select(x, 0, indices)
tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],
        [-1.1734, -0.6571,  0.7230, -0.6004]])
&gt;&gt;&gt; torch.index_select(x, 1, indices)
tensor([[ 0.1427, -0.5414],
        [-0.4664, -0.1228],
        [-1.1734,  0.7230]])
</code></pre><hr>
<pre><code>torch.masked_select(input, mask, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x6839;&#x636E;&#x5E03;&#x5C14;&#x503C;&#x63A9;&#x7801;<code>mask</code>&#x4E3A;&#x5176; &lt;cite&gt;BoolTensor&lt;/cite&gt; &#x7D22;&#x5F15;<code>input</code>&#x5F20;&#x91CF;&#x3002;</p>
<p><code>mask</code>&#x5F20;&#x91CF;&#x548C;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x5B83;&#x4EEC;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x662F;&#x5426;<strong>&#x800C;&#x4E0D;&#x662F;</strong>&#x4F7F;&#x7528;&#x4E0E;&#x539F;&#x59CB;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684;&#x5B58;&#x50A8;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x63A9;&#x7801;</strong> (<em>ByteTensor</em> )&#x2013;&#x5305;&#x542B;&#x4E8C;&#x8FDB;&#x5236;&#x63A9;&#x7801;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x4EE5;&#x4F7F;&#x7528;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; x
tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
        [-1.2035,  1.2252,  0.5002,  0.6248],
        [ 0.1307, -2.0608,  0.1244,  2.0139]])
&gt;&gt;&gt; mask = x.ge(0.5)
&gt;&gt;&gt; mask
tensor([[False, False, False, False],
        [False, True, True, True],
        [False, False, False, True]])
&gt;&gt;&gt; torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
</code></pre><hr>
<pre><code>torch.narrow(input, dim, start, length) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x662F;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x7F29;&#x5C0F;&#x7248;&#x672C;&#x3002; &#x5C3A;&#x5BF8;<code>dim</code>&#x4ECE;<code>start</code>&#x8F93;&#x5165;&#x5230;<code>start + length</code>&#x3002; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x548C;<code>input</code>&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F20;&#x91CF;&#x53D8;&#x7A84;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x7F29;&#x5C0F;&#x8303;&#x56F4;</p>
</li>
<li><p><strong>&#x5F00;&#x59CB;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x8D77;&#x59CB;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x957F;&#x5EA6;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5230;&#x6700;&#x7EC8;&#x5C3A;&#x5BF8;&#x7684;&#x8DDD;&#x79BB;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)
tensor([[ 2,  3],
        [ 5,  6],
        [ 8,  9]])
</code></pre><hr>
<pre><code>torch.nonzero(input, *, out=None, as_tuple=False) &#x2192; LongTensor or tuple of LongTensors&#xB6;
</code></pre><p>Note</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=False)</code></a> (&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6BCF;&#x4E00;&#x884C;&#x90FD;&#x662F;&#x975E;&#x96F6;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p><a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero(..., as_tuple=True)</code></a> &#x8FD4;&#x56DE;&#x4E00;&#x7EF4;&#x7D22;&#x5F15;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5141;&#x8BB8;&#x8FDB;&#x884C;&#x9AD8;&#x7EA7;&#x7D22;&#x5F15;&#xFF0C;&#x56E0;&#x6B64;<code>x[x.nonzero(as_tuple=True)]</code>&#x7ED9;&#x51FA;&#x5F20;&#x91CF;<code>x</code>&#x7684;&#x6240;&#x6709;&#x975E;&#x96F6;&#x503C;&#x3002; &#x5728;&#x8FD4;&#x56DE;&#x7684;&#x5143;&#x7EC4;&#x4E2D;&#xFF0C;&#x6BCF;&#x4E2A;&#x7D22;&#x5F15;&#x5F20;&#x91CF;&#x90FD;&#x5305;&#x542B;&#x7279;&#x5B9A;&#x7EF4;&#x5EA6;&#x7684;&#x975E;&#x96F6;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x6709;&#x5173;&#x8FD9;&#x4E24;&#x79CD;&#x884C;&#x4E3A;&#x7684;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x4E0B;&#x6587;&#x3002;</p>
<p><strong>&#x5F53;</strong> <code>as_tuple</code> <strong>&#x4E3A;&#x201C; False&#x201D;(&#x9ED8;&#x8BA4;&#xFF09;</strong>&#x65F6;&#xFF1A;</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x5305;&#x542B;<code>input</code>&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002; &#x7ED3;&#x679C;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x90FD;&#x5305;&#x542B;<code>input</code>&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002; &#x7ED3;&#x679C;&#x6309;&#x5B57;&#x5178;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#xFF0C;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7D22;&#x5F15;&#x66F4;&#x6539;&#x6700;&#x5FEB;(C &#x6837;&#x5F0F;&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x5177;&#x6709;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x5C3A;&#x5BF8;&#xFF0C;&#x5219;&#x6240;&#x5F97;&#x7D22;&#x5F15;&#x5F20;&#x91CF;<code>out</code>&#x7684;&#x5927;&#x5C0F;&#x4E3A;<img src="img/b6080a6ed8a7dd471ec6fd4b1023bc23.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" alt="">&#x662F;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x3002;</p>
<p><strong>&#x5F53;</strong> <code>as_tuple</code> <strong>&#x4E3A;&#x201C; True&#x201D;</strong> &#x65F6;&#xFF1A;</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5728;<code>input</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x5305;&#x542B;<code>input</code>&#x6240;&#x6709;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;(&#x5728;&#x8BE5;&#x7EF4;&#x5EA6;&#x4E2D;&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x5177;&#x6709;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x5C3A;&#x5BF8;&#xFF0C;&#x5219;&#x751F;&#x6210;&#x7684;&#x5143;&#x7EC4;&#x5305;&#x542B;<img src="img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" alt="">&#x5927;&#x5C0F;&#x7684;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/e1bf4c09825257a3ffbe6bddb254bcb6.jpg" alt="">&#x662F;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x3002;</p>
<p>&#x4F5C;&#x4E3A;&#x4E00;&#x79CD;&#x7279;&#x6B8A;&#x60C5;&#x51B5;&#xFF0C;&#x5F53;<code>input</code>&#x5177;&#x6709;&#x96F6;&#x7EF4;&#x548C;&#x975E;&#x96F6;&#x6807;&#x91CF;&#x503C;&#x65F6;&#xFF0C;&#x4F1A;&#x5C06;&#x5176;&#x89C6;&#x4E3A;&#x5177;&#x6709;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong>  (<em>LongTensor</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5305;&#x542B;&#x7D22;&#x5F15;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5982;&#x679C;<code>as_tuple</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x5305;&#x542B;&#x7D22;&#x5F15;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>as_tuple</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x90FD;&#x6709;&#x4E00;&#x4E2A; 1-D &#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x5305;&#x542B;&#x6CBF;&#x7740;&#x8BE5;&#x7EF4;&#x5EA6;&#x7684;&#x6BCF;&#x4E2A;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>Return type</p>
<p>LongTensor &#x6216; LongTensor &#x7684;&#x5143;&#x7EC4;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))
tensor([[ 0],
        [ 1],
        [ 2],
        [ 4]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]))
tensor([[ 0,  0],
        [ 1,  1],
        [ 2,  2],
        [ 3,  3]])
&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)
(tensor([0, 1, 2, 4]),)
&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],
                                [0.0, 0.4, 0.0, 0.0],
                                [0.0, 0.0, 1.2, 0.0],
                                [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)
(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))
&gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True)
(tensor([0]),)
</code></pre><hr>
<pre><code>torch.reshape(input, shape) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x7684;&#x6570;&#x636E;&#x548C;&#x5143;&#x7D20;&#x6570;&#x91CF;&#xFF0C;&#x4F46;&#x5177;&#x6709;&#x6307;&#x5B9A;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;&#x53EF;&#x80FD;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x662F;<code>input</code>&#x7684;&#x89C6;&#x56FE;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5B83;&#x5C06;&#x662F;&#x526F;&#x672C;&#x3002; &#x8FDE;&#x7EED;&#x8F93;&#x5165;&#x548C;&#x5177;&#x6709;&#x517C;&#x5BB9;&#x6B65;&#x5E45;&#x7684;&#x8F93;&#x5165;&#x53EF;&#x4EE5;&#x5728;&#x4E0D;&#x590D;&#x5236;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#x8FDB;&#x884C;&#x91CD;&#x5851;&#xFF0C;&#x4F46;&#x662F;&#x60A8;&#x4E0D;&#x5E94;&#x8BE5;&#x4F9D;&#x8D56;&#x590D;&#x5236;&#x4E0E;&#x67E5;&#x770B;&#x884C;&#x4E3A;&#x3002;</p>
<p>&#x5F53;&#x53EF;&#x4EE5;&#x8FD4;&#x56DE;&#x89C6;&#x56FE;&#x65F6;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"><code>torch.Tensor.view()</code></a> &#x3002;</p>
<p>&#x5355;&#x4E2A;&#x5C3A;&#x5BF8;&#x53EF;&#x80FD;&#x4E3A;-1&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5B83;&#x662F;&#x6839;&#x636E;<code>input</code>&#x4E2D;&#x7684;&#x5176;&#x4F59;&#x5C3A;&#x5BF8;&#x548C;&#x5143;&#x7D20;&#x6570;&#x91CF;&#x63A8;&#x65AD;&#x51FA;&#x6765;&#x7684;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x91CD;&#x5851;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5F62;&#x72B6;</strong> (<em>python&#xFF1A;ints</em> &#x7684;&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x65B0;&#x5F62;&#x72B6;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(4.)
&gt;&gt;&gt; torch.reshape(a, (2, 2))
tensor([[ 0.,  1.],
        [ 2.,  3.]])
&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])
&gt;&gt;&gt; torch.reshape(b, (-1,))
tensor([ 0,  1,  2,  3])
</code></pre><hr>
<pre><code>torch.split(tensor, split_size_or_sections, dim=0)&#xB6;
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;&#x62C6;&#x5206;&#x4E3A;&#x591A;&#x4E2A;&#x5757;&#x3002;</p>
<p>&#x5982;&#x679C;<code>split_size_or_sections</code>&#x662F;&#x6574;&#x6570;&#x7C7B;&#x578B;&#xFF0C;&#x5219; <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x5C06;&#x88AB;&#x62C6;&#x5206;&#x4E3A;&#x5927;&#x5C0F;&#x76F8;&#x7B49;&#x7684;&#x5757;(&#x5982;&#x679C;&#x53EF;&#x80FD;&#xFF09;&#x3002; &#x5982;&#x679C;&#x6CBF;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x7684;&#x5F20;&#x91CF;&#x5927;&#x5C0F;&#x4E0D;&#x80FD;&#x88AB;<code>split_size</code>&#x6574;&#x9664;&#xFF0C;&#x5219;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x5757;&#x5C06;&#x8F83;&#x5C0F;&#x3002;</p>
<p>&#x5982;&#x679C;<code>split_size_or_sections</code>&#x662F;&#x5217;&#x8868;&#xFF0C;&#x5219;&#x6839;&#x636E;<code>split_size_or_sections</code>&#x5C06; <a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#x62C6;&#x5206;&#x4E3A;<code>dim</code>&#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<code>dim</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F20;&#x91CF;&#x5206;&#x88C2;&#x3002;</p>
</li>
<li><p><strong>split_size_or_sections</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF09;&#x6216;</em> <em>(</em> <em>&#x5217;&#x8868;</em> <em>(</em> <em>python &#xFF1A;int</em> <em>&#xFF09;</em>&#xFF09;&#x2013;&#x5355;&#x4E2A;&#x5757;&#x7684;&#x5927;&#x5C0F;&#x6216;&#x6BCF;&#x4E2A;&#x5757;&#x7684;&#x5927;&#x5C0F;&#x5217;&#x8868;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6CBF;&#x5176;&#x5F20;&#x91CF;&#x5206;&#x88C2;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>torch.squeeze(input, dim=None, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x4E3A; &lt;cite&gt;1&lt;/cite&gt; &#x7684;<code>input</code>&#x5C3A;&#x5BF8;&#x5747;&#x88AB;&#x5220;&#x9664;&#x3002;</p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x7684;&#x5F62;&#x72B6;&#x4E3A;&#xFF1A;<img src="img/1f976021505083151a3b5d3311ab04c2.jpg" alt="">&#xFF0C;&#x5219;&#x5F20;&#x91CF;&#x4E2D;&#x7684;&lt;cite&gt;&#x5F20;&#x91CF;&#x5C06;&#x4E3A;&#xFF1A;<img src="img/7af8285a40441ae3080550e9267b63f8.jpg" alt="">&#x3002;&lt;/cite&gt;</p>
<p>&#x7ED9;&#x5B9A;<code>dim</code>&#x65F6;&#xFF0C;&#x4EC5;&#x5728;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x4E0A;&#x6267;&#x884C;&#x6324;&#x538B;&#x64CD;&#x4F5C;&#x3002; &#x5982;&#x679C;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x7684;&#x5F62;&#x72B6;&#x4E3A;&#xFF1A;<img src="img/fb1a3c771fc7be02c64e40a24e873471.jpg" alt="">&#xFF0C;<code>squeeze(input, 0)</code>&#x4FDD;&#x6301;&#x5F20;&#x91CF;&#x4E0D;&#x53D8;&#xFF0C;&#x4F46;&#x662F;<code>squeeze(input, 1)</code>&#x4F1A;&#x5C06;&#x5F20;&#x91CF;&#x538B;&#x7F29;&#x4E3A;<img src="img/e48d9b756847043d64d7565153ad4faf.jpg" alt="">&#x5F62;&#x72B6;&#x3002;</p>
<p>Note</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x5B58;&#x50A8;&#xFF0C;&#x56E0;&#x6B64;&#x66F4;&#x6539;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x5185;&#x5BB9;&#x5C06;&#x66F4;&#x6539;&#x53E6;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;&#x7ED9;&#x5B9A;&#xFF0C;&#x5219;&#x4EC5;&#x5728;&#x6B64;&#x7EF4;&#x5EA6;&#x4E0A;&#x538B;&#x7F29;&#x8F93;&#x5165;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)
&gt;&gt;&gt; x.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 0)
&gt;&gt;&gt; y.size()
torch.Size([2, 1, 2, 1, 2])
&gt;&gt;&gt; y = torch.squeeze(x, 1)
&gt;&gt;&gt; y.size()
torch.Size([2, 2, 1, 2])
</code></pre><hr>
<pre><code>torch.stack(tensors, dim=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;&#x7684;&#x5E8F;&#x5217;&#x6CBF;&#x65B0;&#x7EF4;&#x5EA6;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x3002;</p>
<p>&#x6240;&#x6709;&#x5F20;&#x91CF;&#x90FD;&#x5FC5;&#x987B;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF;</strong>(<em>&#x5F20;&#x91CF;&#x5E8F;&#x5217;</em>&#xFF09;&#x2013;&#x8FDE;&#x63A5;&#x7684;&#x5F20;&#x91CF;&#x5E8F;&#x5217;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x63D2;&#x5165;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x5FC5;&#x987B;&#x4ECB;&#x4E8E; 0 &#x548C;&#x7EA7;&#x8054;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#x4E4B;&#x95F4;(&#x542B;&#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<hr>
<pre><code>torch.t(input) &#x2192; Tensor&#xB6;
</code></pre><p>&#x671F;&#x671B;<code>input</code>&#x4E3A;&lt; = 2-D &#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x8F6C;&#x7F6E;&#x5C3A;&#x5BF8; 0 &#x548C; 1&#x3002;</p>
<p>&#x5C06;&#x6309;&#x539F;&#x6837;&#x8FD4;&#x56DE; 0-D &#x548C; 1-D &#x5F20;&#x91CF;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x4EE5;&#x5C06; 2-D &#x5F20;&#x91CF;&#x89C6;&#x4E3A;<code>transpose(input, 0, 1)</code>&#x7684;&#x7B80;&#x5199;&#x51FD;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(())
&gt;&gt;&gt; x
tensor(0.1995)
&gt;&gt;&gt; torch.t(x)
tensor(0.1995)
&gt;&gt;&gt; x = torch.randn(3)
&gt;&gt;&gt; x
tensor([ 2.4320, -0.4608,  0.7702])
&gt;&gt;&gt; torch.t(x)
tensor([.2.4320,.-0.4608,..0.7702])
&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.4875,  0.9158, -0.5872],
        [ 0.3938, -0.6929,  0.6932]])
&gt;&gt;&gt; torch.t(x)
tensor([[ 0.4875,  0.3938],
        [ 0.9158, -0.6929],
        [-0.5872,  0.6932]])
</code></pre><hr>
<pre><code>torch.take(input, index) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7D22;&#x5F15;&#x5904;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002; &#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x89C6;&#x4E3A;&#x89C6;&#x4E3A;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002; &#x7ED3;&#x679C;&#x91C7;&#x7528;&#x4E0E;&#x7D22;&#x5F15;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<em>LongTensor</em> )&#x2013;&#x5F20;&#x91CF;&#x7D22;&#x5F15;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],
                        [6, 7, 8]])
&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))
tensor([ 4,  5,  8])
</code></pre><hr>
<pre><code>torch.transpose(input, dim0, dim1) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x662F;<code>input</code>&#x7684;&#x8F6C;&#x7F6E;&#x7248;&#x672C;&#x3002; &#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;<code>dim0</code>&#x548C;<code>dim1</code>&#x88AB;&#x4EA4;&#x6362;&#x3002;</p>
<p>&#x4EA7;&#x751F;&#x7684;<code>out</code>&#x5F20;&#x91CF;&#x4E0E;<code>input</code>&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x5176;&#x57FA;&#x7840;&#x5B58;&#x50A8;&#xFF0C;&#x56E0;&#x6B64;&#x66F4;&#x6539;&#x4E00;&#x4E2A;&#x5185;&#x5BB9;&#x5C06;&#x66F4;&#x6539;&#x53E6;&#x4E00;&#x4E2A;&#x5185;&#x5BB9;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim0</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x8F6C;&#x7F6E;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>dim1</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x8F6C;&#x7F6E;&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 1.0028, -0.9893,  0.5809],
        [-0.1669,  0.7299,  0.4942]])
&gt;&gt;&gt; torch.transpose(x, 0, 1)
tensor([[ 1.0028, -0.1669],
        [-0.9893,  0.7299],
        [ 0.5809,  0.4942]])
</code></pre><hr>
<pre><code>torch.unbind(input, dim=0) &#x2192; seq&#xB6;
</code></pre><p>&#x5220;&#x9664;&#x5F20;&#x91CF;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x4E0A;&#x6240;&#x6709;&#x5207;&#x7247;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5DF2;&#x7ECF;&#x6CA1;&#x6709;&#x5B83;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x89E3;&#x9664;&#x7ED1;&#x5B9A;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x79FB;&#x9664;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],
&gt;&gt;&gt;                            [4, 5, 6],
&gt;&gt;&gt;                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
</code></pre><hr>
<pre><code>torch.unsqueeze(input, dim, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5728;&#x6307;&#x5B9A;&#x4F4D;&#x7F6E;&#x63D2;&#x5165;&#x7684;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E0E;&#x6B64;&#x5F20;&#x91CF;&#x5171;&#x4EAB;&#x76F8;&#x540C;&#x7684;&#x57FA;&#x7840;&#x6570;&#x636E;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>[-input.dim() - 1, input.dim() + 1)</code>&#x8303;&#x56F4;&#x5185;&#x7684;<code>dim</code>&#x503C;&#x3002; &#x8D1F;&#x7684;<code>dim</code>&#x5BF9;&#x5E94;&#x4E8E;<code>dim</code> = <code>dim + input.dim() + 1</code>&#x5904;&#x5E94;&#x7528;&#x7684; <a href="#torch.unsqueeze" title="torch.unsqueeze"><code>unsqueeze()</code></a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x63D2;&#x5165;&#x5355;&#x4F8B;&#x5C3A;&#x5BF8;&#x7684;&#x7D22;&#x5F15;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
&gt;&gt;&gt; torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])
</code></pre><hr>
<pre><code>torch.where()&#xB6;
</code></pre><hr>
<pre><code>torch.where(condition, x, y) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4ECE;<code>x</code>&#x6216;<code>y</code>&#x4E2D;&#x9009;&#x62E9;&#x7684;&#x5143;&#x7D20;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5177;&#x4F53;&#x53D6;&#x51B3;&#x4E8E;<code>condition</code>&#x3002;</p>
<p>&#x8BE5;&#x64CD;&#x4F5C;&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p><img src="img/12ee903e238f296681b1cef26fef0f8f.jpg" alt=""></p>
<p>Note</p>
<p>&#x5F20;&#x91CF;<code>condition</code>&#xFF0C;<code>x</code>&#x548C;<code>y</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x6761;&#x4EF6;</strong> (<a href="tensors.html#torch.BoolTensor" title="torch.BoolTensor"><em>BoolTensor</em></a>)&#x2013;&#x5F53;&#x4E3A; True(&#x975E;&#x96F6;&#xFF09;&#x65F6;&#xFF0C;&#x4EA7;&#x751F; x&#xFF0C;&#x5426;&#x5219;&#x4EA7;&#x751F; y</p>
</li>
<li><p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5728;<code>condition</code>&#x4E3A;<code>True</code>&#x7684;&#x7D22;&#x5F15;&#x5904;&#x9009;&#x62E9;&#x7684;&#x503C;</p>
</li>
<li><p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5728;<code>condition</code>&#x4E3A;<code>False</code>&#x7684;&#x7D22;&#x5F15;&#x5904;&#x9009;&#x62E9;&#x7684;&#x503C;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F62;&#x72B6;&#x5F20;&#x91CF;&#x7B49;&#x4E8E;<code>condition</code>&#xFF0C;<code>x</code>&#xFF0C;<code>y</code>&#x7684;&#x5E7F;&#x64AD;&#x5F62;&#x72B6;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 2)
&gt;&gt;&gt; y = torch.ones(3, 2)
&gt;&gt;&gt; x
tensor([[-0.4620,  0.3139],
        [ 0.3898, -0.7197],
        [ 0.0478, -0.1657]])
&gt;&gt;&gt; torch.where(x &gt; 0, x, y)
tensor([[ 1.0000,  0.3139],
        [ 0.3898,  1.0000],
        [ 0.0478,  1.0000]])
</code></pre><hr>
<pre><code>torch.where(condition) &#x2192; tuple of LongTensor
</code></pre><p><code>torch.where(condition)</code>&#x4E0E;<code>torch.nonzero(condition, as_tuple=True)</code>&#x76F8;&#x540C;&#x3002;</p>
<p>Note</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.nonzero" title="torch.nonzero"><code>torch.nonzero()</code></a> &#x3002;</p>
<h2 id="&#x53D1;&#x7535;&#x673A;">&#x53D1;&#x7535;&#x673A;</h2>
<hr>
<pre><code>class torch._C.Generator(device=&apos;cpu&apos;) &#x2192; Generator&#xB6;
</code></pre><p>&#x521B;&#x5EFA;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x751F;&#x6210;&#x5668;&#x5BF9;&#x8C61;&#xFF0C;&#x8BE5;&#x5BF9;&#x8C61;&#x7BA1;&#x7406;&#x4EA7;&#x751F;&#x4F2A;&#x968F;&#x673A;&#x6570;&#x7684;&#x7B97;&#x6CD5;&#x7684;&#x72B6;&#x6001;&#x3002; &#x5728;&#x8BB8;&#x591A;<a href="#inplace-random-sampling">&#x5C31;&#x5730;&#x968F;&#x673A;&#x91C7;&#x6837;</a>&#x51FD;&#x6570;&#x4E2D;&#x7528;&#x4F5C;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8BBE;&#x5907;</strong>(<code>torch.device</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x751F;&#x6210;&#x5668;&#x6240;&#x9700;&#x7684;&#x8BBE;&#x5907;&#x3002;</p>
<p>Returns</p>
<p>&#x4E00;&#x4E2A; torch.Generator &#x5BF9;&#x8C61;&#x3002;</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">&#x751F;&#x6210;&#x5668;</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cuda = torch.Generator(device=&apos;cuda&apos;)
</code></pre><pre><code>device&#xB6;
</code></pre><p>Generator.device-&gt;&#x8BBE;&#x5907;</p>
<p>&#x83B7;&#x53D6;&#x751F;&#x6210;&#x5668;&#x7684;&#x5F53;&#x524D;&#x8BBE;&#x5907;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.device
device(type=&apos;cpu&apos;)
</code></pre><hr>
<pre><code>get_state() &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x751F;&#x6210;&#x5668;&#x72B6;&#x6001;&#x4E3A;<code>torch.ByteTensor</code>&#x3002;</p>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;<code>torch.ByteTensor</code>&#xFF0C;&#x5176;&#x4E2D;&#x5305;&#x542B;&#x5C06;&#x751F;&#x6210;&#x5668;&#x8FD8;&#x539F;&#x5230;&#x7279;&#x5B9A;&#x65F6;&#x95F4;&#x70B9;&#x7684;&#x6240;&#x6709;&#x5FC5;&#x8981;&#x4F4D;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.get_state()
</code></pre><hr>
<pre><code>initial_seed() &#x2192; int&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x7684;&#x521D;&#x59CB;&#x79CD;&#x5B50;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.initial_seed()
2147483647
</code></pre><hr>
<pre><code>manual_seed(seed) &#x2192; Generator&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x7684;&#x79CD;&#x5B50;&#x3002; &#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&lt;cite&gt;torch.&#x751F;&#x6210;&#x5668;&lt;/cite&gt;&#x5BF9;&#x8C61;&#x3002; &#x5EFA;&#x8BAE;&#x8BBE;&#x7F6E;&#x4E00;&#x4E2A;&#x5927;&#x79CD;&#x5B50;&#xFF0C;&#x5373;&#x4E00;&#x4E2A;&#x5177;&#x6709; 0 &#x548C; 1 &#x4F4D;&#x5E73;&#x8861;&#x7684;&#x6570;&#x5B57;&#x3002; &#x907F;&#x514D;&#x5728;&#x79CD;&#x5B50;&#x4E2D;&#x5305;&#x542B;&#x8BB8;&#x591A; 0 &#x4F4D;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x79CD;&#x5B50;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6240;&#x9700;&#x7684;&#x79CD;&#x5B50;&#x3002;</p>
<p>Returns</p>
<p>An torch.Generator object.</p>
<p>Return type</p>
<p><a href="#torch._C.Generator" title="torch._C.Generator">Generator</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.manual_seed(2147483647)
</code></pre><hr>
<pre><code>seed() &#x2192; int&#xB6;
</code></pre><p>&#x4ECE; std :: random_device &#x6216;&#x5F53;&#x524D;&#x65F6;&#x95F4;&#x83B7;&#x53D6;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x968F;&#x673A;&#x6570;&#xFF0C;&#x5E76;&#x5C06;&#x5176;&#x7528;&#x4F5C;&#x751F;&#x6210;&#x5668;&#x7684;&#x79CD;&#x5B50;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu.seed()
1516516984916
</code></pre><hr>
<pre><code>set_state(new_state) &#x2192; void&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x751F;&#x6210;&#x5668;&#x72B6;&#x6001;&#x3002;</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>Torch.ByteTensor</em> )&#x2013;&#x6240;&#x9700;&#x72B6;&#x6001;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; g_cpu = torch.Generator()
&gt;&gt;&gt; g_cpu_other = torch.Generator()
&gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())
</code></pre><h2 id="&#x968F;&#x673A;&#x62BD;&#x6837;">&#x968F;&#x673A;&#x62BD;&#x6837;</h2>
<hr>
<pre><code>torch.seed()&#xB6;
</code></pre><p>&#x5C06;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x7684;&#x79CD;&#x5B50;&#x8BBE;&#x7F6E;&#x4E3A;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x968F;&#x673A;&#x6570;&#x3002; &#x8FD4;&#x56DE;&#x7528;&#x4E8E;&#x64AD;&#x79CD; RNG &#x7684; 64 &#x4F4D;&#x6570;&#x5B57;&#x3002;</p>
<hr>
<pre><code>torch.manual_seed(seed)&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x7684;&#x79CD;&#x5B50;&#x3002; &#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&lt;cite&gt;torch.&#x751F;&#x6210;&#x5668;&lt;/cite&gt;&#x5BF9;&#x8C61;&#x3002;</p>
<p>Parameters</p>
<p><strong>seed</strong> (<em>python:int</em>) &#x2013; The desired seed.</p>
<hr>
<pre><code>torch.initial_seed()&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x957F;&#x4E3A; Python &lt;cite&gt;long&lt;/cite&gt; &#x7684;&#x7528;&#x4E8E;&#x751F;&#x6210;&#x968F;&#x673A;&#x6570;&#x7684;&#x521D;&#x59CB;&#x79CD;&#x5B50;&#x3002;</p>
<hr>
<pre><code>torch.get_rng_state()&#xB6;
</code></pre><p>&#x4EE5; &lt;cite&gt;torch.ByteTensor&lt;/cite&gt; &#x7684;&#x5F62;&#x5F0F;&#x8FD4;&#x56DE;&#x968F;&#x673A;&#x6570;&#x751F;&#x6210;&#x5668;&#x72B6;&#x6001;&#x3002;</p>
<hr>
<pre><code>torch.set_rng_state(new_state)&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x968F;&#x673A;&#x6570;&#x751F;&#x6210;&#x5668;&#x72B6;&#x6001;&#x3002;</p>
<p>Parameters</p>
<p><strong>new_state</strong>  (<em>torch.ByteTensor</em> )&#x2013;&#x6240;&#x9700;&#x72B6;&#x6001;</p>
<pre><code>torch.default_generator Returns the default CPU torch.Generator&#xB6;
</code></pre><hr>
<pre><code>torch.bernoulli(input, *, generator=None, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4ECE;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x4E2D;&#x63D0;&#x53D6;&#x4E8C;&#x8FDB;&#x5236;&#x968F;&#x673A;&#x6570;(0 &#x6216; 1&#xFF09;&#x3002;</p>
<p><code>input</code>&#x5F20;&#x91CF;&#x5E94;&#x4E3A;&#x5305;&#x542B;&#x7528;&#x4E8E;&#x7ED8;&#x5236;&#x4E8C;&#x8FDB;&#x5236;&#x968F;&#x673A;&#x6570;&#x7684;&#x6982;&#x7387;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x56E0;&#x6B64;&#xFF0C;<code>input</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x503C;&#x90FD;&#x5FC5;&#x987B;&#x5728;&#x4EE5;&#x4E0B;&#x8303;&#x56F4;&#x5185;&#xFF1A;<img src="img/b96a669c7fd5b0f5ea99f4373d6eab2e.jpg" alt="">&#x3002;</p>
<p>&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;<img src="img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" alt="">&#x5143;&#x7D20;&#x5C06;&#x6839;&#x636E;<code>input</code>&#x4E2D;&#x7ED9;&#x51FA;&#x7684;<img src="img/55a50814a1cfd1ab95d71ca3a7f311fb.jpg" alt="">&#x6982;&#x7387;&#x503C;&#x7ED8;&#x5236;&#x4E00;&#x4E2A;<img src="img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" alt="">&#x503C;&#x3002;</p>
<p><img src="img/f1fc132b2ecf782642591904c4f9b7de.jpg" alt=""></p>
<p>&#x8FD4;&#x56DE;&#x7684;<code>out</code>&#x5F20;&#x91CF;&#x4EC5;&#x5177;&#x6709;&#x503C; 0 &#x6216; 1&#xFF0C;&#x5E76;&#x4E14;&#x5177;&#x6709;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p><code>out</code>&#x53EF;&#x4EE5;&#x5177;&#x6709;&#x6574;&#x6570;<code>dtype</code>&#xFF0C;&#x4F46;&#x662F;<code>input</code>&#x5FC5;&#x987B;&#x5177;&#x6709;&#x6D6E;&#x70B9;<code>dtype</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x4F2F;&#x52AA;&#x5229;&#x5206;&#x5E03;&#x7684;&#x6982;&#x7387;&#x503C;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x751F;&#x6210;&#x5668;</strong>(<code>torch.Generator</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x7528;&#x4E8E;&#x91C7;&#x6837;&#x7684;&#x4F2A;&#x968F;&#x673A;&#x6570;&#x751F;&#x6210;&#x5668;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
&gt;&gt;&gt; a
tensor([[ 0.1737,  0.0950,  0.3609],
        [ 0.7148,  0.0289,  0.2676],
        [ 0.9456,  0.8937,  0.7202]])
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 1.,  1.,  1.]])

&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])
&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0
&gt;&gt;&gt; torch.bernoulli(a)
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
</code></pre><hr>
<pre><code>torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) &#x2192; LongTensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6BCF;&#x884C;&#x5305;&#x542B;<code>num_samples</code>&#x7D22;&#x5F15;&#xFF0C;&#x8FD9;&#x4E9B;&#x7D22;&#x5F15;&#x662F;&#x4ECE;&#x4F4D;&#x4E8E;&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x76F8;&#x5E94;&#x884C;&#x4E2D;&#x7684;&#x591A;&#x9879;&#x5F0F;&#x6982;&#x7387;&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#x7684;&#x3002;</p>
<p>Note</p>
<p><code>input</code>&#x7684;&#x884C;&#x4E0D;&#x9700;&#x8981;&#x52A0;&#x603B;&#x4E3A; 1(&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x8FD9;&#x4E9B;&#x503C;&#x7528;&#x4F5C;&#x6743;&#x91CD;&#xFF09;&#xFF0C;&#x4F46;&#x5FC5;&#x987B;&#x4E3A;&#x975E;&#x8D1F;&#x6570;&#xFF0C;&#x6709;&#x9650;&#x4E14;&#x603B;&#x548C;&#x4E3A;&#x975E;&#x96F6;&#x3002;</p>
<p>&#x6839;&#x636E;&#x6BCF;&#x4E2A;&#x6837;&#x672C;&#x7684;&#x91C7;&#x6837;&#x65F6;&#x95F4;&#xFF0C;&#x7D22;&#x5F15;&#x4ECE;&#x5DE6;&#x5230;&#x53F3;&#x6392;&#x5E8F;(&#x7B2C;&#x4E00;&#x4E2A;&#x6837;&#x672C;&#x653E;&#x5728;&#x7B2C;&#x4E00;&#x5217;&#x4E2D;&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x5411;&#x91CF;&#xFF0C;&#x5219;<code>out</code>&#x662F;&#x5927;&#x5C0F;<code>num_samples</code>&#x7684;&#x5411;&#x91CF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x5177;&#x6709; &lt;cite&gt;m&lt;/cite&gt; &#x884C;&#x7684;&#x77E9;&#x9635;&#xFF0C;&#x5219;<code>out</code>&#x662F;&#x5F62;&#x72B6;<img src="img/5d2de3653458e6a14a8a4f0fa87b3ad1.jpg" alt="">&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;&#x66FF;&#x6362;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x62BD;&#x53D6;&#x6837;&#x672C;&#x8FDB;&#x884C;&#x66FF;&#x6362;&#x3002;</p>
<p>&#x5982;&#x679C;&#x6CA1;&#x6709;&#xFF0C;&#x5219;&#x5B83;&#x4EEC;&#x5C06;&#x88AB;&#x66FF;&#x6362;&#x800C;&#x4E0D;&#x4F1A;&#x88AB;&#x7ED8;&#x5236;&#xFF0C;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x5F53;&#x4E3A;&#x4E00;&#x884C;&#x7ED8;&#x5236;&#x6837;&#x672C;&#x7D22;&#x5F15;&#x65F6;&#xFF0C;&#x65E0;&#x6CD5;&#x4E3A;&#x8BE5;&#x884C;&#x518D;&#x6B21;&#x7ED8;&#x5236;&#x5B83;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x7ED8;&#x5236;&#x65F6;&#x4E0D;&#x8FDB;&#x884C;&#x66FF;&#x6362;&#xFF0C;&#x5219;<code>num_samples</code>&#x5FC5;&#x987B;&#x5C0F;&#x4E8E;<code>input</code>&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x6570;&#x76EE;(&#x5982;&#x679C;&#x662F;&#x77E9;&#x9635;&#xFF0C;&#x5219;&#x5FC5;&#x987B;&#x5C0F;&#x4E8E;<code>input</code>&#x6BCF;&#x884C;&#x4E2D;&#x975E;&#x96F6;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x6570;&#x76EE;&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5305;&#x542B;&#x6982;&#x7387;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>num_samples</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8981;&#x7ED8;&#x5236;&#x7684;&#x6837;&#x672C;&#x6570;</p>
</li>
<li><p><strong>&#x66FF;&#x6362;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x5426;&#x4F7F;&#x7528;&#x66FF;&#x6362;&#x7ED8;&#x5236;</p>
</li>
<li><p><strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2013; a pseudorandom number generator for sampling</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights
&gt;&gt;&gt; torch.multinomial(weights, 2)
tensor([1, 2])
&gt;&gt;&gt; torch.multinomial(weights, 4) # ERROR!
RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,
not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])
</code></pre><hr>
<pre><code>torch.normal()&#xB6;
</code></pre><hr>
<pre><code>torch.normal(mean, std, *, generator=None, out=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4ECE;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x7ED9;&#x51FA;&#x7684;&#x72EC;&#x7ACB;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x968F;&#x673A;&#x6570;&#x5F20;&#x91CF;&#x3002;</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> &#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x5143;&#x7D20;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x5747;&#x503C;</p>
<p><a href="#torch.std" title="torch.std"><code>std</code></a> &#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x8F93;&#x51FA;&#x5143;&#x7D20;&#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;</p>
<p><a href="#torch.mean" title="torch.mean"><code>mean</code></a> &#x548C; <a href="#torch.std" title="torch.std"><code>std</code></a> &#x7684;&#x5F62;&#x72B6;&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F;&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x4E2D;&#x5143;&#x7D20;&#x7684;&#x603B;&#x6570;&#x5FC5;&#x987B;&#x76F8;&#x540C;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x5F62;&#x72B6;&#x4E0D;&#x5339;&#x914D;&#x65F6;&#xFF0C;&#x5C06; <a href="#torch.mean" title="torch.mean"><code>mean</code></a> &#x7684;&#x5F62;&#x72B6;&#x7528;&#x4F5C;&#x8FD4;&#x56DE;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5747;&#x503C;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x5747;&#x503C;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>std</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2013; a pseudorandom number generator for sampling</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,
          8.0505,   8.1408,   9.0563,  10.0566])
</code></pre><hr>
<pre><code>torch.normal(mean=0.0, std, out=None) &#x2192; Tensor
</code></pre><p>&#x4E0E;&#x4E0A;&#x9762;&#x7684;&#x529F;&#x80FD;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x5747;&#x503C;&#x5728;&#x6240;&#x6709;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5E73;&#x5747;&#x503C;</strong> (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6240;&#x6709;&#x5206;&#x5E03;&#x7684;&#x5E73;&#x5747;&#x503C;</p>
</li>
<li><p><strong>std</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor of per-element standard deviations</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.))
tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])
</code></pre><hr>
<pre><code>torch.normal(mean, std=1.0, out=None) &#x2192; Tensor
</code></pre><p>&#x4E0E;&#x4E0A;&#x9762;&#x7684;&#x51FD;&#x6570;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x662F;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x5728;&#x6240;&#x6709;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>mean</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor of per-element means</p>
</li>
<li><p><strong>std</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6240;&#x6709;&#x53D1;&#x884C;&#x7248;&#x7684;&#x6807;&#x51C6;&#x5DEE;</p>
</li>
<li><p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.))
tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])
</code></pre><hr>
<pre><code>torch.normal(mean, std, size, *, out=None) &#x2192; Tensor
</code></pre><p>&#x4E0E;&#x4E0A;&#x8FF0;&#x529F;&#x80FD;&#x76F8;&#x4F3C;&#xFF0C;&#x4F46;&#x5747;&#x503C;&#x548C;&#x6807;&#x51C6;&#x5DEE;&#x5728;&#x6240;&#x6709;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;&#x4E4B;&#x95F4;&#x5171;&#x4EAB;&#x3002; &#x6240;&#x5F97;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x7531;<code>size</code>&#x7ED9;&#x51FA;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5E73;&#x5747;&#x503C;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x6240;&#x6709;&#x5206;&#x5E03;&#x7684;&#x5E73;&#x5747;&#x503C;</p>
</li>
<li><p><strong>std</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x6240;&#x6709;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong> (<em>python&#xFF1A;int ...</em> )&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x6574;&#x6570;&#x5E8F;&#x5217;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.normal(2, 3, size=(1, 4))
tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])
</code></pre><hr>
<pre><code>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4ECE;&#x533A;&#x95F4;<img src="img/22e975ba7fcff9173338360452b96179.jpg" alt="">&#x8FD4;&#x56DE;&#x5747;&#x5300;&#x5206;&#x5E03;&#x7684;&#x968F;&#x673A;&#x5F20;&#x91CF;</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x7531;&#x53D8;&#x91CF;&#x53C2;&#x6570;<code>size</code>&#x5B9A;&#x4E49;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> (<em>python:int...</em>) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.rand(4)
tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
&gt;&gt;&gt; torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
        [ 0.3816,  0.7249,  0.0998]])
</code></pre><hr>
<pre><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E0E;<code>input</code>&#x5927;&#x5C0F;&#x76F8;&#x540C;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x7531;&#x95F4;&#x9694;<img src="img/22e975ba7fcff9173338360452b96179.jpg" alt="">&#x4E0A;&#x5747;&#x5300;&#x5206;&#x5E03;&#x7684;&#x968F;&#x673A;&#x6570;&#x586B;&#x5145;&#x3002; <code>torch.rand_like(input)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr>
<pre><code>torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7531;&#x5728;<code>low</code>(&#x5305;&#x62EC;&#xFF09;&#x548C;<code>high</code>(&#x4E0D;&#x5305;&#x62EC;&#xFF09;&#x4E4B;&#x95F4;&#x5747;&#x5300;&#x751F;&#x6210;&#x7684;&#x968F;&#x673A;&#x6574;&#x6570;&#x586B;&#x5145;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x4F4E;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x4ECE;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x6700;&#x4F4E;&#x6574;&#x6570;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>&#x9AD8;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x4ECE;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x6700;&#x9AD8;&#x6574;&#x6570;&#x4E4B;&#x4E0A;&#x4E00;&#x4E2A;&#x3002;</p>
</li>
<li><p><strong>&#x5927;&#x5C0F;</strong>(<em>&#x5143;&#x7EC4;</em>&#xFF09;&#x2013;&#x5B9A;&#x4E49;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5F62;&#x72B6;&#x7684;&#x5143;&#x7EC4;&#x3002;</p>
</li>
<li><p><strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2013; a pseudorandom number generator for sampling</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randint(3, 5, (3,))
tensor([4, 3, 4])

&gt;&gt;&gt; torch.randint(10, (2, 2))
tensor([[0, 2],
        [5, 5]])

&gt;&gt;&gt; torch.randint(3, 10, (2, 2))
tensor([[4, 5],
        [6, 7]])
</code></pre><hr>
<pre><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x4E0E;&#x5F20;&#x91CF;<code>input</code>&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x586B;&#x5145;&#x4E86;&#x5728;<code>low</code>(&#x5305;&#x62EC;&#xFF09;&#x548C;<code>high</code>(&#x6392;&#x9664;&#xFF09;&#x4E4B;&#x95F4;&#x5747;&#x5300;&#x751F;&#x6210;&#x7684;&#x968F;&#x673A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>low</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; Lowest integer to be drawn from the distribution. Default: 0.</p>
</li>
<li><p><strong>high</strong> (<em>python:int</em>) &#x2013; One above the highest integer to be drawn from the distribution.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr>
<pre><code>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4ECE;&#x5E73;&#x5747;&#x503C;&#x4E3A; &lt;cite&gt;0&lt;/cite&gt; &#xFF0C;&#x65B9;&#x5DEE;&#x4E3A; &lt;cite&gt;1&lt;/cite&gt; &#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x4E2D;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x586B;&#x5145;&#x6709;&#x968F;&#x673A;&#x6570;&#x7684;&#x5F20;&#x91CF;(&#x4E5F;&#x79F0;&#x4E3A;&#x6807;&#x51C6;&#x6B63;&#x6001;&#x5206;&#x5E03;&#xFF09;&#x3002;</p>
<p><img src="img/6a5aeab1deaf496af03eb65c0690d32b.jpg" alt=""></p>
<p>The shape of the tensor is defined by the variable argument <code>size</code>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>size</strong> (<em>python:int...</em>) &#x2013; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>).</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randn(4)
tensor([-2.1436,  0.9966,  2.3426, -0.6366])
&gt;&gt;&gt; torch.randn(2, 3)
tensor([[ 1.5954,  2.8929, -1.0923],
        [ 1.1719, -0.4709, -0.1996]])
</code></pre><hr>
<pre><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x7531;&#x5747;&#x503C; 0 &#x548C;&#x65B9;&#x5DEE; 1 &#x7684;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x968F;&#x673A;&#x6570;&#x586B;&#x5145;&#x3002;<code>torch.randn_like(input)</code>&#x7B49;&#x6548;&#x4E8E;<code>torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the size of <code>input</code> will determine size of the output tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<hr>
<pre><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) &#x2192; LongTensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4ECE;<code>0</code>&#x5230;<code>n - 1</code>&#x7684;&#x6574;&#x6570;&#x7684;&#x968F;&#x673A;&#x6392;&#x5217;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x4E0A;&#x9650;(&#x4E0D;&#x5305;&#x62EC;&#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>torch.int64</code>&#x3002;</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned Tensor. Default: <code>torch.strided</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.randperm(4)
tensor([2, 1, 0, 3])
</code></pre><h3 id="&#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837;">&#x5C31;&#x5730;&#x968F;&#x673A;&#x62BD;&#x6837;</h3>
<p>&#x5728; Tensor &#x4E0A;&#x8FD8;&#x5B9A;&#x4E49;&#x4E86;&#x4E00;&#x4E9B;&#x5C31;&#x5730;&#x968F;&#x673A;&#x91C7;&#x6837;&#x51FD;&#x6570;&#x3002; &#x5355;&#x51FB;&#x4EE5;&#x67E5;&#x770B;&#x5176;&#x6587;&#x6863;&#xFF1A;</p>
<ul>
<li><p><a href="tensors.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - <a href="#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a> &#x7684;&#x5C31;&#x5730;&#x7248;&#x672C;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> -&#x4ECE;&#x67EF;&#x897F;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> -&#x4ECE;&#x6307;&#x6570;&#x5206;&#x5E03;&#x4E2D;&#x5F97;&#x51FA;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> -&#x4ECE;&#x51E0;&#x4F55;&#x5206;&#x5E03;&#x4E2D;&#x7ED8;&#x5236;&#x7684;&#x5143;&#x7D20;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> -&#x6765;&#x81EA;&#x5BF9;&#x6570;&#x6B63;&#x6001;&#x5206;&#x5E03;&#x7684;&#x6837;&#x672C;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - <a href="#torch.normal" title="torch.normal"><code>torch.normal()</code></a> &#x7684;&#x5C31;&#x5730;&#x7248;&#x672C;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.random_" title="torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> -&#x4ECE;&#x79BB;&#x6563;&#x5747;&#x5300;&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><a href="tensors.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> -&#x4ECE;&#x8FDE;&#x7EED;&#x5747;&#x5300;&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#x7684;&#x6570;&#x5B57;</p>
</li>
</ul>
<h3 id="&#x51C6;&#x968F;&#x673A;&#x62BD;&#x6837;">&#x51C6;&#x968F;&#x673A;&#x62BD;&#x6837;</h3>
<hr>
<pre><code>class torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)&#xB6;
</code></pre><p><a href="#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code>torch.quasirandom.SobolEngine</code></a> &#x662F;&#x7528;&#x4E8E;&#x751F;&#x6210;(&#x52A0;&#x6270;&#xFF09;Sobol &#x5E8F;&#x5217;&#x7684;&#x5F15;&#x64CE;&#x3002; Sobol &#x5E8F;&#x5217;&#x662F;&#x4F4E;&#x5DEE;&#x5F02;&#x51C6;&#x968F;&#x673A;&#x5E8F;&#x5217;&#x7684;&#x4E00;&#x4E2A;&#x793A;&#x4F8B;&#x3002;</p>
<p>&#x7528;&#x4E8E; Sobol &#x5E8F;&#x5217;&#x7684;&#x5F15;&#x64CE;&#x7684;&#x8FD9;&#x79CD;&#x5B9E;&#x73B0;&#x65B9;&#x5F0F;&#x80FD;&#x591F;&#x5BF9;&#x6700;&#x5927;&#x7EF4;&#x5EA6;&#x4E3A; 1111 &#x7684;&#x5E8F;&#x5217;&#x8FDB;&#x884C;&#x91C7;&#x6837;&#x3002;&#x5B83;&#x4F7F;&#x7528;&#x65B9;&#x5411;&#x7F16;&#x53F7;&#x751F;&#x6210;&#x8FD9;&#x4E9B;&#x5E8F;&#x5217;&#xFF0C;&#x5E76;&#x4E14;&#x8FD9;&#x4E9B;&#x7F16;&#x53F7;&#x5DF2;&#x4ECE;<a href="http://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111" target="_blank">&#x6B64;&#x5904;</a>&#x6539;&#x7F16;&#x800C;&#x6765;&#x3002;</p>
<p>&#x53C2;&#x8003;&#x6587;&#x732E;</p>
<ul>
<li><p>Art B. Owen&#x3002; &#x4E89;&#x593A; Sobol &#x548C; Niederreiter-Xing &#x70B9;&#x3002; &#x590D;&#x6742;&#x6027;&#x6742;&#x5FD7;&#xFF0C;14(4&#xFF09;&#xFF1A;466-489&#xFF0C;1998 &#x5E74; 12 &#x6708;&#x3002;</p>
</li>
<li><p>I. M. Sobol&#x3002; &#x7ACB;&#x65B9;&#x4F53;&#x4E2D;&#x70B9;&#x7684;&#x5206;&#x5E03;&#x548C;&#x79EF;&#x5206;&#x7684;&#x51C6;&#x786E;&#x8BC4;&#x4F30;&#x3002; &#x55EF; Vychisl&#x3002; &#x57AB;&#x3002; &#x6211;&#x5728;&#x3002; Phys&#x3002;&#xFF0C;7&#xFF1A;784-802&#xFF0C;1967&#x3002;</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5C3A;&#x5BF8;</strong> (<em>Int</em> )&#x2013;&#x8981;&#x7ED8;&#x5236;&#x7684;&#x5E8F;&#x5217;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x6270;&#x4E71;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5C06;&#x5176;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#x5C06;&#x4EA7;&#x751F;&#x6270;&#x4E71;&#x7684; Sobol &#x5E8F;&#x5217;&#x3002; &#x52A0;&#x6270;&#x80FD;&#x591F;&#x4EA7;&#x751F;&#x66F4;&#x597D;&#x7684; Sobol &#x5E8F;&#x5217;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
<li><p><strong>&#x79CD;&#x5B50;</strong> (<em>Int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8FD9;&#x662F;&#x52A0;&#x6270;&#x7684;&#x79CD;&#x5B50;&#x3002; &#x5982;&#x679C;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x5C06;&#x968F;&#x673A;&#x6570;&#x751F;&#x6210;&#x5668;&#x7684;&#x79CD;&#x5B50;&#x8BBE;&#x7F6E;&#x4E3A;&#x6B64;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5B83;&#x5C06;&#x4F7F;&#x7528;&#x968F;&#x673A;&#x79CD;&#x5B50;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code></p>
</li>
</ul>
<p>&#x4F8B;&#x5B50;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; soboleng = torch.quasirandom.SobolEngine(dimension=5)
&gt;&gt;&gt; soboleng.draw(3)
tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])
</code></pre><hr>
<pre><code>draw(n=1, out=None, dtype=torch.float32)&#xB6;
</code></pre><p>&#x4ECE; Sobol &#x5E8F;&#x5217;&#x4E2D;&#x7ED8;&#x5236;<code>n</code>&#x70B9;&#x5E8F;&#x5217;&#x7684;&#x529F;&#x80FD;&#x3002; &#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x6837;&#x672C;&#x53D6;&#x51B3;&#x4E8E;&#x5148;&#x524D;&#x7684;&#x6837;&#x672C;&#x3002; &#x7ED3;&#x679C;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<img src="img/4a928fac0225cfecb1270d6afba7caf9.jpg" alt="">&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n</strong>  (<em>Int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x7ED8;&#x5236;&#x70B9;&#x5E8F;&#x5217;&#x7684;&#x957F;&#x5EA6;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1</p>
</li>
<li><p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>dtype</strong> (<code>torch.dtype</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>torch.float32</code></p>
</li>
</ul>
<hr>
<pre><code>fast_forward(n)&#xB6;
</code></pre><p>&#x901A;&#x8FC7;<code>n</code>&#x6B65;&#x9AA4;&#x5FEB;&#x901F;&#x524D;&#x8FDB;<code>SobolEngine</code>&#x72B6;&#x6001;&#x7684;&#x529F;&#x80FD;&#x3002; &#x8FD9;&#x7B49;&#x6548;&#x4E8E;&#x4E0D;&#x4F7F;&#x7528;&#x6837;&#x672C;&#x7ED8;&#x5236;<code>n</code>&#x6837;&#x672C;&#x3002;</p>
<p>Parameters</p>
<p><strong>n</strong>  (<em>Int</em> )&#x2013;&#x5FEB;&#x8FDB;&#x7684;&#x6B65;&#x6570;&#x3002;</p>
<hr>
<pre><code>reset()&#xB6;
</code></pre><p>&#x5C06;<code>SobolEngine</code>&#x91CD;&#x7F6E;&#x4E3A;&#x57FA;&#x672C;&#x72B6;&#x6001;&#x7684;&#x529F;&#x80FD;&#x3002;</p>
<h2 id="&#x5E8F;&#x5217;&#x5316;">&#x5E8F;&#x5217;&#x5316;</h2>
<hr>
<pre><code>torch.save(obj, f, pickle_module=&lt;module &apos;pickle&apos; from &apos;/opt/conda/lib/python3.6/pickle.py&apos;&gt;, pickle_protocol=2, _use_new_zipfile_serialization=False)&#xB6;
</code></pre><p>&#x5C06;&#x5BF9;&#x8C61;&#x4FDD;&#x5B58;&#x5230;&#x78C1;&#x76D8;&#x6587;&#x4EF6;&#x3002;</p>
<p>&#x53E6;&#x8BF7;&#x53C2;&#x89C1;&#xFF1A;<a href="notes/serialization.html#recommend-saving-models">&#x63A8;&#x8350;&#x7684;&#x6A21;&#x578B;&#x4FDD;&#x5B58;&#x65B9;&#x6CD5;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>obj</strong> &#x2013;&#x4FDD;&#x5B58;&#x7684;&#x5BF9;&#x8C61;</p>
</li>
<li><p><strong>f</strong> &#x2013;&#x7C7B;&#x4F3C;&#x4E8E;&#x6587;&#x4EF6;&#x7684;&#x5BF9;&#x8C61;(&#x5FC5;&#x987B;&#x5B9E;&#x73B0;&#x5199;&#x5165;&#x548C;&#x5237;&#x65B0;&#xFF09;&#x6216;&#x5305;&#x542B;&#x6587;&#x4EF6;&#x540D;&#x7684;&#x5B57;&#x7B26;&#x4E32;</p>
</li>
<li><p><strong>pickle_module</strong> &#x2013;&#x7528;&#x4E8E;&#x814C;&#x5236;&#x5143;&#x6570;&#x636E;&#x548C;&#x5BF9;&#x8C61;&#x7684;&#x6A21;&#x5757;</p>
</li>
<li><p><strong>pickle_protocol</strong> &#x2013;&#x53EF;&#x4EE5;&#x6307;&#x5B9A;&#x4E3A;&#x8986;&#x76D6;&#x9ED8;&#x8BA4;&#x534F;&#x8BAE;</p>
</li>
</ul>
<p>Warning</p>
<p>&#x5982;&#x679C;&#x4F7F;&#x7528;&#x7684;&#x662F; Python 2&#xFF0C;&#x5219; <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> &#x4E0D;&#x652F;&#x6301;<code>StringIO.StringIO</code>&#x4F5C;&#x4E3A;&#x6709;&#x6548;&#x7684;&#x7C7B;&#x4F3C;&#x6587;&#x4EF6;&#x7684;&#x5BF9;&#x8C61;&#x3002; &#x8FD9;&#x662F;&#x56E0;&#x4E3A; write &#x65B9;&#x6CD5;&#x5E94;&#x8FD4;&#x56DE;&#x5199;&#x5165;&#x7684;&#x5B57;&#x8282;&#x6570;&#xFF1B; <code>StringIO.write()</code>&#x4E0D;&#x8FD9;&#x6837;&#x505A;&#x3002;</p>
<p>&#x8BF7;&#x6539;&#x7528;<code>io.BytesIO</code>&#x4E4B;&#x7C7B;&#x7684;&#x4E1C;&#x897F;&#x3002;</p>
<p>&#x4F8B;</p>
<pre><code>&gt;&gt;&gt; # Save to file
&gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])
&gt;&gt;&gt; torch.save(x, &apos;tensor.pt&apos;)
&gt;&gt;&gt; # Save to io.BytesIO buffer
&gt;&gt;&gt; buffer = io.BytesIO()
&gt;&gt;&gt; torch.save(x, buffer)
</code></pre><hr>
<pre><code>torch.load(f, map_location=None, pickle_module=&lt;module &apos;pickle&apos; from &apos;/opt/conda/lib/python3.6/pickle.py&apos;&gt;, **pickle_load_args)&#xB6;
</code></pre><p>&#x4ECE;&#x6587;&#x4EF6;&#x52A0;&#x8F7D;&#x7528; <a href="#torch.save" title="torch.save"><code>torch.save()</code></a> &#x4FDD;&#x5B58;&#x7684;&#x5BF9;&#x8C61;&#x3002;</p>
<p><a href="#torch.load" title="torch.load"><code>torch.load()</code></a> &#x4F7F;&#x7528; Python &#x7684;&#x89E3;&#x5F00;&#x5DE5;&#x5177;&#xFF0C;&#x4F46;&#x4F1A;&#x7279;&#x522B;&#x5904;&#x7406;&#x4F4D;&#x4E8E;&#x5F20;&#x91CF;&#x4E4B;&#x4E0B;&#x7684;&#x5B58;&#x50A8;&#x3002; &#x5B83;&#x4EEC;&#x9996;&#x5148;&#x5728; CPU &#x4E0A;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#xFF0C;&#x7136;&#x540E;&#x79FB;&#x5230;&#x4FDD;&#x5B58;&#x5B83;&#x4EEC;&#x7684;&#x8BBE;&#x5907;&#x4E0A;&#x3002; &#x5982;&#x679C;&#x5931;&#x8D25;(&#x4F8B;&#x5982;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD0;&#x884C;&#x65F6;&#x7CFB;&#x7EDF;&#x6CA1;&#x6709;&#x67D0;&#x4E9B;&#x8BBE;&#x5907;&#xFF09;&#xFF0C;&#x5219;&#x4F1A;&#x5F15;&#x53D1;&#x5F02;&#x5E38;&#x3002; &#x4F46;&#x662F;&#xFF0C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>map_location</code>&#x53C2;&#x6570;&#x5C06;&#x5B58;&#x50A8;&#x52A8;&#x6001;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x5230;&#x4E00;&#x7EC4;&#x5907;&#x7528;&#x8BBE;&#x5907;&#x3002;</p>
<p>&#x5982;&#x679C;<code>map_location</code>&#x662F;&#x53EF;&#x8C03;&#x7528;&#x7684;&#xFF0C;&#x5219;&#x5C06;&#x4E3A;&#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x5316;&#x5B58;&#x50A8;&#x8C03;&#x7528;&#x4E00;&#x6B21;&#xFF0C;&#x5E76;&#x5E26;&#x6709;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#xFF1A;storage &#x548C; location&#x3002; storage &#x53C2;&#x6570;&#x5C06;&#x662F;&#x9A7B;&#x7559;&#x5728; CPU &#x4E0A;&#x7684;&#x5B58;&#x50A8;&#x7684;&#x521D;&#x59CB;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x3002; &#x6BCF;&#x4E2A;&#x5E8F;&#x5217;&#x5316;&#x5B58;&#x50A8;&#x90FD;&#x6709;&#x4E00;&#x4E2A;&#x4E0E;&#x4E4B;&#x5173;&#x8054;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#xFF0C;&#x8BE5;&#x6807;&#x7B7E;&#x6807;&#x8BC6;&#x4E86;&#x4ECE;&#x4E2D;&#x8FDB;&#x884C;&#x4FDD;&#x5B58;&#x7684;&#x8BBE;&#x5907;&#xFF0C;&#x8BE5;&#x6807;&#x7B7E;&#x662F;&#x4F20;&#x9012;&#x7ED9;<code>map_location</code>&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x3002; &#x5185;&#x7F6E;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#x662F;&#x7528;&#x4E8E; CPU &#x5F20;&#x91CF;&#x7684;<code>&apos;cpu&apos;</code>&#x548C;&#x7528;&#x4E8E; CUDA &#x5F20;&#x91CF;&#x7684;<code>&apos;cuda:device_id&apos;</code>(&#x4F8B;&#x5982;<code>&apos;cuda:2&apos;</code>&#xFF09;&#x3002; <code>map_location</code>&#x5E94;&#x8BE5;&#x8FD4;&#x56DE;<code>None</code>&#x6216;&#x5B58;&#x50A8;&#x3002; &#x5982;&#x679C;<code>map_location</code>&#x8FD4;&#x56DE;&#x5B58;&#x50A8;&#xFF0C;&#x5B83;&#x5C06;&#x7528;&#x4F5C;&#x6700;&#x7EC8;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x7684;&#x5BF9;&#x8C61;&#xFF0C;&#x5DF2;&#x7ECF;&#x79FB;&#x81F3;&#x6B63;&#x786E;&#x7684;&#x8BBE;&#x5907;&#x3002; &#x5426;&#x5219;&#xFF0C; <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> &#x5C06;&#x9000;&#x56DE;&#x5230;&#x9ED8;&#x8BA4;&#x884C;&#x4E3A;&#xFF0C;&#x5C31;&#x50CF;&#x672A;&#x6307;&#x5B9A;<code>map_location</code>&#x4E00;&#x6837;&#x3002;</p>
<p>&#x5982;&#x679C;<code>map_location</code>&#x662F; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#x5BF9;&#x8C61;&#x6216;&#x4E0E;&#x8BBE;&#x5907;&#x6807;&#x7B7E;&#x51B2;&#x7A81;&#x7684;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;&#x5219;&#x5B83;&#x6307;&#x793A;&#x5E94;&#x52A0;&#x8F7D;&#x6240;&#x6709;&#x5F20;&#x91CF;&#x7684;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5426;&#x5219;&#xFF0C;&#x5982;&#x679C;<code>map_location</code>&#x662F;&#x5B57;&#x5178;&#xFF0C;&#x5B83;&#x5C06;&#x7528;&#x4E8E;&#x5C06;&#x6587;&#x4EF6;(&#x952E;&#xFF09;&#x4E2D;&#x51FA;&#x73B0;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x5230;&#x6307;&#x5B9A;&#x5C06;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;(&#x503C;&#xFF09;&#x653E;&#x7F6E;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#x3002;</p>
<p>&#x7528;&#x6237;&#x6269;&#x5C55;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>torch.serialization.register_package()</code>&#x6CE8;&#x518C;&#x81EA;&#x5DF1;&#x7684;&#x4F4D;&#x7F6E;&#x6807;&#x7B7E;&#x4EE5;&#x53CA;&#x6807;&#x8BB0;&#x548C;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>f</strong> &#x2013;&#x7C7B;&#x4F3C;&#x4E8E;&#x6587;&#x4EF6;&#x7684;&#x5BF9;&#x8C61;(&#x5FC5;&#x987B;&#x5B9E;&#x73B0;<code>read()</code>&#xFF0C;&#xFF1A;meth<code>readline</code>&#xFF0C;&#xFF1A;meth<code>tell</code>&#x548C;&#xFF1A;meth<code>seek</code>&#xFF09;&#x6216;&#x5305;&#x542B;&#x6587;&#x4EF6;&#x540D;&#x7684;&#x5B57;&#x7B26;&#x4E32;</p>
</li>
<li><p><strong>map_location</strong> &#x2013;&#x51FD;&#x6570;&#xFF0C; <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a> &#xFF0C;&#x5B57;&#x7B26;&#x4E32;&#x6216;&#x6307;&#x5B9A;&#x5982;&#x4F55;&#x91CD;&#x65B0;&#x6620;&#x5C04;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;&#x7684;&#x5B57;&#x5178;</p>
</li>
<li><p><strong>pickle_module</strong> &#x2013;&#x7528;&#x4E8E;&#x89E3;&#x5F00;&#x5143;&#x6570;&#x636E;&#x548C;&#x5BF9;&#x8C61;&#x7684;&#x6A21;&#x5757;(&#x5FC5;&#x987B;&#x4E0E;&#x7528;&#x4E8E;&#x5E8F;&#x5217;&#x5316;&#x6587;&#x4EF6;&#x7684;<code>pickle_module</code>&#x5339;&#x914D;&#xFF09;</p>
</li>
<li><p><strong>pickle_load_args</strong> &#x2013;(&#x4EC5;&#x9002;&#x7528;&#x4E8E; Python 3&#xFF09;&#x53EF;&#x9009;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x4F20;&#x9012;&#x7ED9;<code>pickle_module.load()</code>&#x548C;<code>pickle_module.Unpickler()</code>&#xFF0C;&#x4F8B;&#x5982;<code>errors=...</code>&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x5F53;&#x60A8;&#x5728;&#x5305;&#x542B; GPU &#x5F20;&#x91CF;&#x7684;&#x6587;&#x4EF6;&#x4E0A;&#x8C03;&#x7528; <a href="#torch.load" title="torch.load"><code>torch.load()</code></a> &#x65F6;&#xFF0C;&#x8FD9;&#x4E9B;&#x5F20;&#x91CF;&#x5C06;&#x9ED8;&#x8BA4;&#x52A0;&#x8F7D;&#x5230; GPU&#x3002; &#x60A8;&#x53EF;&#x4EE5;&#x5148;&#x8C03;&#x7528;<code>torch.load(.., map_location=&apos;cpu&apos;)</code>&#xFF0C;&#x7136;&#x540E;&#x518D;&#x8C03;&#x7528;<code>load_state_dict()</code>&#xFF0C;&#x4EE5;&#x907F;&#x514D;&#x5728;&#x52A0;&#x8F7D;&#x6A21;&#x578B;&#x68C0;&#x67E5;&#x70B9;&#x65F6; GPU RAM &#x6FC0;&#x589E;&#x3002;</p>
<p>Note</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x5B57;&#x8282;&#x5B57;&#x7B26;&#x4E32;&#x89E3;&#x7801;&#x4E3A;<code>utf-8</code>&#x3002; &#x8FD9;&#x662F;&#x4E3A;&#x4E86;&#x907F;&#x514D;&#x5728; Python 3 &#x4E2D;&#x52A0;&#x8F7D; Python 2 &#x4FDD;&#x5B58;&#x7684;&#x6587;&#x4EF6;&#x65F6;&#x51FA;&#x73B0;&#x5E38;&#x89C1;&#x9519;&#x8BEF;&#x60C5;&#x51B5;<code>UnicodeDecodeError: &apos;ascii&apos; codec can&apos;t decode byte 0x...</code>&#x3002;&#x5982;&#x679C;&#x6B64;&#x9ED8;&#x8BA4;&#x8BBE;&#x7F6E;&#x4E0D;&#x6B63;&#x786E;&#xFF0C;&#x5219;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x989D;&#x5916;&#x7684;<code>encoding</code>&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x6765;&#x6307;&#x5B9A;&#x5E94;&#x5982;&#x4F55;&#x52A0;&#x8F7D;&#x8FD9;&#x4E9B;&#x5BF9;&#x8C61;&#xFF0C;&#x4F8B;&#x5982;<code>encoding=&apos;latin1&apos;</code>&#x4F7F;&#x7528;<code>latin1</code>&#x7F16;&#x7801;&#x5C06;&#x5B83;&#x4EEC;&#x89E3;&#x7801;&#x4E3A;&#x5B57;&#x7B26;&#x4E32;&#xFF0C;<code>encoding=&apos;bytes&apos;</code>&#x5C06;&#x5B83;&#x4EEC;&#x4FDD;&#x7559;&#x4E3A;&#x5B57;&#x8282;&#x6570;&#x7EC4;&#xFF0C;&#x4EE5;&#x540E;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;<code>byte_array.decode(...)</code>&#x8FDB;&#x884C;&#x89E3;&#x7801;&#x3002;</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;)
# Load all tensors onto the CPU
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=torch.device(&apos;cpu&apos;))
# Load all tensors onto the CPU, using a function
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage)
# Load all tensors onto GPU 1
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location=lambda storage, loc: storage.cuda(1))
# Map tensors from GPU 1 to GPU 0
&gt;&gt;&gt; torch.load(&apos;tensors.pt&apos;, map_location={&apos;cuda:1&apos;:&apos;cuda:0&apos;})
# Load tensor from io.BytesIO object
&gt;&gt;&gt; with open(&apos;tensor.pt&apos;, &apos;rb&apos;) as f:
        buffer = io.BytesIO(f.read())
&gt;&gt;&gt; torch.load(buffer)
# Load a module with &apos;ascii&apos; encoding for unpickling
&gt;&gt;&gt; torch.load(&apos;module.pt&apos;, encoding=&apos;ascii&apos;)
</code></pre><h2 id="&#x5E73;&#x884C;&#x6027;">&#x5E73;&#x884C;&#x6027;</h2>
<hr>
<pre><code>torch.get_num_threads() &#x2192; int&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E;&#x5E76;&#x884C;&#x5316; CPU &#x64CD;&#x4F5C;&#x7684;&#x7EBF;&#x7A0B;&#x6570;</p>
<hr>
<pre><code>torch.set_num_threads(int)&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E; CPU &#x4E0A;&#x7684;&#x5185;&#x90E8;&#x8FD0;&#x7B97;&#x5E76;&#x884C;&#x7684;&#x7EBF;&#x7A0B;&#x6570;&#x3002; &#x8B66;&#x544A;&#xFF1A;&#x4E3A;&#x786E;&#x4FDD;&#x4F7F;&#x7528;&#x6B63;&#x786E;&#x7684;&#x7EBF;&#x7A0B;&#x6570;&#xFF0C;&#x5FC5;&#x987B;&#x5728;&#x8FD0;&#x884C; eager&#xFF0C;JIT &#x6216; autograd &#x4EE3;&#x7801;&#x4E4B;&#x524D;&#x8C03;&#x7528; set_num_threads&#x3002;</p>
<hr>
<pre><code>torch.get_num_interop_threads() &#x2192; int&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7528;&#x4E8E; CPU &#x4E0A;&#x7684;&#x4E92;&#x64CD;&#x4F5C;&#x5E76;&#x884C;&#x7684;&#x7EBF;&#x7A0B;&#x6570;(&#x4F8B;&#x5982;&#xFF0C;&#x5728; JIT &#x89E3;&#x91CA;&#x5668;&#x4E2D;&#xFF09;</p>
<hr>
<pre><code>torch.set_num_interop_threads(int)&#xB6;
</code></pre><p>&#x8BBE;&#x7F6E;&#x7528;&#x4E8E; CPU &#x4E0A;&#x7684;&#x4E92;&#x64CD;&#x4F5C;&#x5E76;&#x884C;&#x6027;(&#x4F8B;&#x5982;&#xFF0C;&#x5728; JIT &#x89E3;&#x91CA;&#x5668;&#x4E2D;&#xFF09;&#x7684;&#x7EBF;&#x7A0B;&#x6570;&#x3002; &#x8B66;&#x544A;&#xFF1A;&#x53EA;&#x80FD;&#x5728;&#x4E00;&#x6B21;&#x64CD;&#x4F5C;&#x95F4;&#x5E76;&#x884C;&#x5DE5;&#x4F5C;&#x5F00;&#x59CB;&#x4E4B;&#x524D;(&#x4F8B;&#x5982; JIT &#x6267;&#x884C;&#xFF09;&#x8C03;&#x7528;&#x4E00;&#x6B21;&#x3002;</p>
<h2 id="&#x5C40;&#x90E8;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;">&#x5C40;&#x90E8;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;</h2>
<p>&#x4E0A;&#x4E0B;&#x6587;&#x7BA1;&#x7406;&#x5668;<code>torch.no_grad()</code>&#xFF0C;<code>torch.enable_grad()</code>&#x548C;<code>torch.set_grad_enabled()</code>&#x6709;&#x52A9;&#x4E8E;&#x5C40;&#x90E8;&#x7981;&#x7528;&#x548C;&#x542F;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;&#x3002; &#x6709;&#x5173;&#x5176;&#x7528;&#x6CD5;&#x7684;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;<a href="autograd.html#locally-disable-grad">&#x5C40;&#x90E8;&#x7981;&#x7528;&#x68AF;&#x5EA6;&#x8BA1;&#x7B97;</a>&#x3002; &#x8FD9;&#x4E9B;&#x4E0A;&#x4E0B;&#x6587;&#x7BA1;&#x7406;&#x5668;&#x662F;&#x7EBF;&#x7A0B;&#x672C;&#x5730;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x5982;&#x679C;&#x60A8;&#x4F7F;&#x7528;<code>threading</code>&#x6A21;&#x5757;&#x7B49;&#x5C06;&#x5DE5;&#x4F5C;&#x53D1;&#x9001;&#x5230;&#x53E6;&#x4E00;&#x4E2A;&#x7EBF;&#x7A0B;&#xFF0C;&#x5B83;&#x4EEC;&#x5C06;&#x65E0;&#x6CD5;&#x5DE5;&#x4F5C;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; is_train = False
&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
True

&gt;&gt;&gt; torch.set_grad_enabled(False)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
False
</code></pre><h2 id="&#x6570;&#x5B66;&#x8FD0;&#x7B97;">&#x6570;&#x5B66;&#x8FD0;&#x7B97;</h2>
<h3 id="&#x9010;&#x70B9;&#x64CD;&#x4F5C;">&#x9010;&#x70B9;&#x64CD;&#x4F5C;</h3>
<hr>
<pre><code>torch.abs(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6309;&#x5143;&#x7D20;&#x7684;&#x7EDD;&#x5BF9;&#x503C;&#x3002;</p>
<p><img src="img/726d369abb9c76e751e626cbfef10220.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
</code></pre><hr>
<pre><code>torch.acos(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CD;&#x4F59;&#x5F26;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/25323a5363649a36549d717d5a3cbc3e.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
&gt;&gt;&gt; torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
</code></pre><hr>
<pre><code>torch.add()&#xB6;
</code></pre><hr>
<pre><code>torch.add(input, other, out=None)
</code></pre><p>&#x5C06;&#x6807;&#x91CF;<code>other</code>&#x6DFB;&#x52A0;&#x5230;&#x8F93;&#x5165;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E2D;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/1e18f010799098c127bab9465476d1fb.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x7C7B;&#x578B;&#x4E3A; FloatTensor &#x6216; DoubleTensor&#xFF0C;&#x5219;<code>other</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8981;&#x6DFB;&#x52A0;&#x5230;<code>input</code>&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x7F16;&#x53F7;</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
&gt;&gt;&gt; torch.add(a, 20)
tensor([ 20.0202,  21.0985,  21.3506,  19.3944])
</code></pre><hr>
<pre><code>torch.add(input, alpha=1, other, out=None)
</code></pre><p>&#x5F20;&#x91CF;<code>other</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E58;&#x4EE5;&#x6807;&#x91CF;<code>alpha</code>&#xFF0C;&#x7136;&#x540E;&#x52A0;&#x5230;&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0A;&#x3002; &#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>input</code>&#x548C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p><img src="img/5a5bc06d446342edf134fb0e87f755ec.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>other</code>&#x7684;&#x7C7B;&#x578B;&#x4E3A; FloatTensor &#x6216; DoubleTensor&#xFF0C;&#x5219;<code>alpha</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E00;&#x4E2A;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x6570;&#x5B57;</em>&#xFF09;&#x2013; <code>other</code>&#x7684;&#x6807;&#x91CF;&#x4E58;&#x6CD5;&#x5668;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E8C;&#x4E2A;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.9732, -0.3497,  0.6245,  0.4022])
&gt;&gt;&gt; b = torch.randn(4, 1)
&gt;&gt;&gt; b
tensor([[ 0.3743],
        [-1.7724],
        [-0.5811],
        [-0.8017]])
&gt;&gt;&gt; torch.add(a, 10, b)
tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
        [-18.6971, -18.0736, -17.0994, -17.3216],
        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
</code></pre><hr>
<pre><code>torch.addcdiv(input, value=1, tensor1, tensor2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6267;&#x884C;<code>tensor1</code>&#x9664;&#x4EE5;<code>tensor2</code>&#x7684;&#x5143;&#x7D20;&#xFF0C;&#x5C06;&#x7ED3;&#x679C;&#x4E58;&#x4EE5;&#x6807;&#x91CF;<code>value</code>&#x5E76;&#x5C06;&#x5176;&#x52A0;&#x5230;<code>input</code>&#x4E0A;&#x3002;</p>
<p><img src="img/69053b1e1ce2faac8aae0ee4370775b9.jpg" alt=""></p>
<p><code>input</code>&#xFF0C;<code>tensor1</code>&#x548C;<code>tensor2</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;</a>&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#x7684;&#x8F93;&#x5165;&#xFF0C;<code>value</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6DFB;&#x52A0;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/7adafc45ae24ad65f276f988f5b53f3f.jpg" alt="">&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF; 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5206;&#x5B50;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF; 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5206;&#x6BCD;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)
tensor([[-0.2312, -3.6496,  0.1312],
        [-1.0428,  3.4292, -0.1030],
        [-0.5369, -0.9829,  0.0430]])
</code></pre><hr>
<pre><code>torch.addcmul(input, value=1, tensor1, tensor2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5BF9;<code>tensor1</code>&#x4E0E;<code>tensor2</code>&#x8FDB;&#x884C;&#x5143;&#x7D20;&#x9010;&#x9879;&#x4E58;&#x6CD5;&#xFF0C;&#x5C06;&#x7ED3;&#x679C;&#x4E0E;&#x6807;&#x91CF;<code>value</code>&#x76F8;&#x4E58;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x5176;&#x4E0E;<code>input</code>&#x76F8;&#x52A0;&#x3002;</p>
<p><img src="img/6bfdc94f1a8a13ee1a6ebc9af2d74fda.jpg" alt=""></p>
<p><a href="#torch.tensor" title="torch.tensor"><code>tensor</code></a> &#xFF0C;<code>tensor1</code>&#x548C;<code>tensor2</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> must be a real number, otherwise an integer.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to be added</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/25f6a482a153e2aed6c16040677c1f5e.jpg" alt="">&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF; 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF; 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)
&gt;&gt;&gt; t1 = torch.randn(3, 1)
&gt;&gt;&gt; t2 = torch.randn(1, 3)
&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)
tensor([[-0.8635, -0.6391,  1.6174],
        [-0.7617, -0.5879,  1.7388],
        [-0.8353, -0.6249,  1.6511]])
</code></pre><hr>
<pre><code>torch.angle(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x65B9;&#x5411;&#x89D2;(&#x4EE5;&#x5F27;&#x5EA6;&#x4E3A;&#x5355;&#x4F4D;&#xFF09;&#x3002;</p>
<p><img src="img/5af8c34b3d4c490c508caf11a458d5d6.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159
tensor([ 135.,  135,  -45])
</code></pre><hr>
<pre><code>torch.asin(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CD;&#x6B63;&#x5F26;&#x503C;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/fa8ed00ae85106eea8e8a11b6b66d898.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5962,  1.4985, -0.4396,  1.4525])
&gt;&gt;&gt; torch.asin(a)
tensor([-0.6387,     nan, -0.4552,     nan])
</code></pre><hr>
<pre><code>torch.atan(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CD;&#x6B63;&#x5207;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/8614369d5e7413ce21e9a5d6bcf786cb.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
&gt;&gt;&gt; torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
</code></pre><hr>
<pre><code>torch.atan2(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8003;&#x8651;&#x8C61;&#x9650;&#x7684;<img src="img/c45e59e7ab5c2f7cff1ff37748c0d62b.jpg" alt="">&#x5143;&#x7D20;&#x9010;&#x7EA7;&#x53CD;&#x6B63;&#x5207;&#x3002; &#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x77E2;&#x91CF;<img src="img/2cb0a971340ffd46996a0cf9eb81d376.jpg" alt="">&#x4E0E;&#x77E2;&#x91CF;<img src="img/8456c6ac83b24dbe917ff5a29a771bd7.jpg" alt="">&#x4E4B;&#x95F4;&#x7684;&#x5F27;&#x5EA6;&#x4E3A;&#x7B26;&#x53F7;&#x89D2;&#x3002; (&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;<img src="img/4e409083ee35ea2eeab094ea4f9bf730.jpg" alt="">&#x662F; x &#x5750;&#x6807;&#xFF0C;&#x800C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;<img src="img/0c7468da87ed4da7f144e93eb6e7e60e.jpg" alt="">&#x662F; y &#x5750;&#x6807;&#x3002;&#xFF09;</p>
<p><code>input</code>&#x548C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the first input tensor</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
&gt;&gt;&gt; torch.atan2(a, torch.randn(4))
tensor([ 0.9833,  0.0811, -1.9743, -1.4151])
</code></pre><hr>
<pre><code>torch.bitwise_not(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x6309;&#x4F4D;&#x975E;&#x3002; &#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x662F;&#x6574;&#x6570;&#x6216;&#x5E03;&#x5C14;&#x7C7B;&#x578B;&#x3002; &#x5BF9;&#x4E8E;&#x5E03;&#x5C14;&#x5F20;&#x91CF;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;&#x903B;&#x8F91;&#x975E;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)
</code></pre><hr>
<pre><code>torch.bitwise_xor(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x548C;<code>other</code>&#x7684;&#x6309;&#x4F4D; XOR&#x3002; &#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x662F;&#x6574;&#x6570;&#x6216;&#x5E03;&#x5C14;&#x7C7B;&#x578B;&#x3002; &#x5BF9;&#x4E8E;&#x5E03;&#x5C14;&#x5F20;&#x91CF;&#xFF0C;&#x5B83;&#x8BA1;&#x7B97;&#x903B;&#x8F91; XOR&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> &#x2013;&#x7B2C;&#x4E00;&#x4E2A;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> &#x2013;&#x7B2C;&#x4E8C;&#x4E2A;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example</p>
<pre><code>&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-2, -2,  0], dtype=torch.int8)
&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, False, False])
</code></pre><hr>
<pre><code>torch.ceil(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684; ceil &#x7684;&#x65B0;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5143;&#x7D20;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x6574;&#x6570;&#x3002;</p>
<p><img src="img/3425145dda63005bbc6ff0b2f0331aa7.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.6341, -1.4208, -1.0900,  0.5826])
&gt;&gt;&gt; torch.ceil(a)
tensor([-0., -1., -1.,  1.])
</code></pre><hr>
<pre><code>torch.clamp(input, min, max, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;<code>input</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x9650;&#x5236;&#x5728; &lt;cite&gt;[&lt;/cite&gt; <a href="#torch.min" title="torch.min"><code>min</code></a> &#xFF0C; <a href="#torch.max" title="torch.max"><code>max</code></a> &lt;cite&gt;]&lt;/cite&gt; &#x8303;&#x56F4;&#x5185;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#xFF1A;</p>
<p><img src="img/597a5259be5b1a330eaa6858b12b8994.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#xFF0C;&#x5219;&#x53C2;&#x6570; <a href="#torch.min" title="torch.min"><code>min</code></a> &#x548C; <a href="#torch.max" title="torch.max"><code>max</code></a> &#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x4E3A;&#x5B9E;&#x6570; &#x5E94;&#x8BE5;&#x662F;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>min</strong> (<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8981;&#x94B3;&#x5236;&#x7684;&#x8303;&#x56F4;&#x7684;&#x4E0B;&#x9650;</p>
</li>
<li><p><strong>&#x6700;&#x5927;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8981;&#x94B3;&#x4F4D;&#x7684;&#x8303;&#x56F4;&#x7684;&#x4E0A;&#x9650;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.7120,  0.1734, -0.0478, -0.0922])
&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)
tensor([-0.5000,  0.1734, -0.0478, -0.0922])
</code></pre><hr>
<pre><code>torch.clamp(input, *, min, out=None) &#x2192; Tensor
</code></pre><p>&#x5C06;<code>input</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x9650;&#x5236;&#x4E3A;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; <a href="#torch.min" title="torch.min"><code>min</code></a> &#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#xFF0C;&#x5219;<code>value</code>&#x5E94;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0299, -2.3184,  2.1593, -0.8883])
&gt;&gt;&gt; torch.clamp(a, min=0.5)
tensor([ 0.5000,  0.5000,  2.1593,  0.5000])
</code></pre><hr>
<pre><code>torch.clamp(input, *, max, out=None) &#x2192; Tensor
</code></pre><p>&#x5C06;<code>input</code>&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x9650;&#x5236;&#x4E3A;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; <a href="#torch.max" title="torch.max"><code>max</code></a> &#x3002;</p>
<p>If <code>input</code> is of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, <code>value</code> should be a real number, otherwise it should be an integer.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.7753, -0.4702, -0.4599,  1.1899])
&gt;&gt;&gt; torch.clamp(a, max=0.5)
tensor([ 0.5000, -0.4702, -0.4599,  0.5000])
</code></pre><hr>
<pre><code>torch.conj(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x9010;&#x5143;&#x7D20;&#x5171;&#x8F6D;&#x3002;</p>
<p><img src="img/334473ca71a76cc4b45b56e05f9113be.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])
</code></pre><hr>
<pre><code>torch.cos(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x4F59;&#x5F26;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/a54751040e4e9bbc318ce13170d3575a.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
&gt;&gt;&gt; torch.cos(a)
tensor([ 0.1395,  0.2957,  0.6553,  0.5574])
</code></pre><hr>
<pre><code>torch.cosh(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CC;&#x66F2;&#x4F59;&#x5F26;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/01d5c509dd9312bb70bb293dffb6ee74.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.1632,  1.1835, -0.6979, -0.7325])
&gt;&gt;&gt; torch.cosh(a)
tensor([ 1.0133,  1.7860,  1.2536,  1.2805])
</code></pre><hr>
<pre><code>torch.div()&#xB6;
</code></pre><hr>
<pre><code>torch.div(input, other, out=None) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x8F93;&#x5165;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x9664;&#x4EE5;&#x6807;&#x91CF;<code>other</code>&#xFF0C;&#x7136;&#x540E;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/b67b84d63ecd57a1b4241eca13fb1661.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>input</code>&#x548C;<code>other</code>&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x4E0D;&#x540C;&#xFF0C;&#x5219;&#x6839;&#x636E;&#x7C7B;&#x578B;&#x63D0;&#x5347;<a href="tensor_attributes.html#type-promotion-doc">&#x6587;&#x6863;</a>&#x4E2D;&#x6240;&#x8FF0;&#x7684;&#x89C4;&#x5219;&#x786E;&#x5B9A;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002; ]&#x3002; &#x5982;&#x679C;&#x6307;&#x5B9A;&#x4E86;<code>out</code>&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x5FC5;&#x987B;&#x662F;<a href="tensor_attributes.html#type-promotion-doc">&#x53EF;&#x8F6C;&#x6362;&#x4E3A;</a>&#x5230;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002; &#x6574;&#x6570;&#x9664;&#x4EE5;&#x96F6;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x884C;&#x4E3A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong>(<em>&#x7F16;&#x53F7;</em>&#xFF09;&#x2013;&#x8981;&#x5212;&#x5206;&#x4E3A;<code>input</code>&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x7F16;&#x53F7;</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
&gt;&gt;&gt; torch.div(a, 0.5)
tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])
</code></pre><hr>
<pre><code>torch.div(input, other, out=None) &#x2192; Tensor
</code></pre><p>&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x9664;&#x4EE5;&#x5F20;&#x91CF;<code>other</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x3002; &#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/9fd0198f0d9df7531b6ea4ae384fbd53.jpg" alt=""></p>
<p><code>input</code>&#x548C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;</a>&#x3002; &#x5982;&#x679C;<code>input</code>&#x548C;<code>other</code>&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x4E0D;&#x540C;&#xFF0C;&#x5219;&#x6839;&#x636E;&#x7C7B;&#x578B;&#x63D0;&#x5347;<a href="tensor_attributes.html#type-promotion-doc">&#x6587;&#x6863;</a>&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x89C4;&#x5219;&#x786E;&#x5B9A;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002; ]&#x3002; &#x5982;&#x679C;&#x6307;&#x5B9A;&#x4E86;<code>out</code>&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x5FC5;&#x987B;&#x662F;<a href="tensor_attributes.html#type-promotion-doc">&#x53EF;&#x8F6C;&#x6362;&#x4E3A;</a>&#x5230;&#x6307;&#x5B9A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002; &#x6574;&#x6570;&#x9664;&#x4EE5;&#x96F6;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x884C;&#x4E3A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5206;&#x5B50;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5206;&#x6BCD;&#x5F20;&#x91CF;</p>
</li>
</ul>
<pre><code>Keyword Arguments
</code></pre><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
        [ 0.1815, -1.0111,  0.9805, -1.5923],
        [ 0.1062,  1.4581,  0.7759, -1.2344],
        [-0.1830, -0.0313,  1.1908, -1.4757]])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
&gt;&gt;&gt; torch.div(a, b)
tensor([[-0.4620, -6.6051,  0.5676,  1.2637],
        [ 0.2260, -3.4507, -1.2086,  6.8988],
        [ 0.1322,  4.9764, -0.9564,  5.3480],
        [-0.2278, -0.1068, -1.4678,  6.3936]])
</code></pre><hr>
<pre><code>torch.digamma(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x4E0A;&#x4F3D;&#x9A6C;&#x51FD;&#x6570;&#x7684;&#x5BF9;&#x6570;&#x5BFC;&#x6570;&#x3002;</p>
<p><img src="img/00133aac1a20067edb1ffa26d46a0311.jpg" alt=""></p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7528;&#x4E8E;&#x8BA1;&#x7B97; digamma &#x51FD;&#x6570;&#x7684;&#x5F20;&#x91CF;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
&gt;&gt;&gt; torch.digamma(a)
tensor([-0.5772, -1.9635])
</code></pre><hr>
<pre><code>torch.erf(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002; &#x9519;&#x8BEF;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/8d655f076653c2f1d5a1b07d3f7437ea.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427,  1.0000])
</code></pre><hr>
<pre><code>torch.erfc(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x4E92;&#x8865;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002; &#x4E92;&#x8865;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x5B9A;&#x4E49;&#x5982;&#x4E0B;&#xFF1A;</p>
<p><img src="img/a58cfc335ee9ca516258ccbad689a9fd.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
</code></pre><hr>
<pre><code>torch.erfinv(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x53CD;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x3002; &#x9006;&#x8BEF;&#x5DEE;&#x51FD;&#x6570;&#x5728;<img src="img/cef0a53226b8b835c974e021239fb845.jpg" alt="">&#x8303;&#x56F4;&#x5185;&#x5B9A;&#x4E49;&#x4E3A;&#xFF1A;</p>
<p><img src="img/9e2a92a8e980b6525bc4f1a045bc6f7e.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000,  0.4769,    -inf])
</code></pre><hr>
<pre><code>torch.exp(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;&#x8F93;&#x5165;&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x5143;&#x7D20;&#x6307;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/89fdb5873c4e62755139600cbda25b05.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)]))
tensor([ 1.,  2.])
</code></pre><hr>
<pre><code>torch.expm1(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5143;&#x7D20;&#x7684;&#x6307;&#x6570;&#x4E3A;<code>input</code>&#x7684;&#x8D1F; 1&#x3002;</p>
<p><img src="img/87b2bec9f68b2f04352bccfeb8c06838.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])
</code></pre><hr>
<pre><code>torch.floor(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x4E3A;<code>input</code>&#x7684;&#x4E0B;&#x9650;&#xFF0C;&#x5373;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x6574;&#x6570;&#x3002;</p>
<p><img src="img/660d522c6d1931985ef3cb2c38e0f843.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.8166,  1.5308, -0.2530, -0.2091])
&gt;&gt;&gt; torch.floor(a)
tensor([-1.,  1., -1., -1.])
</code></pre><hr>
<pre><code>torch.fmod(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x9664;&#x6CD5;&#x5143;&#x7D20;&#x7684;&#x4F59;&#x6570;&#x3002;</p>
<p>&#x88AB;&#x9664;&#x6570;&#x548C;&#x9664;&#x6570;&#x53EF;&#x4EE5;&#x540C;&#x65F6;&#x5305;&#x542B;&#x6574;&#x6570;&#x548C;&#x6D6E;&#x70B9;&#x6570;&#x3002; &#x5176;&#x4F59;&#x90E8;&#x5206;&#x4E0E;&#x80A1;&#x606F;<code>input</code>&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x7B26;&#x53F7;&#x3002;</p>
<p>&#x5F53;<code>other</code>&#x662F;&#x5F20;&#x91CF;&#x65F6;&#xFF0C;<code>input</code>&#x548C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x80A1;&#x606F;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>python&#xFF1A;float</em> )&#x2013;&#x9664;&#x6570;&#xFF0C;&#x53EF;&#x4EE5;&#x662F;&#x6570;&#x5B57;&#x6216;&#x6574;&#x6570; &#x4E0E;&#x80A1;&#x606F;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([-1., -0., -1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre><hr>
<pre><code>torch.frac(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5206;&#x6570;&#x90E8;&#x5206;&#x3002;</p>
<p><img src="img/fdd683e9c79ed73d211526708d082e20.jpg" alt=""></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])
</code></pre><hr>
<pre><code>torch.imag(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x9010;&#x5143;&#x7D20; imag &#x503C;&#x3002;</p>
<p><img src="img/23bb4f65e036b1aa01e948c93438b146.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([ 1,  2,  -3])
</code></pre><hr>
<pre><code>torch.lerp(input, end, weight, out=None)&#xB6;
</code></pre><p>&#x6839;&#x636E;&#x6807;&#x91CF;&#x6216;&#x5F20;&#x91CF;<code>weight</code>&#x5BF9;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;<code>start</code>(&#x7531;<code>input</code>&#x7ED9;&#x51FA;&#xFF09;&#x548C;<code>end</code>&#x8FDB;&#x884C;&#x7EBF;&#x6027;&#x63D2;&#x503C;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x6240;&#x5F97;&#x7684;<code>out</code>&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/02b2be2accc587016c8fb3f0c79c2b03.jpg" alt=""></p>
<p><code>start</code>&#x548C;<code>end</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002; &#x5982;&#x679C;<code>weight</code>&#x662F;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>weight</code>&#xFF0C;<code>start</code>&#x548C;<code>end</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5177;&#x6709;&#x8D77;&#x70B9;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x672B;&#x7AEF;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5177;&#x6709;&#x7EC8;&#x70B9;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6743;&#x91CD;</strong> (<em>python&#xFF1A;float</em> <em>&#x6216;</em> <em>tensor</em>&#xFF09;&#x2013;&#x63D2;&#x503C;&#x516C;&#x5F0F;&#x7684;&#x6743;&#x91CD;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; start = torch.arange(1., 5.)
&gt;&gt;&gt; end = torch.empty(4).fill_(10)
&gt;&gt;&gt; start
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; end
tensor([ 10.,  10.,  10.,  10.])
&gt;&gt;&gt; torch.lerp(start, end, 0.5)
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
&gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5))
tensor([ 5.5000,  6.0000,  6.5000,  7.0000])
</code></pre><hr>
<pre><code>torch.lgamma(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x4E0A;&#x4F3D;&#x9A6C;&#x51FD;&#x6570;&#x7684;&#x5BF9;&#x6570;&#x3002;</p>
<p><img src="img/5bc7af83352d4911e4360a1a8b2c3427.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5)
&gt;&gt;&gt; torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])
</code></pre><hr>
<pre><code>torch.log(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x81EA;&#x7136;&#x5BF9;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/6b7510d341734226d626b3362506887e.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
&gt;&gt;&gt; torch.log(a)
tensor([ nan,  nan,  nan,  nan,  nan])
</code></pre><hr>
<pre><code>torch.log10(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4EE5;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x5E95;&#x6570;&#x4E3A;&#x5E95;&#x7684;&#x5BF9;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/585d59455ec4e6d03ca7d2f072b94ebf.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])

&gt;&gt;&gt; torch.log10(a)
tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])
</code></pre><hr>
<pre><code>torch.log1p(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x81EA;&#x7136;&#x5BF9;&#x6570;&#x4E3A;(1 + <code>input</code>&#xFF09;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/a728d50025ad41634df088b6e1222f73.jpg" alt=""></p>
<p>Note</p>
<p>&#x5BF9;&#x4E8E;&#x8F83;&#x5C0F;&#x7684;<code>input</code>&#x503C;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x6BD4; <a href="#torch.log" title="torch.log"><code>torch.log()</code></a> &#x66F4;&#x51C6;&#x786E;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
&gt;&gt;&gt; torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])
</code></pre><hr>
<pre><code>torch.log2(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4EE5;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x5E95;&#x6570;&#x4E3A;&#x5BF9;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/893d53e193443b3650c0541772aa5216.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.rand(5)
&gt;&gt;&gt; a
tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])

&gt;&gt;&gt; torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
</code></pre><hr>
<pre><code>torch.logical_not(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x6309;&#x5143;&#x7D20;&#x903B;&#x8F91;&#x975E;&#x3002; &#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5C06;&#x5177;&#x6709; bool dtype&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E0D;&#x662F;&#x5E03;&#x5C14;&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x5C06;&#x96F6;&#x89C6;&#x4E3A;<code>False</code>&#xFF0C;&#x5C06;&#x975E;&#x96F6;&#x89C6;&#x4E3A;<code>True</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logical_not(torch.tensor([True, False]))
tensor([ False,  True])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))
tensor([ True, False, False])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))
tensor([ True, False, False])
&gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))
tensor([1, 0, 0], dtype=torch.int16)
</code></pre><hr>
<pre><code>torch.logical_xor(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x9010;&#x5143;&#x7D20;&#x903B;&#x8F91; XOR&#x3002; &#x96F6;&#x88AB;&#x89C6;&#x4E3A;<code>False</code>&#xFF0C;&#x975E;&#x96F6;&#x88AB;&#x89C6;&#x4E3A;<code>True</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7528;&#x4E8E;&#x8BA1;&#x7B97; XOR &#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))
tensor([ False, False,  True])
&gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)
&gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)
&gt;&gt;&gt; torch.logical_xor(a, b)
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a.double(), b.double())
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a.double(), b)
tensor([ True,  True, False, False])
&gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True, False, False])
</code></pre><hr>
<pre><code>torch.mul()&#xB6;
</code></pre><hr>
<pre><code>torch.mul(input, other, out=None)
</code></pre><p>&#x5C06;&#x8F93;&#x5165;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x6807;&#x91CF;<code>other</code>&#x76F8;&#x4E58;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/b9353a390a47f91fc123b9db37178994.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#xFF0C;&#x5219;<code>other</code>&#x5E94;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;</p>
<p>Parameters</p>
<ul>
<li><p><strong>{&#x8F93;&#x5165;}</strong> &#x2013;</p>
</li>
<li><p><strong>&#x503C;</strong>(<em>&#x6570;&#x5B57;</em>&#xFF09;&#x2013;&#x8981;&#x4E0E;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x76F8;&#x4E58;&#x7684;&#x6570;&#x5B57;</p>
</li>
<li><p><strong>{out}</strong> &#x2013;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.2015, -0.4255,  2.6087])
&gt;&gt;&gt; torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])
</code></pre><hr>
<pre><code>torch.mul(input, other, out=None)
</code></pre><p>&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E58;&#x4EE5;&#x5F20;&#x91CF;<code>other</code>&#x7684;&#x76F8;&#x5E94;&#x5143;&#x7D20;&#x3002; &#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img src="img/e7c84cbd2cee400eafabf79e53ed4a59.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E00;&#x4E2A;&#x88AB;&#x4E58;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E8C;&#x4E2A;&#x88AB;&#x4E58;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 1)
&gt;&gt;&gt; a
tensor([[ 1.1207],
        [-0.3137],
        [ 0.0700],
        [ 0.8378]])
&gt;&gt;&gt; b = torch.randn(1, 4)
&gt;&gt;&gt; b
tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
&gt;&gt;&gt; torch.mul(a, b)
tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
        [-0.1614, -0.0382,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4312,  0.1019, -0.4394,  1.8753]])
</code></pre><hr>
<pre><code>torch.mvlgamma(input, p) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5143;&#x7D20;&#x5BF9;&#x6570;&#x4E3A;&#x7EF4;&#x5EA6;<img src="img/28f674762c8a83d24f3018848e6314d5.jpg" alt="">&#x7684;&#x591A;&#x5143;&#x5BF9;&#x6570;&#x4F3D;&#x9A6C;&#x51FD;&#x6570; (<a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function" target="_blank">[reference]</a>)&#xFF0C;&#x516C;&#x5F0F;&#x4E3A;</p>
<p><img src="img/e04b3e1a381314038fdb290092d532c3.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;<img src="img/608c3ee390b6bc4569f2210e1e011ed4.jpg" alt="">&#x548C;<img src="img/f7fbd871ee76d8efb8317c342dd5ed9a.jpg" alt="">&#x662F;&#x4F3D;&#x739B;&#x51FD;&#x6570;&#x3002;</p>
<p>&#x5982;&#x679C;&#x4EFB;&#x4F55;&#x5143;&#x7D20;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E;<img src="img/095b4338ff8c0e8670fe99e82033b339.jpg" alt="">&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7528;&#x4E8E;&#x8BA1;&#x7B97;&#x591A;&#x5143;&#x5BF9;&#x6570;&#x4F3D;&#x9A6C;&#x51FD;&#x6570;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>p</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x5C3A;&#x5BF8;&#x6570;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2)
&gt;&gt;&gt; a
tensor([[1.6835, 1.8474, 1.1929],
        [1.0475, 1.7162, 1.4180]])
&gt;&gt;&gt; torch.mvlgamma(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
        [1.0311, 0.3901, 0.5049]])
</code></pre><hr>
<pre><code>torch.neg(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x8D1F;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/95cc4605d1ad5dd05793898a56b9b7c4.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5)
&gt;&gt;&gt; a
tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
&gt;&gt;&gt; torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
</code></pre><hr>
<pre><code>torch.polygamma(n, input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;<code>input</code>&#x4E0A;&#x7684; digamma &#x51FD;&#x6570;&#x7684;<img src="img/e790323c220b523037f6493b45a6be2f.jpg" alt="">&#x5BFC;&#x6570;&#x3002; <img src="img/5131c6ffc9046f0af6229a8f78fea321.jpg" alt="">&#x88AB;&#x79F0;&#x4E3A;&#x591A;&#x4F3D;&#x739B;&#x51FD;&#x6570;&#x7684;&#x9636;&#x6570;&#x3002;</p>
<p><img src="img/b6aa0cf4b0167dab157397df97cd44ad.jpg" alt=""></p>
<p>Note</p>
<p><img src="img/f08d8837b69470f2939526fe3f47dbf5.jpg" alt="">&#x672A;&#x5B9E;&#x73B0;&#x6B64;&#x529F;&#x80FD;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>n</strong>  (<em>python&#xFF1A;int</em> )&#x2013; polygamma &#x51FD;&#x6570;&#x7684;&#x987A;&#x5E8F;</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<pre><code>Example::
</code></pre><pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])
&gt;&gt;&gt; torch.polygamma(1, a)
tensor([1.64493, 4.9348])
</code></pre><hr>
<pre><code>torch.pow()&#xB6;
</code></pre><hr>
<pre><code>torch.pow(input, exponent, out=None) &#x2192; Tensor
</code></pre><p>&#x7528;<code>exponent</code>&#x53D6;<code>input</code>&#x4E2D;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5E42;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x4E0E;&#x7ED3;&#x679C;&#x3002;</p>
<p><code>exponent</code>&#x53EF;&#x4EE5;&#x662F;&#x5355;&#x4E2A;<code>float</code>&#x6570;&#x5B57;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x5177;&#x6709;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#x5143;&#x7D20;&#x6570;&#x7684;&lt;cite&gt;&#x5F20;&#x91CF;&lt;/cite&gt;&#x3002;</p>
<p>&#x5F53;<code>exponent</code>&#x4E3A;&#x6807;&#x91CF;&#x503C;&#x65F6;&#xFF0C;&#x5E94;&#x7528;&#x7684;&#x8FD0;&#x7B97;&#x4E3A;&#xFF1A;</p>
<p><img src="img/71ad83ee7a70b4a097c1393f30ffdf2f.jpg" alt=""></p>
<p>&#x5F53;<code>exponent</code>&#x662F;&#x5F20;&#x91CF;&#x65F6;&#xFF0C;&#x5E94;&#x7528;&#x7684;&#x8FD0;&#x7B97;&#x662F;&#xFF1A;</p>
<p><img src="img/2f65f5d5f3468da870a791f8985d4dba.jpg" alt=""></p>
<p>&#x5F53;<code>exponent</code>&#x662F;&#x5F20;&#x91CF;&#x65F6;&#xFF0C;<code>input</code>&#x548C;<code>exponent</code>&#x7684;&#x5F62;&#x72B6;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6307;&#x6570;</strong> (<em>python&#xFF1A;float</em> <em>&#x6216;</em> <em>tensor</em>&#xFF09;&#x2013;&#x6307;&#x6570;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
&gt;&gt;&gt; torch.pow(a, 2)
tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
&gt;&gt;&gt; exp = torch.arange(1., 5.)

&gt;&gt;&gt; a = torch.arange(1., 5.)
&gt;&gt;&gt; a
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; exp
tensor([ 1.,  2.,  3.,  4.])
&gt;&gt;&gt; torch.pow(a, exp)
tensor([   1.,    4.,   27.,  256.])
</code></pre><hr>
<pre><code>torch.pow(self, exponent, out=None) &#x2192; Tensor
</code></pre><p><code>self</code>&#x662F;&#x6807;&#x91CF;<code>float</code>&#x503C;&#xFF0C;<code>exponent</code>&#x662F;&#x5F20;&#x91CF;&#x3002; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;<code>out</code>&#x4E0E;<code>exponent</code>&#x7684;&#x5F62;&#x72B6;&#x76F8;&#x540C;</p>
<p>&#x5E94;&#x7528;&#x7684;&#x64CD;&#x4F5C;&#x662F;&#xFF1A;</p>
<p><img src="img/8bfb21a8eb8a28ff6ce228b2b58f69e5.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x81EA;&#x6211;</strong> (<em>python&#xFF1A;float</em> )&#x2013;&#x5E42;&#x8FD0;&#x7B97;&#x7684;&#x6807;&#x91CF;&#x57FA;&#x503C;</p>
</li>
<li><p><strong>&#x6307;&#x6570;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6307;&#x6570;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; exp = torch.arange(1., 5.)
&gt;&gt;&gt; base = 2
&gt;&gt;&gt; torch.pow(base, exp)
tensor([  2.,   4.,   8.,  16.])
</code></pre><hr>
<pre><code>torch.real(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x9010;&#x5143;&#x7D20;&#x5B9E;&#x6570;&#x503C;&#x3002;</p>
<p><img src="img/831fe304d759480b4e6403bb0dce7b41.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([ -1,  -2,  3])
</code></pre><hr>
<pre><code>torch.reciprocal(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x5012;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;</p>
<p><img src="img/53b2c49d0eb01403c24924c5088215a4.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.4595, -2.1219, -1.4314,  0.7298])
&gt;&gt;&gt; torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])
</code></pre><hr>
<pre><code>torch.remainder(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>Computes the element-wise remainder of division.</p>
<p>&#x9664;&#x6570;&#x548C;&#x9664;&#x6570;&#x53EF;&#x4EE5;&#x540C;&#x65F6;&#x5305;&#x542B;&#x6574;&#x6570;&#x548C;&#x6D6E;&#x70B9;&#x6570;&#x3002; &#x5176;&#x4F59;&#x90E8;&#x5206;&#x4E0E;&#x9664;&#x6570;&#x7684;&#x7B26;&#x53F7;&#x76F8;&#x540C;&#x3002;</p>
<p>When <code>other</code> is a tensor, the shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the dividend</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>python&#xFF1A;float</em> )&#x2013;&#x9664;&#x6570;&#x53EF;&#x4EE5;&#x662F;&#x6570;&#x5B57;&#x6216;&#x5F20;&#x91CF; &#x4E0E;&#x80A1;&#x606F;&#x5F62;&#x72B6;&#x76F8;&#x540C;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
tensor([ 1.,  0.,  1.,  1.,  0.,  1.])
&gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)
tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])
</code></pre><p>&#x4E5F;&#x53EF;&#x4EE5;&#x770B;&#x770B;</p>
<p><a href="#torch.fmod" title="torch.fmod"><code>torch.fmod()</code></a> &#xFF0C;&#x5B83;&#x7B49;&#x6548;&#x4E8E; C &#x5E93;&#x51FD;&#x6570;<code>fmod()</code>&#x6765;&#x8BA1;&#x7B97;&#x5143;&#x7D20;&#x7684;&#x9664;&#x6CD5;&#x4F59;&#x6570;&#x3002;</p>
<hr>
<pre><code>torch.round(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x90FD;&#x820D;&#x5165;&#x5230;&#x6700;&#x63A5;&#x8FD1;&#x7684;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
&gt;&gt;&gt; torch.round(a)
tensor([ 1.,  1.,  1., -1.])
</code></pre><hr>
<pre><code>torch.rsqrt(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x5E73;&#x65B9;&#x6839;&#x7684;&#x5012;&#x6570;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/b203a1112169097b0c446f64347eee67.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.0370,  0.2970,  1.5420, -0.9105])
&gt;&gt;&gt; torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])
</code></pre><hr>
<pre><code>torch.sigmoid(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684; S &#x5F62;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/9580d259e0be66d0dc07cf0c8c19fc80.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
&gt;&gt;&gt; torch.sigmoid(a)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])
</code></pre><hr>
<pre><code>torch.sign(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7B26;&#x53F7;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/b1970c66011a7d9a1f7a3f76d56b69a5.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3])
&gt;&gt;&gt; a
tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
&gt;&gt;&gt; torch.sign(a)
tensor([ 1., -1.,  0.,  1.])
</code></pre><hr>
<pre><code>torch.sin(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x6B63;&#x5F26;&#x503C;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/b2949ef2a0b48170c3ee143bbcbf9575.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.5461,  0.1347, -2.7266, -0.2746])
&gt;&gt;&gt; torch.sin(a)
tensor([-0.5194,  0.1343, -0.4032, -0.2711])
</code></pre><hr>
<pre><code>torch.sinh(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CC;&#x66F2;&#x6B63;&#x5F26;&#x503C;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/f701007992bbd41bad0c0d47fa17cff9.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.5380, -0.8632, -0.1265,  0.9399])
&gt;&gt;&gt; torch.sinh(a)
tensor([ 0.5644, -0.9744, -0.1268,  1.0845])
</code></pre><hr>
<pre><code>torch.sqrt(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x5E73;&#x65B9;&#x6839;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/8d503a40d472f4e83ed2f136b2d59121.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-2.0755,  1.0226,  0.0831,  0.4806])
&gt;&gt;&gt; torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])
</code></pre><hr>
<pre><code>torch.tan(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x5207;&#x7EBF;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/2b43ec9d6d179b58d128269ab29a3a9e.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-1.2027, -1.7687,  0.4412, -1.3856])
&gt;&gt;&gt; torch.tan(a)
tensor([-2.5930,  4.9859,  0.4722, -5.3366])
</code></pre><hr>
<pre><code>torch.tanh(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x53CC;&#x66F2;&#x6B63;&#x5207;&#x503C;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/7d427ae6124c99de58f1f36b2ae6d5f6.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
&gt;&gt;&gt; torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])
</code></pre><hr>
<pre><code>torch.trunc(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x5177;&#x6709;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x622A;&#x65AD;&#x7684;&#x6574;&#x6570;&#x503C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 3.4742,  0.5466, -0.8008, -0.9079])
&gt;&gt;&gt; torch.trunc(a)
tensor([ 3.,  0., -0., -0.])
</code></pre><h3 id="&#x51CF;&#x5C11;&#x64CD;&#x4F5C;">&#x51CF;&#x5C11;&#x64CD;&#x4F5C;</h3>
<hr>
<pre><code>torch.argmax()&#xB6;
</code></pre><hr>
<pre><code>torch.argmax(input) &#x2192; LongTensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x8FD9;&#x662F; <a href="#torch.max" title="torch.max"><code>torch.max()</code></a> &#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x503C;&#x3002; &#x6709;&#x5173;&#x6B64;&#x65B9;&#x6CD5;&#x7684;&#x786E;&#x5207;&#x8BED;&#x4E49;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x5176;&#x6587;&#x6863;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a)
tensor(0)
</code></pre><hr>
<pre><code>torch.argmax(input, dim, keepdim=False) &#x2192; LongTensor
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E0A;&#x5F20;&#x91CF;&#x6700;&#x5927;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>This is the second value returned by <a href="#torch.max" title="torch.max"><code>torch.max()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x7F29;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x5982;&#x679C;<code>None</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6241;&#x5E73;&#x5316;&#x8F93;&#x5165;&#x7684; argmax&#x3002;</p>
</li>
<li><p><strong>keepdim</strong>  (<em>bool</em> )&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x4FDD;&#x7559;<code>dim</code>&#x3002; &#x5FFD;&#x7565;<code>dim=None</code>&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
        [-0.7401, -0.8805, -0.3402, -1.1936],
        [ 0.4907, -1.3948, -1.0691, -0.3132],
        [-1.6092,  0.5419, -0.2993,  0.3195]])
&gt;&gt;&gt; torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])
</code></pre><hr>
<pre><code>torch.argmin()&#xB6;
</code></pre><hr>
<pre><code>torch.argmin(input) &#x2192; LongTensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x8FD9;&#x662F; <a href="#torch.min" title="torch.min"><code>torch.min()</code></a> &#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x503C;&#x3002; &#x6709;&#x5173;&#x6B64;&#x65B9;&#x6CD5;&#x7684;&#x786E;&#x5207;&#x8BED;&#x4E49;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x5176;&#x6587;&#x6863;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a)
tensor(13)
</code></pre><hr>
<pre><code>torch.argmin(input, dim, keepdim=False, out=None) &#x2192; LongTensor
</code></pre><p>&#x8FD4;&#x56DE;&#x6574;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E0A;&#x5F20;&#x91CF;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>This is the second value returned by <a href="#torch.min" title="torch.min"><code>torch.min()</code></a>. See its documentation for the exact semantics of this method.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x7F29;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x5982;&#x679C;<code>None</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6241;&#x5E73;&#x5316;&#x8F93;&#x5165;&#x7684; argmin&#x3002;</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not. Ignored if <code>dim=None</code>.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
        [ 1.0100, -1.1975, -0.0102, -0.4732],
        [-0.9240,  0.1207, -0.7506, -1.0213],
        [ 1.7809, -1.2960,  0.9384,  0.1438]])
&gt;&gt;&gt; torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])
</code></pre><hr>
<pre><code>torch.dist(input, other, p=2) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;(<code>input</code>-<code>other</code>&#xFF09;&#x7684; p &#x8303;&#x6570;</p>
<p>The shapes of <code>input</code> and <code>other</code> must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x53F3;&#x4FA7;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>p</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8981;&#x8BA1;&#x7B97;&#x7684;&#x8303;&#x6570;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x
tensor([-1.5393, -0.8675,  0.5916,  1.6321])
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; y
tensor([ 0.0967, -1.0511,  0.6295,  0.8360])
&gt;&gt;&gt; torch.dist(x, y, 3.5)
tensor(1.6727)
&gt;&gt;&gt; torch.dist(x, y, 3)
tensor(1.6973)
&gt;&gt;&gt; torch.dist(x, y, 0)
tensor(inf)
&gt;&gt;&gt; torch.dist(x, y, 1)
tensor(2.6537)
</code></pre><hr>
<pre><code>torch.logsumexp(input, dim, keepdim=False, out=None)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x603B;&#x6307;&#x6570;&#x5BF9;&#x6570;&#x3002; &#x8BE5;&#x8BA1;&#x7B97;&#x5728;&#x6570;&#x503C;&#x4E0A;&#x662F;&#x7A33;&#x5B9A;&#x7684;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x7531;&lt;cite&gt;&#x660F;&#x6697;&lt;/cite&gt;&#x7ED9;&#x51FA;&#x7684;&#x603B;&#x548C;&#x6307;&#x6570;<img src="img/36608d1dd28464666846576485c40a7b.jpg" alt="">&#x548C;&#x5176;&#x4ED6;&#x6307;&#x6570;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">&#xFF0C;&#x7ED3;&#x679C;&#x4E3A;</p>
<blockquote>
<p><img src="img/9a755b15a82f2dcda096a2a363073062.jpg" alt=""></p>
</blockquote>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x4F46;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x7684;&#x5927;&#x5C0F;&#x4E3A; 1&#x3002;&#x5426;&#x5219;&#xFF0C;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x51CF;&#x5C11; 1(&#x6216;<code>len(dim)</code>&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#x6216;</em> <em>python&#xFF1A;ints</em> &#x7684;&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x8981;&#x51CF;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>keepdim</strong>  (<em>bool</em> )&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x4FDD;&#x7559;<code>dim</code>&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<pre><code>Example::
</code></pre><pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; torch.logsumexp(a, 1)
tensor([ 0.8442,  1.4322,  0.8711])
</code></pre><hr>
<pre><code>torch.mean()&#xB6;
</code></pre><hr>
<pre><code>torch.mean(input) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x5E73;&#x5747;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.2294, -0.5481,  1.3288]])
&gt;&gt;&gt; torch.mean(a)
tensor(0.3367)
</code></pre><hr>
<pre><code>torch.mean(input, dim, keepdim=False, out=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x5E73;&#x5747;&#x503C;&#x3002; &#x5982;&#x679C;<code>dim</code>&#x662F;&#x5C3A;&#x5BF8;&#x5217;&#x8868;&#xFF0C;&#x8BF7;&#x7F29;&#x5C0F;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
        [-0.9644,  1.0131, -0.6549, -1.4279],
        [-0.2951, -1.3350, -0.7694,  0.5600],
        [ 1.0842, -0.9580,  0.3623,  0.2343]])
&gt;&gt;&gt; torch.mean(a, 1)
tensor([-0.0163, -0.5085, -0.4599,  0.1807])
&gt;&gt;&gt; torch.mean(a, 1, True)
tensor([[-0.0163],
        [-0.5085],
        [-0.4599],
        [ 0.1807]])
</code></pre><hr>
<pre><code>torch.median()&#xB6;
</code></pre><hr>
<pre><code>torch.median(input) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x4E2D;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 1.5219, -1.5212,  0.2202]])
&gt;&gt;&gt; torch.median(a)
tensor(0.2202)
</code></pre><hr>
<pre><code>torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(values, indices)</code>&#xFF0C;&#x5176;&#x4E2D;<code>values</code>&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x4E2D;&#x503C;&#x3002; <code>indices</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x4E2D;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<code>dim</code>&#x662F;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x53EA;&#x662F;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x7F29;&#x5C0F;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>&#x503C;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x7D22;&#x5F15;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x7D22;&#x5F15;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 5)
&gt;&gt;&gt; a
tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
&gt;&gt;&gt; torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
</code></pre><hr>
<pre><code>torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(values, indices)</code>&#xFF0C;&#x5176;&#x4E2D;<code>values</code>&#x662F;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x4F17;&#x6570;&#x503C;&#xFF0C;&#x5373;&#x8BE5;&#x884C;&#x4E2D;&#x6700;&#x5E38;&#x51FA;&#x73B0;&#x7684;&#x503C;&#xFF0C;&#x800C;<code>indices</code>&#x662F;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E; &#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x6A21;&#x5F0F;&#x503C;&#x3002;</p>
<p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x53EA;&#x662F;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>Note</p>
<p>&#x5C1A;&#x672A;&#x4E3A;<code>torch.cuda.Tensor</code>&#x5B9A;&#x4E49;&#x6B64;&#x529F;&#x80FD;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; the dimension to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>values</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor</p>
</li>
<li><p><strong>indices</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output index tensor</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randint(10, (5,))
&gt;&gt;&gt; a
tensor([6, 5, 1, 0, 2])
&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()
&gt;&gt;&gt; torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
</code></pre><hr>
<pre><code>torch.norm(input, p=&apos;fro&apos;, dim=None, keepdim=False, out=None, dtype=None)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x8303;&#x6570;&#x6216;&#x5411;&#x91CF;&#x8303;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>p</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>inf</em> <em>&#xFF0C;</em> <em>-inf</em> <em>&#xFF0C;</em> <em>&apos;&#x6765;&#x56DE;</em> <em>&#xFF0C;</em> <em>&apos;nuc&apos;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;</p>
<p>&#x89C4;&#x8303;&#x7684;&#x987A;&#x5E8F;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&apos;fro&apos;</code>&#x53EF;&#x4EE5;&#x8BA1;&#x7B97;&#x4EE5;&#x4E0B;&#x89C4;&#x8303;&#xFF1A;</p>
<p>| </p>
<p>&#x5965;&#x5FB7;</p>
<p> | </p>
<p>&#x77E9;&#x9635;&#x8303;&#x6570;</p>
<p> | </p>
<p>&#x5411;&#x91CF;&#x8303;&#x6570;</p>
<p> |
| --- | --- | --- |
| &#x6CA1;&#x6709; | Frobenius &#x8303;&#x6570; | 2 &#x8303;&#x6570; |
| &#x6765;&#x56DE; | Frobenius norm | &#x2013; |
| &#x2018;nuc&#x2019; | &#x6838;&#x89C4;&#x8303; | &#x2013; |
| &#x5176;&#x4ED6; | &#x5F53; dim &#x4E3A; None &#x65F6;&#x4F5C;&#x4E3A; vec &#x89C4;&#x8303; | sum(abs(x&#xFF09;<strong> ord&#xFF09;</strong>(1./ord&#xFF09; |</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>2 &#x4E2A;&#x5143;&#x7EC4;&#x7684; python&#xFF1A;ints</em> <em>&#xFF0C;</em> <em>2 &#x4E2A;&#x5217;&#x8868; python&#xFF1A;ints</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;&#x4E3A; int&#xFF0C;&#x5219;&#x5C06;&#x8BA1;&#x7B97;&#x5411;&#x91CF;&#x8303;&#x6570;&#xFF0C;&#x5982;&#x679C;&#x4E3A; int &#x7684; 2 &#x5143;&#x7EC4;&#xFF0C;&#x5219;&#x5C06;&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x8303;&#x6570;&#x3002; &#x5982;&#x679C;&#x503C;&#x4E3A; None&#xFF0C;&#x5219;&#x5728;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x53EA;&#x6709;&#x4E8C;&#x7EF4;&#x65F6;&#x5C06;&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x8303;&#x6570;&#xFF0C;&#x800C;&#x5728;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x53EA;&#x6709;&#x4E00;&#x7EF4;&#x65F6;&#x5C06;&#x8BA1;&#x7B97;&#x5411;&#x91CF;&#x8303;&#x6570;&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E24;&#x4E2A;&#x4EE5;&#x4E0A;&#x7684;&#x7EF4;&#xFF0C;&#x5219;&#x77E2;&#x91CF;&#x8303;&#x6570;&#x5C06;&#x5E94;&#x7528;&#x4E8E;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x7EF4;&#x3002;</p>
</li>
<li><p><strong>keepdim</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x662F;&#x5426;&#x4FDD;&#x7559;<code>dim</code>&#x3002; &#x5982;&#x679C;<code>dim</code> = <code>None</code>&#x548C;<code>out</code> = <code>None</code>&#x5219;&#x5FFD;&#x7565;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>dim</code> = <code>None</code>&#x548C;<code>out</code> = <code>None</code>&#x5219;&#x5FFD;&#x7565;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x5982;&#x679C;&#x5DF2;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x5728;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x65F6;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5F3A;&#x5236;&#x8F6C;&#x6362;&#x4E3A;&#xFF1A;attr&#xFF1A;&#x201C; dtype&#x201D;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4
&gt;&gt;&gt; b = a.reshape((3, 3))
&gt;&gt;&gt; torch.norm(a)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(b)
tensor(7.7460)
&gt;&gt;&gt; torch.norm(a, float(&apos;inf&apos;))
tensor(4.)
&gt;&gt;&gt; torch.norm(b, float(&apos;inf&apos;))
tensor(4.)
&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
&gt;&gt;&gt; torch.norm(c, dim=0)
tensor([1.4142, 2.2361, 5.0000])
&gt;&gt;&gt; torch.norm(c, dim=1)
tensor([3.7417, 4.2426])
&gt;&gt;&gt; torch.norm(c, p=1, dim=1)
tensor([6., 6.])
&gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
&gt;&gt;&gt; torch.norm(d, dim=(1,2))
tensor([ 3.7417, 11.2250])
&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
(tensor(3.7417), tensor(11.2250))
</code></pre><hr>
<pre><code>torch.prod()&#xB6;
</code></pre><hr>
<pre><code>torch.prod(input, dtype=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x4E58;&#x79EF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x5982;&#x679C;&#x6307;&#x5B9A;&#xFF0C;&#x5219;&#x5728;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x4E4B;&#x524D;&#x5C06;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A;<code>dtype</code>&#x3002; &#x8FD9;&#x5BF9;&#x4E8E;&#x9632;&#x6B62;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x6EA2;&#x51FA;&#x5F88;&#x6709;&#x7528;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x65E0;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8020,  0.5428, -1.5854]])
&gt;&gt;&gt; torch.prod(a)
tensor(0.6902)
</code></pre><hr>
<pre><code>torch.prod(input, dim, keepdim=False, dtype=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x4E58;&#x79EF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x4F46;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x7684;&#x5927;&#x5C0F;&#x4E3A; 1&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; the dimension to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)
&gt;&gt;&gt; a
tensor([[ 0.5261, -0.3837],
        [ 1.1857, -0.2498],
        [-1.1646,  0.0705],
        [ 1.1131, -1.0629]])
&gt;&gt;&gt; torch.prod(a, 1)
tensor([-0.2018, -0.2962, -0.0821, -1.1831])
</code></pre><hr>
<pre><code>torch.std()&#xB6;
</code></pre><hr>
<pre><code>torch.std(input, unbiased=True) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002;</p>
<p>&#x5982;&#x679C;<code>unbiased</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x5C06;&#x901A;&#x8FC7;&#x6709;&#x504F;&#x4F30;&#x8BA1;&#x91CF;&#x8BA1;&#x7B97;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x4F7F;&#x7528;&#x8D1D;&#x585E;&#x5C14;&#x7684;&#x66F4;&#x6B63;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x65E0;&#x504F;</strong> (<em>bool</em> )&#x2013;&#x662F;&#x5426;&#x4F7F;&#x7528;&#x65E0;&#x504F;&#x4F30;&#x8BA1;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.8166, -1.3802, -0.3560]])
&gt;&gt;&gt; torch.std(a)
tensor(0.5130)
</code></pre><hr>
<pre><code>torch.std(input, dim, keepdim=False, unbiased=True, out=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x5728;&#x6807;&#x51C6;<code>dim</code>&#x4E2D;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x3002; &#x5982;&#x679C;<code>dim</code>&#x662F;&#x5C3A;&#x5BF8;&#x5217;&#x8868;&#xFF0C;&#x8BF7;&#x7F29;&#x5C0F;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],
        [ 1.5027, -0.3270,  0.5905,  0.6538],
        [-1.5745,  1.3330, -0.5596, -0.6548],
        [ 0.1264, -0.5080,  1.6420,  0.1992]])
&gt;&gt;&gt; torch.std(a, dim=1)
tensor([ 1.0311,  0.7477,  1.2204,  0.9087])
</code></pre><hr>
<pre><code>torch.std_mean()&#xB6;
</code></pre><hr>
<pre><code>torch.std_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6807;&#x51C6;&#x5DEE;&#x548C;&#x5747;&#x503C;&#x3002;</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.3364, 0.3591, 0.9462]])
&gt;&gt;&gt; torch.std_mean(a)
(tensor(0.3457), tensor(0.5472))
</code></pre><hr>
<pre><code>torch.std(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x8FD4;&#x56DE;<code>dim</code>&#x5F20;&#x91CF;&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x6807;&#x51C6;&#x504F;&#x5DEE;&#x548C;&#x5747;&#x503C;&#x3002; &#x5982;&#x679C;<code>dim</code>&#x662F;&#x5C3A;&#x5BF8;&#x5217;&#x8868;&#xFF0C;&#x8BF7;&#x7F29;&#x5C0F;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],
        [ 0.9267,  1.0612,  1.1050, -0.6014],
        [ 0.0154,  1.9301,  0.0125, -1.0904],
        [-1.9711, -0.7748, -1.3840,  0.5067]])
&gt;&gt;&gt; torch.std_mean(a, 1)
(tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))
</code></pre><hr>
<pre><code>torch.sum()&#xB6;
</code></pre><hr>
<pre><code>torch.sum(input, dtype=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x603B;&#x548C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.1133, -0.9567,  0.2958]])
&gt;&gt;&gt; torch.sum(a)
tensor(-0.5475)
</code></pre><hr>
<pre><code>torch.sum(input, dim, keepdim=False, dtype=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x603B;&#x548C;&#x3002; &#x5982;&#x679C;<code>dim</code>&#x662F;&#x5C3A;&#x5BF8;&#x5217;&#x8868;&#xFF0C;&#x8BF7;&#x7F29;&#x5C0F;&#x6240;&#x6709;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
        [-0.2993,  0.9138,  0.9337, -1.6864],
        [ 0.1132,  0.7892, -0.1003,  0.5688],
        [ 0.3637, -0.9906, -0.4752, -1.5197]])
&gt;&gt;&gt; torch.sum(a, 1)
tensor([-0.4598, -0.1381,  1.3708, -2.6217])
&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)
&gt;&gt;&gt; torch.sum(b, (2, 1))
tensor([  435.,  1335.,  2235.,  3135.])
</code></pre><hr>
<pre><code>torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x7684;&#x552F;&#x4E00;&#x5143;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x4E0E; <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> &#x4E0D;&#x540C;&#xFF0C;&#x56E0;&#x4E3A;&#x8BE5;&#x529F;&#x80FD;&#x8FD8;&#x6D88;&#x9664;&#x4E86;&#x975E;&#x8FDE;&#x7EED;&#x7684;&#x91CD;&#x590D;&#x503C;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;&#x524D;&#xFF0C;&#x5728; CUDA &#x5B9E;&#x73B0;&#x548C; CPU &#x5B9E;&#x73B0;&#x4E2D;&#xFF0C;&#x5F53;&#x6307;&#x5B9A; dim &#x65F6;&#xFF0C;&#x65E0;&#x8BBA; &lt;cite&gt;sort&lt;/cite&gt; &#x53C2;&#x6570;&#x5982;&#x4F55;&#xFF0C; &lt;cite&gt;torch.unique&lt;/cite&gt; &#x59CB;&#x7EC8;&#x5728;&#x5F00;&#x59CB;&#x65F6;&#x5BF9;&#x5F20;&#x91CF;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002; &#x6392;&#x5E8F;&#x53EF;&#x80FD;&#x4F1A;&#x5F88;&#x6162;&#xFF0C;&#x56E0;&#x6B64;&#x5982;&#x679C;&#x60A8;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x5DF2;&#x88AB;&#x6392;&#x5E8F;&#xFF0C;&#x5EFA;&#x8BAE;&#x4F7F;&#x7528; <a href="#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> &#x4EE5;&#x907F;&#x514D;&#x6392;&#x5E8F;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>&#x6392;&#x5E8F;&#x7684;</strong> (<em>bool</em> )&#x2013;&#x5728;&#x8FD4;&#x56DE;&#x4E3A;&#x8F93;&#x51FA;&#x4E4B;&#x524D;&#x662F;&#x5426;&#x6309;&#x5347;&#x5E8F;&#x5BF9;&#x552F;&#x4E00;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002;</p>
</li>
<li><p><strong>return_inverse</strong>  (<em>bool</em> )&#x2013;&#x662F;&#x5426;&#x8FD8;&#x8FD4;&#x56DE;&#x539F;&#x59CB;&#x8F93;&#x5165;&#x4E2D;&#x5143;&#x7D20;&#x5728;&#x8FD4;&#x56DE;&#x7684;&#x552F;&#x4E00;&#x5217;&#x8868;&#x4E2D;&#x6240;&#x5904;&#x4F4D;&#x7F6E;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
</li>
<li><p><strong>return_counts</strong>  (<em>bool</em> )&#x2013;&#x662F;&#x5426;&#x8FD8;&#x8FD4;&#x56DE;&#x6BCF;&#x4E2A;&#x552F;&#x4E00;&#x5143;&#x7D20;&#x7684;&#x8BA1;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5E94;&#x7528;&#x552F;&#x4E00;&#x5C3A;&#x5BF8;&#x3002; &#x5982;&#x679C;<code>None</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x62FC;&#x5408;&#x8F93;&#x5165;&#x7684;&#x552F;&#x4E00;&#x6027;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x6216;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#x5305;&#x542B;</p>
<blockquote>
<ul>
<li><strong>&#x8F93;&#x51FA;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x552F;&#x4E00;&#x6807;&#x91CF;&#x5143;&#x7D20;&#x7684;&#x8F93;&#x51FA;&#x5217;&#x8868;&#x3002;</li>
</ul>
<ul>
<li><strong>inverse_indices</strong> (<em>tensor</em>&#xFF09;&#xFF1A;(&#x53EF;&#x9009;&#xFF09;&#x5982;&#x679C;<code>return_inverse</code>&#x4E3A; True&#xFF0C;&#x5C06;&#x6709;&#x4E00;&#x4E2A;&#x989D;&#x5916;&#x7684;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;(&#x5F62;&#x72B6;&#x4E0E;&#x8F93;&#x5165;&#x76F8;&#x540C;&#xFF09;&#x8868;&#x793A;&#x539F;&#x59CB;&#x8F93;&#x5165;&#x4E2D;&#x5143;&#x7D20;&#x6240;&#x5728;&#x4F4D;&#x7F6E;&#x7684;&#x7D22;&#x5F15; &#x6620;&#x5C04;&#x5230;&#x8F93;&#x51FA;&#x4E2D;&#xFF1B; &#x5426;&#x5219;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x5C06;&#x4EC5;&#x8FD4;&#x56DE;&#x5355;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</li>
</ul>
<ul>
<li><strong>&#x8BA1;&#x6570;&#x4E3A;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;(&#x53EF;&#x9009;&#xFF09;&#x5982;&#x679C;<code>return_counts</code>&#x4E3A; True&#xFF0C;&#x5219;&#x5C06;&#x6709;&#x4E00;&#x4E2A;&#x989D;&#x5916;&#x7684;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;(&#x4E0E; output &#x6216; output.size(dim&#xFF09;&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x5982;&#x679C; dim &#x4E3A; &#x4EE3;&#x8868;&#x6BCF;&#x4E2A;&#x552F;&#x4E00;&#x503C;&#x6216;&#x5F20;&#x91CF;&#x7684;&#x51FA;&#x73B0;&#x6B21;&#x6570;&#x3002;</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>(&#x53EF;&#x9009;&#xFF09;&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>(&#x53EF;&#x9009;&#xFF09;&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
&gt;&gt;&gt; output
tensor([ 2,  3,  1])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([ 0,  2,  1,  2])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([ 1,  2,  3])
&gt;&gt;&gt; inverse_indices
tensor([[ 0,  2],
        [ 1,  2]])
</code></pre><hr>
<pre><code>torch.unique_consecutive(input, return_inverse=False, return_counts=False, dim=None)&#xB6;
</code></pre><p>&#x4ECE;&#x6BCF;&#x4E2A;&#x8FDE;&#x7EED;&#x7684;&#x7B49;&#x6548;&#x5143;&#x7D20;&#x7EC4;&#x4E2D;&#x9664;&#x53BB;&#x9664;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x5916;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002;</p>
<p>Note</p>
<p>&#x5728;&#x6B64;&#x529F;&#x80FD;&#x4EC5;&#x6D88;&#x9664;&#x8FDE;&#x7EED;&#x91CD;&#x590D;&#x503C;&#x7684;&#x610F;&#x4E49;&#x4E0A;&#xFF0C;&#x6B64;&#x529F;&#x80FD;&#x4E0E; <a href="#torch.unique" title="torch.unique"><code>torch.unique()</code></a> &#x4E0D;&#x540C;&#x3002; &#x6B64;&#x8BED;&#x4E49;&#x7C7B;&#x4F3C;&#x4E8E; C ++&#x4E2D;&#x7684; &lt;cite&gt;std :: unique&lt;/cite&gt; &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>return_inverse</strong> (<em>bool</em>) &#x2013; Whether to also return the indices for where elements in the original input ended up in the returned unique list.</p>
</li>
<li><p><strong>return_counts</strong> (<em>bool</em>) &#x2013; Whether to also return the counts for each unique element.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; the dimension to apply unique. If <code>None</code>, the unique of the flattened input is returned. default: <code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>A tensor or a tuple of tensors containing</p>
<blockquote>
<ul>
<li><strong>output</strong> (<em>Tensor</em>): the output list of unique scalar elements.</li>
</ul>
<ul>
<li><strong>inverse_indices</strong> (<em>Tensor</em>): (optional) if <code>return_inverse</code> is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor.</li>
</ul>
<ul>
<li><strong>counts</strong> (<em>Tensor</em>): (optional) if <code>return_counts</code> is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional), <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])
&gt;&gt;&gt; output = torch.unique_consecutive(x)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])

&gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; inverse_indices
tensor([0, 0, 1, 1, 2, 3, 3, 4])

&gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True)
&gt;&gt;&gt; output
tensor([1, 2, 3, 1, 2])
&gt;&gt;&gt; counts
tensor([2, 2, 1, 2, 1])
</code></pre><hr>
<pre><code>torch.var()&#xB6;
</code></pre><hr>
<pre><code>torch.var(input, unbiased=True) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x65B9;&#x5DEE;&#x3002;</p>
<p>&#x5982;&#x679C;<code>unbiased</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x5C06;&#x901A;&#x8FC7;&#x6709;&#x504F;&#x4F30;&#x8BA1;&#x91CF;&#x8BA1;&#x7B97;&#x65B9;&#x5DEE;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x4F7F;&#x7528;&#x8D1D;&#x585E;&#x5C14;&#x7684;&#x66F4;&#x6B63;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[-0.3425, -1.2636, -0.4864]])
&gt;&gt;&gt; torch.var(a)
tensor(0.2455)
</code></pre><hr>
<pre><code>torch.var(input, dim, keepdim=False, unbiased=True, out=None) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x65B9;&#x5DEE;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.3567,  1.7385, -1.3042,  0.7423],
        [ 1.3436, -0.1015, -0.9834, -0.8438],
        [ 0.6056,  0.1089, -0.3112, -1.4085],
        [-0.7700,  0.6074, -0.1469,  0.7777]])
&gt;&gt;&gt; torch.var(a, 1)
tensor([ 1.7444,  1.1363,  0.7356,  0.5112])
</code></pre><hr>
<pre><code>torch.var_mean()&#xB6;
</code></pre><hr>
<pre><code>torch.var_mean(input, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x65B9;&#x5DEE;&#x548C;&#x5747;&#x503C;&#x3002;</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[0.0146, 0.4258, 0.2211]])
&gt;&gt;&gt; torch.var_mean(a)
(tensor(0.0423), tensor(0.2205))
</code></pre><hr>
<pre><code>torch.var_mean(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)
</code></pre><p>&#x8FD4;&#x56DE;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x65B9;&#x5DEE;&#x548C;&#x5747;&#x503C;&#x3002;</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p>
<p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel&#x2019;s correction will be used.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em> <em>or</em> <em>tuple of python:ints</em>) &#x2013; the dimension or dimensions to reduce.</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>unbiased</strong> (<em>bool</em>) &#x2013; whether to use the unbiased estimation or not</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.5650,  2.0415, -0.1024, -0.5790],
        [ 0.2325, -2.6145, -1.6428, -0.3537],
        [-0.2159, -1.1069,  1.2882, -1.3265],
        [-0.6706, -1.5893,  0.6827,  1.6727]])
&gt;&gt;&gt; torch.var_mean(a, 1)
(tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))
</code></pre><h3 id="&#x6BD4;&#x8F83;&#x884C;&#x52A8;">&#x6BD4;&#x8F83;&#x884C;&#x52A8;</h3>
<hr>
<pre><code>torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) &#x2192; bool&#xB6;
</code></pre><p>&#x6B64;&#x51FD;&#x6570;&#x68C0;&#x67E5;<code>input</code>&#x548C;<code>other</code>&#x662F;&#x5426;&#x90FD;&#x6EE1;&#x8DB3;&#x4EE5;&#x4E0B;&#x6761;&#x4EF6;&#xFF1A;</p>
<p><img src="img/cbd6149aa59b9b03eadc2c023182fe68.jpg" alt=""></p>
<p>&#x5BF9;&#x4E8E;<code>input</code>&#x548C;<code>other</code>&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#xFF0C;&#x90FD;&#x662F;&#x9010;&#x5143;&#x7D20;&#x7684;&#x3002; &#x6B64;&#x51FD;&#x6570;&#x7684;&#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E; <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html" target="_blank">numpy.allclose</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6BD4;&#x8F83;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6BD4;&#x8F83;&#x7684;&#x7B2C;&#x4E8C;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>atol</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x7EDD;&#x5BF9;&#x516C;&#x5DEE;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1e-08</p>
</li>
<li><p><strong>rtol</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x76F8;&#x5BF9;&#x516C;&#x5DEE;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1e-05</p>
</li>
<li><p><strong>equal_nan</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;<code>True</code>&#xFF0C;&#x5219;&#x5C06;&#x4E24;&#x4E2A;<code>NaN</code> s &#x76F8;&#x7B49;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
True
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&apos;nan&apos;)]), torch.tensor([1.0, float(&apos;nan&apos;)]))
False
&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float(&apos;nan&apos;)]), torch.tensor([1.0, float(&apos;nan&apos;)]), equal_nan=True)
True
</code></pre><hr>
<pre><code>torch.argsort(input, dim=-1, descending=False, out=None) &#x2192; LongTensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x6309;&#x503C;&#x5347;&#x5E8F;&#x5BF9;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x5F20;&#x91CF;&#x6392;&#x5E8F;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x8FD9;&#x662F; <a href="#torch.sort" title="torch.sort"><code>torch.sort()</code></a> &#x8FD4;&#x56DE;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x503C;&#x3002; &#x6709;&#x5173;&#x6B64;&#x65B9;&#x6CD5;&#x7684;&#x786E;&#x5207;&#x8BED;&#x4E49;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x5176;&#x6587;&#x6863;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8981;&#x6392;&#x5E8F;&#x7684;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>&#x964D;&#x5E8F;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x6392;&#x5E8F;&#x987A;&#x5E8F;(&#x5347;&#x5E8F;&#x6216;&#x964D;&#x5E8F;&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
        [ 0.1598,  0.0788, -0.0745, -1.2700],
        [ 1.2208,  1.0722, -0.7064,  1.2564],
        [ 0.0669, -0.2318, -0.8229, -0.9280]])

&gt;&gt;&gt; torch.argsort(a, dim=1)
tensor([[2, 0, 3, 1],
        [3, 2, 1, 0],
        [2, 1, 0, 3],
        [3, 2, 1, 0]])
</code></pre><hr>
<pre><code>torch.eq(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x6309;&#x5143;&#x7D20;&#x76F8;&#x7B49;</p>
<p>&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x662F;&#x6570;&#x5B57;&#x6216;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5F62;&#x72B6;&#x53EF;&#x4EE5;&#x4E0E;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x4E00;&#x8D77;&#x5E7F;&#x64AD;&#x4E3A;&#x7684;<a href="notes/broadcasting.html#broadcasting-semantics">&#x3002;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6BD4;&#x8F83;&#x7684;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>python&#xFF1A;float</em> )&#x2013;&#x8981;&#x6BD4;&#x8F83;&#x7684;&#x5F20;&#x91CF;&#x6216;&#x503C;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x3002; &#x5FC5;&#x987B;&#x662F; &lt;cite&gt;ByteTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>&#x5728;&#x6BCF;&#x4E2A;&#x6BD4;&#x8F83;&#x4E3A;&#x771F;&#x7684;&#x4F4D;&#x7F6E;&#x5305;&#x542B;&#x4E00;&#x4E2A;&#x771F;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ 1,  0],
        [ 0,  1]], dtype=torch.uint8)
</code></pre><hr>
<pre><code>torch.equal(input, other) &#x2192; bool&#xB6;
</code></pre><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x548C;&#x5143;&#x7D20;&#xFF0C;&#x5219;&#x4E3A;<code>True</code>&#xFF0C;&#x5426;&#x5219;&#x4E3A;<code>False</code>&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
True
</code></pre><hr>
<pre><code>torch.ge(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x9010;&#x5143;&#x7D20;&#x8BA1;&#x7B97;<img src="img/5807a6d54e9791f46e3bc29daa0ee7a1.jpg" alt="">&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x5FC5;&#x987B;&#x4E3A; &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])
</code></pre><hr>
<pre><code>torch.gt(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x9010;&#x5143;&#x7D20;&#x8BA1;&#x7B97;<img src="img/0f05b585a740804201541c969a0fcf12.jpg" alt="">&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
</code></pre><hr>
<pre><code>torch.isfinite()&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#xFF0C;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x8868;&#x793A;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x5426;&#x4E3A;&lt;cite&gt;&#x6709;&#x9650;&lt;/cite&gt;&#x3002;</p>
<blockquote>
<pre><code>Arguments:
</code></pre><p>&#x5F20;&#x91CF;(&#x5F20;&#x91CF;&#xFF09;&#xFF1A;&#x8981;&#x68C0;&#x67E5;&#x7684;&#x5F20;&#x91CF;</p>
<pre><code>Returns:
</code></pre><p>&#x5F20;&#x91CF;&#xFF1A;<code>A torch.Tensor with dtype torch.bool</code>&#x5728;&#x6709;&#x9650;&#x5143;&#x7D20;&#x7684;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x5747;&#x5305;&#x542B; True&#xFF0C;&#x5426;&#x5219;&#x5305;&#x542B; False</p>
<p>Example:</p>
<pre><code>&amp;gt;&amp;gt;&amp;gt; torch.isfinite(torch.tensor([1, float(&apos;inf&apos;), 2, float(&apos;-inf&apos;), float(&apos;nan&apos;)]))
tensor([True,  False,  True,  False,  False])
</code></pre></blockquote>
<hr>
<pre><code>torch.isinf(tensor)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x8868;&#x793A;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x5426;&#x4E3A; &lt;cite&gt;+/- INF&lt;/cite&gt; &#x3002;</p>
<p>Parameters</p>
<p><strong>&#x5F20;&#x91CF;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x68C0;&#x67E5;&#x7684;&#x5F20;&#x91CF;</p>
<p>Returns</p>
<p><code>A torch.Tensor with dtype torch.bool</code>&#x5728; &lt;cite&gt;+/- INF&lt;/cite&gt; &#x5143;&#x7D20;&#x7684;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x5747;&#x5305;&#x542B; True&#xFF0C;&#x5426;&#x5219;&#x5305;&#x542B; False</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isinf(torch.tensor([1, float(&apos;inf&apos;), 2, float(&apos;-inf&apos;), float(&apos;nan&apos;)]))
tensor([False,  True,  False,  True,  False])
</code></pre><hr>
<pre><code>torch.isnan()&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E26;&#x6709;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x7684;&#x65B0;&#x5F20;&#x91CF;&#xFF0C;&#x5E03;&#x5C14;&#x5143;&#x7D20;&#x8868;&#x793A;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x5426;&#x4E3A; &lt;cite&gt;NaN&lt;/cite&gt; &#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x68C0;&#x67E5;&#x7684;&#x5F20;&#x91CF;</p>
<p>Returns</p>
<p>&#x5728; &lt;cite&gt;NaN&lt;/cite&gt; &#x5143;&#x7D20;&#x7684;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x5305;&#x542B; True &#x7684;<code>torch.BoolTensor</code>&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.isnan(torch.tensor([1, float(&apos;nan&apos;), 2]))
tensor([False, True, False])
</code></pre><hr>
<pre><code>torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -&gt; (Tensor, LongTensor)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(values, indices)</code>&#xFF0C;&#x5176;&#x4E2D;<code>values</code>&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x7B2C;<code>k</code>&#x4E2A;&#x6700;&#x5C0F;&#x5143;&#x7D20;&#x3002; <code>indices</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B;<code>dim</code>&#xFF0C;&#x5219;&#x9009;&#x62E9;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x7684;&#x6700;&#x540E;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;<code>values</code>&#x548C;<code>indices</code>&#x5F20;&#x91CF;&#x4E0E;<code>input</code>&#x7684;&#x5927;&#x5C0F;&#x76F8;&#x540C;&#xFF0C;&#x4F46;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x7684;&#x5F20;&#x91CF;&#x4E3A; 1&#x3002;&#x5426;&#x5219;&#xFF0C;<code>dim</code>&#x4F1A;&#x53D7;&#x5230;&#x6324;&#x538B; (&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;<code>values</code>&#x548C;<code>indices</code>&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x90FD;&#x6BD4;<code>input</code>&#x5F20;&#x91CF;&#x5C0F; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>k</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x7B2C; k &#x4E2A;&#x6700;&#x5C0F;&#x5143;&#x7D20;&#x7684; k</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6CBF;&#x7B2C; k &#x4E2A;&#x503C;&#x67E5;&#x627E;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>keepdim</strong> (<em>bool</em>) &#x2013; whether the output tensor has <code>dim</code> retained or not.</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;(Tensor&#xFF0C;LongTensor&#xFF09;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x53EF;&#x4EE5;&#x53EF;&#x9009;&#x5730;&#x7528;&#x4F5C;&#x8F93;&#x51FA;&#x7F13;&#x51B2;&#x533A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.kthvalue(x, 4)
torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))

&gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]])
&gt;&gt;&gt; torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
</code></pre><hr>
<pre><code>torch.le(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x9010;&#x5143;&#x7D20;&#x8BA1;&#x7B97;<img src="img/155c37e00e2691f1d70e95a9eb0156f1.jpg" alt="">&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>A <code>torch.BoolTensor</code> containing a True at each location where comparison is true</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, False], [True, True]])
</code></pre><hr>
<pre><code>torch.lt(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x9010;&#x5143;&#x7D20;&#x8BA1;&#x7B97;<img src="img/6953e508ba80b2b7609d4e21f205f593.jpg" alt="">&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5728;&#x6BCF;&#x4E2A;&#x6BD4;&#x8F83;&#x4E3A;&#x771F;&#x7684;&#x4F4D;&#x7F6E;&#x5904;&#x5305;&#x542B;&#x201C;&#x771F;&#x201D;&#x7684; &lt;cite&gt;Torch.BoolTensor&lt;/cite&gt;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
</code></pre><hr>
<pre><code>torch.max()&#xB6;
</code></pre><hr>
<pre><code>torch.max(input) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>{input}</strong> &#x2013;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6763,  0.7445, -2.2369]])
&gt;&gt;&gt; torch.max(a)
tensor(0.7445)
</code></pre><hr>
<pre><code>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(values, indices)</code>&#xFF0C;&#x5176;&#x4E2D;<code>values</code>&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002; <code>indices</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x6700;&#x5927;&#x503C;(argmax&#xFF09;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x53EA;&#x662F;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>{input}</strong> &#x2013;</p>
</li>
<li><p><strong>{dim}</strong> &#x2013;</p>
</li>
<li><p><strong>&#x9ED8;&#x8BA4;</strong> (<em>{keepdim}</em> )&#x2013; <code>False</code>&#x3002;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x7ED3;&#x679C;&#x5143;&#x7EC4;(max&#xFF0C;max_indices&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
        [ 1.1949, -1.1127, -2.2379, -0.6702],
        [ 1.5717, -0.9207,  0.1297, -1.8768],
        [-0.6172,  1.0036, -0.6060, -0.2432]])
&gt;&gt;&gt; torch.max(a, 1)
torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))
</code></pre><hr>
<pre><code>torch.max(input, other, out=None) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x5F20;&#x91CF;<code>other</code>&#x7684;&#x5BF9;&#x5E94;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;&#xFF0C;&#x5E76;&#x83B7;&#x5F97;&#x9010;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5927;&#x503C;&#x3002;</p>
<p><code>input</code>&#x548C;<code>other</code>&#x7684;&#x5F62;&#x72B6;&#x4E0D;&#x9700;&#x8981;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x5B83;&#x4EEC;&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#x3002;</p>
<p><img src="img/6b8f532893b7a5323f14fb2e70fe152e.jpg" alt=""></p>
<p>Note</p>
<p>&#x5F53;&#x5F62;&#x72B6;&#x4E0D;&#x5339;&#x914D;&#x65F6;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5F62;&#x72B6;&#x9075;&#x5FAA;<a href="notes/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;&#x89C4;&#x5219;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.2942, -0.7416,  0.2653, -0.1584])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([ 0.8722, -1.7421, -0.4141, -0.5055])
&gt;&gt;&gt; torch.max(a, b)
tensor([ 0.8722, -0.7416,  0.2653, -0.1584])
</code></pre><hr>
<pre><code>torch.min()&#xB6;
</code></pre><hr>
<pre><code>torch.min(input) &#x2192; Tensor
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x5F20;&#x91CF;&#x4E2D;&#x6240;&#x6709;&#x5143;&#x7D20;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x3002;</p>
<p>Parameters</p>
<p><strong>{input}</strong> &#x2013;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)
&gt;&gt;&gt; a
tensor([[ 0.6750,  1.0857,  1.7197]])
&gt;&gt;&gt; torch.min(a)
tensor(0.6750)
</code></pre><hr>
<pre><code>torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(values, indices)</code>&#xFF0C;&#x5176;&#x4E2D;<code>values</code>&#x662F;&#x5728;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x3002; <code>indices</code>&#x662F;&#x627E;&#x5230;&#x7684;&#x6BCF;&#x4E2A;&#x6700;&#x5C0F;&#x503C;&#x7684;&#x7D22;&#x5F15;&#x4F4D;&#x7F6E;(argmin&#xFF09;&#x3002;</p>
<p>&#x5982;&#x679C;<code>keepdim</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5927;&#x5C0F;&#x4E0E;<code>input</code>&#x76F8;&#x540C;&#xFF0C;&#x53EA;&#x662F;&#x5C3A;&#x5BF8;&#x4E3A; 1 &#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<code>dim</code>&#x3002;&#x5426;&#x5219;&#xFF0C;&#x5C06;&#x538B;&#x7F29;<code>dim</code>(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>)&#xFF0C;&#x5BFC;&#x81F4;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x6BD4;<code>input</code>&#x5C0F; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>{input}</strong> &#x2013;</p>
</li>
<li><p><strong>{dim}</strong> &#x2013;</p>
</li>
<li><p><strong>{keepdim}</strong> &#x2013;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x4E24;&#x4E2A;&#x8F93;&#x51FA;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;(min&#xFF0C;min_indices&#xFF09;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a
tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
        [-1.4644, -0.2635, -0.3651,  0.6134],
        [ 0.2457,  0.0384,  1.0128,  0.7015],
        [-0.1153,  2.9849,  2.1458,  0.5788]])
&gt;&gt;&gt; torch.min(a, 1)
torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))
</code></pre><hr>
<pre><code>torch.min(input, other, out=None) &#x2192; Tensor
</code></pre><p>&#x5C06;&#x5F20;&#x91CF;<code>input</code>&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4E0E;&#x5F20;&#x91CF;<code>other</code>&#x7684;&#x5BF9;&#x5E94;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;&#xFF0C;&#x5E76;&#x6309;&#x5143;&#x7D20;&#x53D6;&#x6700;&#x5C0F;&#x503C;&#x3002; &#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x5F20;&#x91CF;&#x3002;</p>
<p>The shapes of <code>input</code> and <code>other</code> don&#x2019;t need to match, but they must be <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p><img src="img/f033730f350678f87f2f08ee833a82fe.jpg" alt=""></p>
<p>Note</p>
<p>When the shapes do not match, the shape of the returned output tensor follows the <a href="notes/broadcasting.html#broadcasting-semantics">broadcasting rules</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([ 0.8137, -1.1740, -0.6460,  0.6308])
&gt;&gt;&gt; b = torch.randn(4)
&gt;&gt;&gt; b
tensor([-0.1369,  0.1555,  0.4019, -0.1929])
&gt;&gt;&gt; torch.min(a, b)
tensor([-0.1369, -1.1740, -0.6460, -0.1929])
</code></pre><hr>
<pre><code>torch.ne(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x9010;&#x5143;&#x7D20;&#x8BA1;&#x7B97;<img src="img/efbfcdce474b4ae10bb5dacb46481266.jpg" alt="">&#x3002;</p>
<p>The second argument can be a number or a tensor whose shape is <a href="notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the first argument.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to compare</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <em>python:float</em>) &#x2013; the tensor or value to compare</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor that must be a &lt;cite&gt;BoolTensor&lt;/cite&gt;</p>
</li>
</ul>
<p>Returns</p>
<p><code>torch.BoolTensor</code>&#x5728;&#x6BD4;&#x8F83;&#x4E3A;&#x771F;&#x7684;&#x6BCF;&#x4E2A;&#x4F4D;&#x7F6E;&#x90FD;&#x5305;&#x542B;&#x201C;&#x771F;&#x201D;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [True, False]])
</code></pre><hr>
<pre><code>torch.sort(input, dim=-1, descending=False, out=None) -&gt; (Tensor, LongTensor)&#xB6;
</code></pre><p>&#x6CBF;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x6309;&#x503C;&#x5347;&#x5E8F;&#x5BF9;<code>input</code>&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#x3002;</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>&#x5982;&#x679C;<code>descending</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x5143;&#x7D20;&#x5C06;&#x6309;&#x503C;&#x964D;&#x5E8F;&#x6392;&#x5E8F;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;(&#x503C;&#xFF0C;&#x7D22;&#x5F15;&#xFF09;&#x7684;&#x547D;&#x540D;&#x5143;&#x7EC4;&#xFF0C;&#x5176;&#x4E2D;&lt;cite&gt;&#x503C;&lt;/cite&gt;&#x662F;&#x6392;&#x5E8F;&#x7684;&#x503C;&#xFF0C;&lt;cite&gt;&#x7D22;&#x5F15;&lt;/cite&gt;&#x662F;&#x539F;&#x59CB;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x5F20;&#x91CF;&#x4E2D;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; the dimension to sort along</p>
</li>
<li><p><strong>descending</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; controls the sorting order (ascending or descending)</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;(&lt;cite&gt;&#x5F20;&#x91CF;&lt;/cite&gt;&#xFF0C; &lt;cite&gt;LongTensor&lt;/cite&gt; )&#xFF0C;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x5C06;&#x5176;&#x7528;&#x4F5C;&#x8F93;&#x51FA;&#x7F13;&#x51B2;&#x533A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)
&gt;&gt;&gt; sorted, indices = torch.sort(x)
&gt;&gt;&gt; sorted
tensor([[-0.2162,  0.0608,  0.6719,  2.3332],
        [-0.5793,  0.0061,  0.6058,  0.9497],
        [-0.5071,  0.3343,  0.9553,  1.0960]])
&gt;&gt;&gt; indices
tensor([[ 1,  0,  2,  3],
        [ 3,  1,  0,  2],
        [ 0,  3,  1,  2]])

&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)
&gt;&gt;&gt; sorted
tensor([[-0.5071, -0.2162,  0.6719, -0.5793],
        [ 0.0608,  0.0061,  0.9497,  0.3343],
        [ 0.6058,  0.9553,  1.0960,  2.3332]])
&gt;&gt;&gt; indices
tensor([[ 2,  0,  0,  1],
        [ 0,  1,  1,  2],
        [ 1,  2,  2,  0]])
</code></pre><hr>
<pre><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x6CBF;&#x7ED9;&#x5B9A;&#x7EF4;&#x5EA6;&#x7684;&#x7ED9;&#x5B9A;<code>input</code>&#x5F20;&#x91CF;&#x7684;<code>k</code>&#x6700;&#x5927;&#x5143;&#x7D20;&#x3002;</p>
<p>If <code>dim</code> is not given, the last dimension of the &lt;cite&gt;input&lt;/cite&gt; is chosen.</p>
<p>&#x5982;&#x679C;<code>largest</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE; &lt;cite&gt;k&lt;/cite&gt; &#x4E2A;&#x6700;&#x5C0F;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&lt;cite&gt;(&#x503C;&#xFF0C;&#x7D22;&#x5F15;&#xFF09;&lt;/cite&gt;&#x7684;&#x547D;&#x540D;&#x5143;&#x7EC4;&#xFF0C;&#x5176;&#x4E2D;&lt;cite&gt;&#x7D22;&#x5F15;&lt;/cite&gt;&#x662F;&#x539F;&#x59CB;&lt;cite&gt;&#x8F93;&#x5165;&lt;/cite&gt;&#x5F20;&#x91CF;&#x4E2D;&#x5143;&#x7D20;&#x7684;&#x7D22;&#x5F15;&#x3002;</p>
<p>&#x5E03;&#x5C14;&#x9009;&#x9879;<code>sorted</code>&#x5982;&#x679C;&#x4E3A;<code>True</code>&#xFF0C;&#x5C06;&#x786E;&#x4FDD;&#x8FD4;&#x56DE;&#x7684; &lt;cite&gt;k&lt;/cite&gt; &#x5143;&#x7D20;&#x672C;&#x8EAB;&#x5DF2;&#x6392;&#x5E8F;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>k</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x201C; top-k&#x201D;&#x4E2D;&#x7684; k</p>
</li>
<li><p><strong>dim</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; the dimension to sort along</p>
</li>
<li><p><strong>&#x6700;&#x5927;&#x7684;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x8FD4;&#x56DE;&#x6700;&#x5927;&#x8FD8;&#x662F;&#x6700;&#x5C0F;&#x5143;&#x7D20;</p>
</li>
<li><p><strong>&#x6392;&#x5E8F;&#x7684;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x6309;&#x6392;&#x5E8F;&#x987A;&#x5E8F;&#x8FD4;&#x56DE;&#x5143;&#x7D20;</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x63D0;&#x4F9B;(Tensor&#xFF0C;LongTensor&#xFF09;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4; &#x7F13;&#x51B2;&#x533A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
</code></pre><h3 id="&#x5149;&#x8C31;&#x64CD;&#x4F5C;">&#x5149;&#x8C31;&#x64CD;&#x4F5C;</h3>
<hr>
<pre><code>torch.fft(input, signal_ndim, normalized=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x590D;&#x6570;&#x5230;&#x590D;&#x6570;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6570;&#x5230;&#x590D;&#x6570;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;&#x3002; &#x5FFD;&#x7565;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x5B83;&#x5C06;&#x8BA1;&#x7B97;&#x4EE5;&#x4E0B;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p><img src="img/f1d658cd4fac30eca4c9293d39d0d2fe.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;<img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt=""> = <code>signal_ndim</code>&#x662F;&#x4FE1;&#x53F7;&#x5C3A;&#x5BF8;&#x7684;&#x6570;&#x91CF;&#xFF0C;<img src="img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" alt="">&#x662F;&#x4FE1;&#x53F7;&#x5C3A;&#x5BF8;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x652F;&#x6301;<code>signal_ndim</code>&#x6307;&#x793A;&#x7684; 1D&#xFF0C;2D &#x548C; 3D &#x590D;&#x6570;&#x5230;&#x590D;&#x6570;&#x8F6C;&#x6362;&#x3002; <code>input</code>&#x5FC5;&#x987B;&#x662F;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x4E3A; 2&#xFF0C;&#x4EE3;&#x8868;&#x590D;&#x6570;&#x7684;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#xFF0C;&#x5E76;&#x4E14;&#x81F3;&#x5C11;&#x5E94;&#x5177;&#x6709;<code>signal_ndim + 1</code>&#x4E2A;&#x5C3A;&#x5BF8;&#xFF0C;&#x5E76;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x524D;&#x6279;&#x5C3A;&#x5BF8;&#x3002; &#x5982;&#x679C;<code>normalized</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x901A;&#x8FC7;&#x5C06;&#x7ED3;&#x679C;&#x9664;&#x4EE5;<img src="img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" alt="">&#x6765;&#x5BF9;&#x7ED3;&#x679C;&#x8FDB;&#x884C;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x4EE5;&#x4F7F;&#x8FD0;&#x7B97;&#x7B26;&#x4E3A;&#x4E00;&#x5143;&#x3002;</p>
<p>&#x5C06;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x4E00;&#x8D77;&#x8FD4;&#x56DE;&#x4E3A;<code>input</code>&#x5F62;&#x72B6;&#x76F8;&#x540C;&#x7684;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x53CD;&#x51FD;&#x6570;&#x4E3A; <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> &#x3002;</p>
<p>Note</p>
<p>&#x5BF9;&#x4E8E; CUDA &#x5F20;&#x91CF;&#xFF0C;LRU &#x7F13;&#x5B58;&#x7528;&#x4E8E; cuFFT &#x8BA1;&#x5212;&#xFF0C;&#x4EE5;&#x52A0;&#x5FEB;&#x5728;&#x5177;&#x6709;&#x76F8;&#x540C;&#x914D;&#x7F6E;&#x7684;&#x76F8;&#x540C;&#x51E0;&#x4F55;&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#x4E0A;&#x91CD;&#x590D;&#x8FD0;&#x884C; FFT &#x65B9;&#x6CD5;&#x7684;&#x901F;&#x5EA6;&#x3002; &#x6709;&#x5173;&#x5982;&#x4F55;&#x76D1;&#x89C6;&#x548C;&#x63A7;&#x5236;&#x7F13;&#x5B58;&#x7684;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="notes/cuda.html#cufft-plan-cache">cuFFT &#x8BA1;&#x5212;&#x7F13;&#x5B58;</a>&#x3002;</p>
<p>Warning</p>
<p>&#x5BF9;&#x4E8E; CPU &#x5F20;&#x91CF;&#xFF0C;&#x6B64;&#x65B9;&#x6CD5;&#x5F53;&#x524D;&#x4EC5;&#x9002;&#x7528;&#x4E8E; MKL&#x3002; &#x4F7F;&#x7528;<code>torch.backends.mkl.is_available()</code>&#x68C0;&#x67E5;&#x662F;&#x5426;&#x5B89;&#x88C5;&#x4E86; MKL&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x81F3;&#x5C11;<code>signal_ndim</code> <code>+ 1</code>&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>signal_ndim</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x6BCF;&#x4E2A;&#x4FE1;&#x53F7;&#x4E2D;&#x7684;&#x7EF4;&#x6570;&#x3002; <code>signal_ndim</code>&#x53EA;&#x80FD;&#x662F; 1&#x3001;2 &#x6216; 3</p>
</li>
<li><p><strong>&#x6807;&#x51C6;&#x5316;&#x7684;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x6807;&#x51C6;&#x5316;&#x7ED3;&#x679C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x590D;&#x6570;&#x5230;&#x590D;&#x6570;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x7684;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # unbatched 2D FFT
&gt;&gt;&gt; x = torch.randn(4, 3, 2)
&gt;&gt;&gt; torch.fft(x, 2)
tensor([[[-0.0876,  1.7835],
         [-2.0399, -2.9754],
         [ 4.4773, -5.0119]],

        [[-1.5716,  2.7631],
         [-3.8846,  5.2652],
         [ 0.2046, -0.7088]],

        [[ 1.9938, -0.5901],
         [ 6.5637,  6.4556],
         [ 2.9865,  4.9318]],

        [[ 7.0193,  1.1742],
         [-1.3717, -2.1084],
         [ 2.0289,  2.9357]]])
&gt;&gt;&gt; # batched 1D FFT
&gt;&gt;&gt; torch.fft(x, 1)
tensor([[[ 1.8385,  1.2827],
         [-0.1831,  1.6593],
         [ 2.4243,  0.5367]],

        [[-0.9176, -1.5543],
         [-3.9943, -2.9860],
         [ 1.2838, -2.9420]],

        [[-0.8854, -0.6860],
         [ 2.4450,  0.0808],
         [ 1.3076, -0.5768]],

        [[-0.1231,  2.7411],
         [-0.3075, -1.7295],
         [-0.5384, -2.0299]]])
&gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT
&gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2)
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; y.shape
torch.Size([3, 3, 5, 5, 2])
</code></pre><hr>
<pre><code>torch.ifft(input, signal_ndim, normalized=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x590D;&#x6570;&#x5230;&#x9006;&#x79BB;&#x6563;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6742;&#x5230;&#x590D;&#x6742;&#x7684;&#x9006;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;&#x3002; &#x5FFD;&#x7565;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF0C;&#x5B83;&#x5C06;&#x8BA1;&#x7B97;&#x4EE5;&#x4E0B;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p><img src="img/5040603503ecec3ee3de71bfe027769c.jpg" alt=""></p>
<p>where <img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt=""> = <code>signal_ndim</code> is number of dimensions for the signal, and <img src="img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" alt=""> is the size of signal dimension <img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">.</p>
<p>&#x53C2;&#x6570;&#x89C4;&#x8303;&#x51E0;&#x4E4E;&#x4E0E; <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> &#x76F8;&#x540C;&#x3002; &#x4F46;&#x662F;&#xFF0C;&#x5982;&#x679C;&#x5C06;<code>normalized</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5B83;&#x5C06;&#x8FD4;&#x56DE;&#x7ED3;&#x679C;&#x4E58;&#x4EE5;<img src="img/77177756d6e74e1e1a97289dfeea062c.jpg" alt="">&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4ECE;&#x800C;&#x6210;&#x4E3A;&#x4E00;&#x5143;&#x8FD0;&#x7B97;&#x7B26;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x8981;&#x53CD;&#x8F6C; <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> &#xFF0C;&#x5E94;&#x4E3A; <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> &#x8BBE;&#x7F6E;<code>normalized</code>&#x81EA;&#x53D8;&#x91CF;&#x3002;</p>
<p>Returns the real and the imaginary parts together as one tensor of the same shape of <code>input</code>.</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x53CD;&#x51FD;&#x6570;&#x4E3A; <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> &#x3002;</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li><p><strong>signal_ndim</strong> (<em>python:int</em>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x590D;&#x6570;&#x5230;&#x590D;&#x6570;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x7684;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(3, 3, 2)
&gt;&gt;&gt; x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])
&gt;&gt;&gt; y = torch.fft(x, 2)
&gt;&gt;&gt; torch.ifft(y, 2)  # recover x
tensor([[[ 1.2766,  1.3680],
         [-0.8337,  2.0251],
         [ 0.9465, -1.4390]],

        [[-0.1890,  1.6010],
         [ 1.1034, -1.9230],
         [-0.9482,  1.0775]],

        [[-0.7708, -0.8176],
         [-0.1843, -0.2287],
         [-1.9034, -0.2196]]])
</code></pre><hr>
<pre><code>torch.rfft(input, signal_ndim, normalized=False, onesided=True) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6742;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6742;&#x7684;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;&#x3002; &#x5B83;&#x5728;&#x6570;&#x5B66;&#x4E0A;&#x7B49;&#x6548;&#x4E8E; <a href="#torch.fft" title="torch.fft"><code>fft()</code></a> &#xFF0C;&#x53EA;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x683C;&#x5F0F;&#x4E0D;&#x540C;&#x3002;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x652F;&#x6301; 1D&#xFF0C;2D &#x548C; 3D &#x5B9E;&#x6570;&#x5230;&#x590D;&#x6742;&#x7684;&#x53D8;&#x6362;&#xFF0C;&#x7531;<code>signal_ndim</code>&#x6307;&#x793A;&#x3002; <code>input</code>&#x5FC5;&#x987B;&#x662F;&#x81F3;&#x5C11;&#x5177;&#x6709;<code>signal_ndim</code>&#x5C3A;&#x5BF8;&#x4E14;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x524D;&#x5BFC;&#x6279;&#x5C3A;&#x5BF8;&#x7684;&#x5F20;&#x91CF;&#x3002; &#x5982;&#x679C;<code>normalized</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x901A;&#x8FC7;&#x5C06;&#x7ED3;&#x679C;&#x9664;&#x4EE5;<img src="img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" alt="">&#x6765;&#x6807;&#x51C6;&#x5316;&#x7ED3;&#x679C;&#xFF0C;&#x4EE5;&#x4F7F;&#x8FD0;&#x7B97;&#x7B26;&#x4E3A; the&#xFF0C;&#x5176;&#x4E2D;<img src="img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" alt="">&#x662F;&#x4FE1;&#x53F7;&#x7EF4;&#x5EA6;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6742;&#x7684;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x9075;&#x5FAA;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#xFF1A;</p>
<p><img src="img/eacabf6004703215e86dc3522380c6ed.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;&#x7D22;&#x5F15;&#x7B97;&#x672F;&#x662F;&#x8BA1;&#x7B97;&#x6A21;&#x91CF;&#x7684;&#x76F8;&#x5E94;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#xFF0C;<img src="img/f1deebf6650afaf555a88b62fc3992f0.jpg" alt="">&#x662F;&#x5171;&#x8F6D;&#x7B97;&#x5B50;&#xFF0C;<img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt=""> = <code>signal_ndim</code>&#x3002; <code>onesided</code>&#x6807;&#x5FD7;&#x63A7;&#x5236;&#x662F;&#x5426;&#x907F;&#x514D;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x4E2D;&#x7684;&#x5197;&#x4F59;&#x3002; &#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x8F93;&#x51FA;&#x5C06;&#x4E0D;&#x662F;&#x5F62;&#x72B6;<img src="img/f6dde7278b3a3d1ca525c45b7ad2b155.jpg" alt="">&#x7684;&#x5B8C;&#x5168;&#x590D;&#x6742;&#x7ED3;&#x679C;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x662F;<code>input</code>&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4F46;&#x662F;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x5C06;&#x662F;<img src="img/ab88558f86cd940a2c036983b4a17bc0.jpg" alt="">&#x5C3A;&#x5BF8;&#x7684;&#x4E00;&#x534A; &#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x53CD;&#x51FD;&#x6570;&#x4E3A; <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> &#x3002;</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x81F3;&#x5C11;<code>signal_ndim</code>&#x5C3A;&#x5BF8;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>signal_ndim</strong> (<em>python:int</em>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li><p><strong>&#x5355;&#x9762;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E00;&#x534A;&#x7ED3;&#x679C;&#x4EE5;&#x907F;&#x514D;&#x91CD;&#x590D;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6570;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x7684;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5, 5)
&gt;&gt;&gt; torch.rfft(x, 2).shape
torch.Size([5, 3, 2])
&gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape
torch.Size([5, 5, 2])
</code></pre><hr>
<pre><code>torch.irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x590D;&#x6570;&#x5230;&#x5B9E;&#x6570;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x9006;&#x53D8;&#x6362;</p>
<p>&#x6B64;&#x65B9;&#x6CD5;&#x8BA1;&#x7B97;&#x590D;&#x6570;&#x5230;&#x5B9E;&#x6570;&#x7684;&#x79BB;&#x6563;&#x5085;&#x91CC;&#x53F6;&#x9006;&#x53D8;&#x6362;&#x3002; &#x5B83;&#x5728;&#x6570;&#x5B66;&#x4E0A;&#x7B49;&#x6548;&#x4E8E; <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> &#xFF0C;&#x53EA;&#x662F;&#x8F93;&#x5165;&#x548C;&#x8F93;&#x51FA;&#x683C;&#x5F0F;&#x4E0D;&#x540C;&#x3002;</p>
<p>&#x53C2;&#x6570;&#x89C4;&#x8303;&#x51E0;&#x4E4E;&#x4E0E; <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> &#x76F8;&#x540C;&#x3002; &#x4E0E; <a href="#torch.ifft" title="torch.ifft"><code>ifft()</code></a> &#x76F8;&#x4F3C;&#xFF0C;&#x5982;&#x679C;<code>normalized</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x901A;&#x8FC7;&#x5C06;&#x7ED3;&#x679C;&#x4E0E;<img src="img/ed8d1a6498daf3d5b9174f9c1535b2bd.jpg" alt="">&#x76F8;&#x4E58;&#x6765;&#x5BF9;&#x7ED3;&#x679C;&#x8FDB;&#x884C;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x4EE5;&#x4F7F;&#x8FD0;&#x7B97;&#x7B26;&#x4E3A; ary&#xFF0C;&#x5176;&#x4E2D;<img src="img/8d61ccc06c1a60ca6e3c19f12aee4f33.jpg" alt="">&#x662F;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F; &#x5C3A;&#x5BF8;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">&#x3002;</p>
<p>Note</p>
<p>&#x7531;&#x4E8E;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#xFF0C;<code>input</code>&#x4E0D;&#x9700;&#x8981;&#x5305;&#x542B;&#x5B8C;&#x6574;&#x7684;&#x590D;&#x6570;&#x9891;&#x7387;&#x503C;&#x3002; &#x5927;&#x7EA6;&#x4E00;&#x534A;&#x7684;&#x503C;&#x5C31;&#x8DB3;&#x591F;&#x4E86;&#xFF0C;&#x5C31;&#x50CF; <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> &#x548C;<code>rfft(signal, onesided=True)</code>&#x7ED9;&#x51FA;<code>input</code>&#x7684;&#x60C5;&#x51B5;&#x4E00;&#x6837;&#x3002; &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x8BF7;&#x5C06;&#x6B64;&#x65B9;&#x6CD5;&#x7684;<code>onesided</code>&#x53C2;&#x6570;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#x3002; &#x6B64;&#x5916;&#xFF0C;&#x539F;&#x59CB;&#x4FE1;&#x53F7;&#x5F62;&#x72B6;&#x4FE1;&#x606F;&#x6709;&#x65F6;&#x4F1A;&#x4E22;&#x5931;&#xFF0C;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x5C06;<code>signal_sizes</code>&#x8BBE;&#x7F6E;&#x4E3A;&#x539F;&#x59CB;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;(&#x5982;&#x679C;&#x5904;&#x4E8E;&#x6279;&#x5904;&#x7406;&#x6A21;&#x5F0F;&#xFF0C;&#x5219;&#x6CA1;&#x6709;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#xFF09;&#xFF0C;&#x4EE5;&#x6062;&#x590D;&#x6B63;&#x786E;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x8981;&#x53CD;&#x8F6C; <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> &#xFF0C;&#x5E94;&#x4E3A; <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> &#x8BBE;&#x7F6E;<code>normalized</code>&#x548C;<code>onesided</code>&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x5E76;&#x4E14;&#x6700;&#x597D;&#x4F7F;&#x7528;<code>signal_sizes</code>&#x4EE5;&#x907F;&#x514D;&#x5927;&#x5C0F; &#x4E0D;&#x5339;&#x914D;&#x3002; &#x6709;&#x5173;&#x5927;&#x5C0F;&#x4E0D;&#x5339;&#x914D;&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x4E0B;&#x9762;&#x7684;&#x793A;&#x4F8B;&#x3002;</p>
<p>&#x6709;&#x5173;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#x7684;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> &#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x7684;&#x53CD;&#x51FD;&#x6570;&#x4E3A; <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> &#x3002;</p>
<p>Warning</p>
<p>&#x901A;&#x5E38;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x5165;&#x5E94;&#x5305;&#x542B;&#x9075;&#x5FAA;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#x7684;&#x503C;&#x3002; &#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x5373;&#x4F7F;<code>onesided</code>&#x4E3A;<code>True</code>&#xFF0C;&#x4ECD;&#x7136;&#x7ECF;&#x5E38;&#x9700;&#x8981;&#x5728;&#x67D0;&#x4E9B;&#x90E8;&#x5206;&#x4E0A;&#x4FDD;&#x6301;&#x5BF9;&#x79F0;&#x3002; &#x5F53;&#x4E0D;&#x6EE1;&#x8DB3;&#x6B64;&#x8981;&#x6C42;&#x65F6;&#xFF0C; <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> &#x7684;&#x884C;&#x4E3A;&#x662F;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x3002; &#x7531;&#x4E8E; <a href="autograd.html#torch.autograd.gradcheck" title="torch.autograd.gradcheck"><code>torch.autograd.gradcheck()</code></a> &#x4F30;&#x8BA1;&#x5E26;&#x6709;&#x70B9;&#x6270;&#x52A8;&#x7684;&#x6570;&#x5B57;&#x96C5;&#x53EF;&#x6BD4;&#x884C;&#x5217;&#x5F0F;&#xFF0C;&#x56E0;&#x6B64; <a href="#torch.irfft" title="torch.irfft"><code>irfft()</code></a> &#x51E0;&#x4E4E;&#x80AF;&#x5B9A;&#x4F1A;&#x5931;&#x8D25;&#x3002;</p>
<p>Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See <a href="notes/cuda.html#cufft-plan-cache">cuFFT plan cache</a> for more details on how to monitor and control the cache.</p>
<p>Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor of at least <code>signal_ndim</code> <code>+ 1</code> dimensions</p>
</li>
<li><p><strong>signal_ndim</strong> (<em>python:int</em>) &#x2013; the number of dimensions in each signal. <code>signal_ndim</code> can only be 1, 2 or 3</p>
</li>
<li><p><strong>normalized</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; controls whether to return normalized results. Default: <code>False</code></p>
</li>
<li><p><strong>&#x5355;&#x9762;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;<code>input</code>&#x662F;&#x5426;&#x5BF9;&#x534A;&#x4EE5;&#x907F;&#x514D;&#x5197;&#x4F59;&#xFF0C;&#x4F8B;&#x5982;&#x901A;&#x8FC7; <a href="#torch.rfft" title="torch.rfft"><code>rfft()</code></a> &#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code></p>
</li>
<li><p><strong>signal_sizes</strong> (&#x5217;&#x8868;&#x6216;<code>torch.Size</code>&#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x539F;&#x59CB;&#x4FE1;&#x53F7;&#x7684;&#x5927;&#x5C0F;(&#x65E0;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#xFF09;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x590D;&#x6570;&#x5230;&#x5B9E;&#x6570;&#x5085;&#x7ACB;&#x53F6;&#x9006;&#x53D8;&#x6362;&#x7ED3;&#x679C;&#x7684;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size
&gt;&gt;&gt; x = torch.randn(4, 5)

&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape
torch.Size([4, 3, 2])
&gt;&gt;&gt;
&gt;&gt;&gt; # now we use the original shape to recover x
&gt;&gt;&gt; x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
&gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True)
&gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x
tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],
        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],
        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],
        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])
</code></pre><hr>
<pre><code>torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode=&apos;reflect&apos;, normalized=False, onesided=True)&#xB6;
</code></pre><p>&#x77ED;&#x65F6;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;(STFT&#xFF09;&#x3002;</p>
<p>&#x5FFD;&#x7565;&#x53EF;&#x9009;&#x7684;&#x6279;&#x5904;&#x7406;&#x7EF4;&#xFF0C;&#x6B64;&#x65B9;&#x6CD5;&#x5C06;&#x8BA1;&#x7B97;&#x4EE5;&#x4E0B;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p><img src="img/33a339c5713e9db708c9eed0a7969d63.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;<img src="img/03327e7b697db30918e96b2209927929.jpg" alt="">&#x662F;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x7D22;&#x5F15;&#xFF0C;<img src="img/47bff0d0b1fe99458172eae026405291.jpg" alt="">&#x662F;<img src="img/c913af4e6c1176b6e9846f5f85011461.jpg" alt="">&#x7684;&#x9891;&#x7387;&#x3002; &#x5F53;<code>onesided</code>&#x4E3A;&#x9ED8;&#x8BA4;&#x503C;<code>True</code>&#x65F6;&#xFF0C;</p>
<ul>
<li><p><code>input</code>&#x5FC5;&#x987B;&#x662F;&#x4E00;&#x7EF4;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x6216;&#x4E8C;&#x7EF4;&#x65F6;&#x95F4;&#x5E8F;&#x5217;&#x6279;&#x6B21;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>hop_length</code>&#x4E3A;<code>None</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x5219;&#x5C06;&#x5176;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>floor(n_fft / 4)</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>win_length</code>&#x4E3A;<code>None</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x5219;&#x5C06;&#x5176;&#x89C6;&#x4E3A;&#x7B49;&#x4E8E;<code>n_fft</code>&#x3002;</p>
</li>
<li><p><code>window</code>&#x53EF;&#x4EE5;&#x662F;&#x5927;&#x5C0F;&#x4E3A;<code>win_length</code>&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x4F8B;&#x5982;&#x6765;&#x81EA; <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> &#x3002; &#x5982;&#x679C;<code>window</code>&#x4E3A;<code>None</code>(&#x9ED8;&#x8BA4;&#xFF09;&#xFF0C;&#x5219;&#x5C06;&#x5176;&#x89C6;&#x4E3A;&#x7A97;&#x53E3;&#x4E2D;&#x5230;&#x5904;&#x90FD;&#x6709;<img src="img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" alt="">&#x3002; &#x5982;&#x679C;&#x4F7F;&#x7528;<img src="img/3579be09fb723df705e009678d545d64.jpg" alt="">&#xFF0C;&#x5219;&#x5C06;<code>window</code>&#x7684;&#x4E24;&#x9762;&#x586B;&#x5145;&#x957F;&#x5EA6;<code>n_fft</code>&#xFF0C;&#x7136;&#x540E;&#x518D;&#x8FDB;&#x884C;&#x5E94;&#x7528;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>center</code>&#x4E3A;<code>True</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x5219;&#x5C06;&#x5728;&#x4E24;&#x8FB9;&#x90FD;&#x586B;&#x5145;<code>input</code>&#xFF0C;&#x4EE5;&#x4F7F;&#x7B2C;<img src="img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" alt="">&#x5E27;&#x4F4D;&#x4E8E;&#x65F6;&#x95F4;<img src="img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" alt="">&#x7684;&#x4E2D;&#x5FC3;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x7B2C;<img src="img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" alt="">&#x5E27;&#x5728;&#x65F6;&#x95F4;<img src="img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" alt="">&#x5F00;&#x59CB;&#x3002;</p>
</li>
<li><p><code>pad_mode</code>&#x786E;&#x5B9A;&#x5F53;<code>center</code>&#x4E3A;<code>True</code>&#x65F6;&#x5728;<code>input</code>&#x4E0A;&#x4F7F;&#x7528;&#x7684;&#x586B;&#x5145;&#x65B9;&#x6CD5;&#x3002; &#x6709;&#x5173;&#x6240;&#x6709;&#x53EF;&#x7528;&#x9009;&#x9879;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="nn.functional.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>torch.nn.functional.pad()</code></a> &#x3002; &#x9ED8;&#x8BA4;&#x503C;&#x4E3A;<code>&quot;reflect&quot;</code>&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>onesided</code>&#x4E3A;<code>True</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x5219;&#x4EC5;&#x8FD4;&#x56DE;<img src="img/c15755e95a374ed10ce009af8ac323f5.jpg" alt="">&#x4E2D;<img src="img/47bff0d0b1fe99458172eae026405291.jpg" alt="">&#x7684;&#x503C;&#xFF0C;&#x56E0;&#x4E3A;&#x5B9E;&#x6570;&#x5230;&#x590D;&#x6570;&#x5085;&#x91CC;&#x53F6;&#x53D8;&#x6362;&#x6EE1;&#x8DB3;&#x5171;&#x8F6D;&#x5BF9;&#x79F0;&#x6027;&#xFF0C;&#x5373;<img src="img/da5553744a960ad6822ef967ba0ea46f.jpg" alt="">&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>normalized</code>&#x4E3A;<code>True</code>(&#x9ED8;&#x8BA4;&#x4E3A;<code>False</code>&#xFF09;&#xFF0C;&#x5219;&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5F52;&#x4E00;&#x5316;&#x7684; STFT &#x7ED3;&#x679C;&#xFF0C;&#x5373;&#x4E58;&#x4EE5;<img src="img/3a83bac3c29283a2eb3fcc60e15f37aa.jpg" alt="">&#x3002;</p>
</li>
</ul>
<p>&#x5C06;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x4E00;&#x8D77;&#x8FD4;&#x56DE;&#x4E3A;&#x4E00;&#x4E2A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/1d33f2bcea40c743acb3c7e78a69ca62.jpg" alt="">&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x662F;&#x53EF;&#x9009;&#x7684;<code>input</code>&#x6279;&#x5927;&#x5C0F;&#xFF0C;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x662F;&#x5E94;&#x7528; STFT &#x7684;&#x9891;&#x7387;&#x6570;&#xFF0C;<img src="img/e76349865309323eafcc6a541e3073ae.jpg" alt="">&#x662F;&#x603B;&#x6570; &#x6240;&#x4F7F;&#x7528;&#x7684;&#x5E27;&#x6570;&#xFF0C;&#x6700;&#x540E;&#x4E00;&#x7EF4;&#x4E2D;&#x7684;&#x6BCF;&#x4E00;&#x5BF9;&#x4EE3;&#x8868;&#x4E00;&#x4E2A;&#x590D;&#x6570;&#xFF0C;&#x4F5C;&#x4E3A;&#x5B9E;&#x90E8;&#x548C;&#x865A;&#x90E8;&#x3002;</p>
<p>Warning</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x5728;&#x7248;&#x672C; 0.4.1 &#x66F4;&#x6539;&#x4E86;&#x7B7E;&#x540D;&#x3002; &#x4F7F;&#x7528;&#x524D;&#x4E00;&#x4E2A;&#x7B7E;&#x540D;&#x8FDB;&#x884C;&#x8C03;&#x7528;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x6216;&#x8FD4;&#x56DE;&#x9519;&#x8BEF;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor</p>
</li>
<li><p><strong>n_fft</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x5085;&#x7ACB;&#x53F6;&#x53D8;&#x6362;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>hop_length</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x76F8;&#x90BB;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x6846;&#x67B6;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code>(&#x7B49;&#x540C;&#x4E8E;<code>floor(n_fft / 4)</code>&#xFF09;</p>
</li>
<li><p><strong>win_length</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x7A97;&#x53E3;&#x6846;&#x67B6;&#x548C; STFT &#x8FC7;&#x6EE4;&#x5668;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code>(&#x7B49;&#x540C;&#x4E8E;<code>n_fft</code>&#xFF09;</p>
</li>
<li><p><strong>&#x7A97;&#x53E3;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x9009;&#x7A97;&#x53E3;&#x529F;&#x80FD;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code>(&#x88AB;&#x89C6;&#x4E3A;&#x6240;&#x6709;<img src="img/21c5bc8d40ee5ab2e18d64aaba8359c7.jpg" alt="">&#x7684;&#x7A97;&#x53E3;&#xFF09;</p>
</li>
<li><p><strong>&#x4E2D;&#x5FC3;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x5426;&#x5728;&#x4E24;&#x4FA7;&#x90FD;&#x586B;&#x5145;<code>input</code>&#x4EE5;&#x4FBF;&#x7B2C;<img src="img/ea198b23d3d3f891bcc7684b8659b3e4.jpg" alt="">&#x4E2A;&#x5E27;&#x4F4D;&#x4E8E; &#x96C6;&#x4E2D;&#x5728;&#x65F6;&#x95F4;<img src="img/33e0acbd6c2b37b3ef3123f4dc18a1c0.jpg" alt="">&#x4E0A;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code></p>
</li>
<li><p><strong>pad_mode</strong> (<em>&#x5B57;&#x7B26;&#x4E32;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x5F53;<code>center</code>&#x4E3A;<code>True</code>&#x65F6;&#x4F7F;&#x7528;&#x7684;&#x586B;&#x5145;&#x65B9;&#x6CD5;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>&quot;reflect&quot;</code></p>
</li>
<li><p><strong>&#x89C4;&#x8303;&#x5316;&#x7684;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x89C4;&#x8303;&#x5316;&#x7684; STFT &#x7ED3;&#x679C;&#x9ED8;&#x8BA4;&#xFF1A;<code>False</code></p>
</li>
<li><p><strong>&#x5355;&#x9762;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E00;&#x534A;&#x7ED3;&#x679C;&#x4EE5;&#x907F;&#x514D;&#x5197;&#x4F59;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B; STFT &#x7ED3;&#x679C;&#x7684;&#x5F20;&#x91CF;&#x5177;&#x6709;&#x4E0A;&#x8FF0;&#x5F62;&#x72B6;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr>
<pre><code>torch.bartlett_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>Bartlett &#x7A97;&#x53E3;&#x529F;&#x80FD;&#x3002;</p>
<p><img src="img/7a8f3ed9f6703292e22f8787927206ae.jpg" alt=""></p>
<p>&#x5176;&#x4E2D;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x662F;&#x6574;&#x4E2A;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x7684;&#x6B63;&#x6574;&#x6570;&#x3002; <code>periodic</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x4FEE;&#x526A;&#x6389;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x91CD;&#x590D;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x597D;&#x7528;&#x4F5C;&#x5177;&#x6709; <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> &#x4E4B;&#x7C7B;&#x7684;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>periodic</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5B9E;&#x9645;&#x4E0A;&#x4E3A;<img src="img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" alt="">&#x3002; &#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x662F;<code>torch.bartlett_window(L, periodic=True)</code>&#x7B49;&#x4E8E;<code>torch.bartlett_window(L + 1, periodic=False)[:-1])</code>&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>window_length</code> <img src="img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" alt="">&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5305;&#x542B;&#x5355;&#x4E2A;&#x503C; 1&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8FD4;&#x56DE;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;</p>
</li>
<li><p><strong>&#x5468;&#x671F;&#x6027;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;&#x4E3A; True&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7528;&#x4F5C;&#x5468;&#x671F;&#x6027;&#x51FD;&#x6570;&#x7684;&#x7A97;&#x53E3;&#x3002; &#x5982;&#x679C;&#x4E3A; False&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#x4F7F;&#x7528;&#x5168;&#x5C40;&#x9ED8;&#x8BA4;&#x503C;(&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>)&#x3002; &#x4EC5;&#x652F;&#x6301;&#x6D6E;&#x70B9;&#x7C7B;&#x578B;&#x3002;</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x5E03;&#x5C40;&#x3002; &#x4EC5;&#x652F;&#x6301;<code>torch.strided</code>(&#x5BC6;&#x96C6;&#x5E03;&#x5C40;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;&#x4E3A;<img src="img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr>
<pre><code>torch.blackman_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5E03;&#x83B1;&#x514B;&#x66FC;&#x7A97;&#x53E3;&#x529F;&#x80FD;&#x3002;</p>
<p><img src="img/ee8023c13513826303e160aa8135b175.jpg" alt=""></p>
<p>where <img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt=""> is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x7684;&#x6B63;&#x6574;&#x6570;&#x3002; <code>periodic</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x4FEE;&#x526A;&#x6389;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x91CD;&#x590D;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x597D;&#x7528;&#x4F5C;&#x5177;&#x6709; <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> &#x4E4B;&#x7C7B;&#x7684;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>periodic</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5B9E;&#x9645;&#x4E0A;&#x4E3A;<img src="img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" alt="">&#x3002; &#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x662F;<code>torch.blackman_window(L, periodic=True)</code>&#x7B49;&#x4E8E;<code>torch.blackman_window(L + 1, periodic=False)[:-1])</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code> <img src="img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" alt="">, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<em>python:int</em>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img src="img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" alt=""> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr>
<pre><code>torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6C49;&#x660E;&#x7A97;&#x529F;&#x80FD;&#x3002;</p>
<p><img src="img/5fe81b7bd5bd7c7e122962f6739b7c8e.jpg" alt=""></p>
<p>where <img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt=""> is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x7684;&#x6B63;&#x6574;&#x6570;&#x3002; <code>periodic</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x4FEE;&#x526A;&#x6389;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x91CD;&#x590D;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x597D;&#x7528;&#x4F5C;&#x5177;&#x6709; <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> &#x4E4B;&#x7C7B;&#x7684;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>periodic</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5B9E;&#x9645;&#x4E0A;&#x4E3A;<img src="img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" alt="">&#x3002; &#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x662F;<code>torch.hamming_window(L, periodic=True)</code>&#x7B49;&#x4E8E;<code>torch.hamming_window(L + 1, periodic=False)[:-1])</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code> <img src="img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" alt="">, the returned window contains a single value 1.</p>
<p>Note</p>
<p>&#x8FD9;&#x662F; <a href="#torch.hann_window" title="torch.hann_window"><code>torch.hann_window()</code></a> &#x7684;&#x901A;&#x7528;&#x7248;&#x672C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<em>python:int</em>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>alpha</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;&#x7CFB;&#x6570;<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt=""></p>
</li>
<li><p><strong>beta</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;&#x7CFB;&#x6570;<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt=""></p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img src="img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" alt=""> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<hr>
<pre><code>torch.hann_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6C49;&#x6069;&#x7A97;&#x53E3;&#x529F;&#x80FD;&#x3002;</p>
<p><img src="img/05f672670a77d8bc009f743a19461201.jpg" alt=""></p>
<p>where <img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt=""> is the full window size.</p>
<p>&#x8F93;&#x5165;<code>window_length</code>&#x662F;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x5927;&#x5C0F;&#x7684;&#x6B63;&#x6574;&#x6570;&#x3002; <code>periodic</code>&#x6807;&#x5FD7;&#x786E;&#x5B9A;&#x8FD4;&#x56DE;&#x7684;&#x7A97;&#x53E3;&#x662F;&#x5426;&#x4ECE;&#x5BF9;&#x79F0;&#x7A97;&#x53E3;&#x4E2D;&#x4FEE;&#x526A;&#x6389;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x91CD;&#x590D;&#x503C;&#xFF0C;&#x5E76;&#x51C6;&#x5907;&#x597D;&#x7528;&#x4F5C;&#x5177;&#x6709; <a href="#torch.stft" title="torch.stft"><code>torch.stft()</code></a> &#x4E4B;&#x7C7B;&#x7684;&#x5468;&#x671F;&#x6027;&#x7A97;&#x53E3;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x5982;&#x679C;<code>periodic</code>&#x4E3A;&#x771F;&#xFF0C;&#x5219;&#x4E0A;&#x5F0F;&#x4E2D;&#x7684;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5B9E;&#x9645;&#x4E0A;&#x4E3A;<img src="img/1cfea2e0a42e6ef5aee44b22f2bf2ab4.jpg" alt="">&#x3002; &#x53E6;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x662F;<code>torch.hann_window(L, periodic=True)</code>&#x7B49;&#x4E8E;<code>torch.hann_window(L + 1, periodic=False)[:-1])</code>&#x3002;</p>
<p>Note</p>
<p>If <code>window_length</code> <img src="img/193049f37e5b8049aa1c2f98d7bf7b2c.jpg" alt="">, the returned window contains a single value 1.</p>
<p>Parameters</p>
<ul>
<li><p><strong>window_length</strong> (<em>python:int</em>) &#x2013; the size of returned window</p>
</li>
<li><p><strong>periodic</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If True, returns a window to be used as periodic function. If False, return a symmetric window.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). Only floating point types are supported.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; the desired layout of returned window tensor. Only <code>torch.strided</code> (dense layout) is supported.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>requires_grad</strong> (<em>bool__,</em> <em>optional</em>) &#x2013; If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>
</li>
</ul>
<p>Returns</p>
<p>A 1-D tensor of size <img src="img/ba92c6cf7c07822f0e23d550a6cf1108.jpg" alt=""> containing the window</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<h3 id="&#x5176;&#x4ED6;&#x4F5C;&#x4E1A;">&#x5176;&#x4ED6;&#x4F5C;&#x4E1A;</h3>
<hr>
<pre><code>torch.bincount(input, weights=None, minlength=0) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x975E;&#x8D1F;&#x6574;&#x6570;&#x6570;&#x7EC4;&#x4E2D;&#x6BCF;&#x4E2A;&#x503C;&#x7684;&#x9891;&#x7387;&#x3002;</p>
<p>&#x9664;&#x975E;<code>input</code>&#x4E3A;&#x7A7A;&#xFF0C;&#x5426;&#x5219; bin &#x7684;&#x6570;&#x91CF;(&#x5927;&#x5C0F; 1&#xFF09;&#x6BD4;<code>input</code>&#x4E2D;&#x7684;&#x6700;&#x5927;&#x503C;&#x5927;&#x4E00;&#x4E2A;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x7ED3;&#x679C;&#x662F;&#x5927;&#x5C0F;&#x4E3A; 0 &#x7684;&#x5F20;&#x91CF;&#x3002;&#x5982;&#x679C;&#x6307;&#x5B9A;<code>minlength</code>&#xFF0C;&#x5219; bin &#x7684;&#x6570;&#x91CF;&#x4E3A; &#x81F3;&#x5C11;<code>minlength</code>&#x4E3A;&#x7A7A;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x4E3A;&#x7A7A;&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x4E3A;&#x5927;&#x5C0F;&#x4E3A;<code>minlength</code>&#x7684;&#x5F20;&#x91CF;&#x586B;&#x5145;&#x96F6;&#x3002; &#x5982;&#x679C;<code>n</code>&#x662F;&#x4F4D;&#x7F6E;<code>i</code>&#x7684;&#x503C;&#xFF0C;&#x5219;&#x5982;&#x679C;&#x6307;&#x5B9A;&#x4E86;<code>weights</code>&#x5219;&#x4E3A;<code>out[n] += weights[i]</code>&#xFF0C;&#x5426;&#x5219;&#x6307;&#x5B9A;<code>out[n] += 1</code>&#x3002;</p>
<p>Note</p>
<p>&#x4F7F;&#x7528; CUDA &#x540E;&#x7AEF;&#x65F6;&#xFF0C;&#x6B64;&#x64CD;&#x4F5C;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x4E0D;&#x786E;&#x5B9A;&#x7684;&#x884C;&#x4E3A;&#xFF0C;&#x4E0D;&#x5BB9;&#x6613;&#x5173;&#x95ED;&#x3002; &#x6709;&#x5173;&#x80CC;&#x666F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;<a href="notes/randomness.html">&#x91CD;&#x73B0;&#x6027;</a>&#x7684;&#x6CE8;&#x91CA;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013; 1-d int &#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x6743;&#x91CD;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x53EF;&#x9009;&#xFF0C;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x4E2D;&#x6BCF;&#x4E2A;&#x503C;&#x7684;&#x6743;&#x91CD;&#x3002; &#x5E94;&#x5177;&#x6709;&#x4E0E;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x76F8;&#x540C;&#x7684;&#x5927;&#x5C0F;&#x3002;</p>
</li>
<li><p><strong>minlength</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x53EF;&#x9009;&#xFF0C;&#x6700;&#x5C0F;&#x5B58;&#x50A8;&#x7BB1;&#x6570;&#x3002; &#x5E94;&#x8BE5;&#x662F;&#x975E;&#x8D1F;&#x7684;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5982;&#x679C;<code>input</code>&#x4E3A;&#x975E;&#x7A7A;&#xFF0C;&#x5219;&#x4E3A;<code>Size([max(input) + 1])</code>&#x5F62;&#x72B6;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5426;&#x5219;&#x4E3A;<code>Size(0)</code></p>
<p>Return type</p>
<p>&#x8F93;&#x51FA;(<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64)
&gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5)
&gt;&gt;&gt; input, weights
(tensor([4, 3, 6, 3, 4]),
 tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

&gt;&gt;&gt; torch.bincount(input)
tensor([0, 0, 0, 2, 2, 0, 1])

&gt;&gt;&gt; input.bincount(weights)
tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
</code></pre><hr>
<pre><code>torch.broadcast_tensors(*tensors) &#x2192; List of Tensors&#xB6;
</code></pre><p>&#x6839;&#x636E;<a href="notes/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;&#x8BED;&#x4E49;</a>&#x5E7F;&#x64AD;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x3002;</p>
<p>Parameters</p>
<p><strong>*&#x5F20;&#x91CF;</strong> &#x2013;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x76F8;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x5F20;&#x91CF;</p>
<p>Warning</p>
<p>&#x5E7F;&#x64AD;&#x5F20;&#x91CF;&#x7684;&#x4E00;&#x4E2A;&#x4EE5;&#x4E0A;&#x5143;&#x7D20;&#x53EF;&#x4EE5;&#x5F15;&#x7528;&#x5355;&#x4E2A;&#x5B58;&#x50A8;&#x4F4D;&#x7F6E;&#x3002; &#x7ED3;&#x679C;&#xFF0C;&#x5C31;&#x5730;&#x64CD;&#x4F5C;(&#x5C24;&#x5176;&#x662F;&#x77E2;&#x91CF;&#x5316;&#x7684;&#x64CD;&#x4F5C;&#xFF09;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x7684;&#x884C;&#x4E3A;&#x3002; &#x5982;&#x679C;&#x9700;&#x8981;&#x5199;&#x5F20;&#x91CF;&#xFF0C;&#x8BF7;&#x5148;&#x514B;&#x9686;&#x5B83;&#x4EEC;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(3).view(1, 3)
&gt;&gt;&gt; y = torch.arange(2).view(2, 1)
&gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)
&gt;&gt;&gt; a.size()
torch.Size([2, 3])
&gt;&gt;&gt; a
tensor([[0, 1, 2],
        [0, 1, 2]])
</code></pre><hr>
<pre><code>torch.cartesian_prod(*tensors)&#xB6;
</code></pre><p>&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x5E8F;&#x5217;&#x7684;&#x7B1B;&#x5361;&#x5C14;&#x79EF;&#x3002; &#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E; python &#x7684; &lt;cite&gt;itertools.product&lt;/cite&gt; &#x3002;</p>
<p>Parameters</p>
<p><strong>*&#x5F20;&#x91CF;</strong> &#x2013;&#x4EFB;&#x610F;&#x6570;&#x91CF;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p>Returns</p>
<pre><code>A tensor equivalent to converting all the input tensors into lists,
</code></pre><p>&#x5728;&#x8FD9;&#x4E9B;&#x5217;&#x8868;&#x4E0A;&#x6267;&#x884C; &lt;cite&gt;itertools.product&lt;/cite&gt; &#xFF0C;&#x6700;&#x540E;&#x5C06;&#x7ED3;&#x679C;&#x5217;&#x8868;&#x8F6C;&#x6362;&#x4E3A;&#x5F20;&#x91CF;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; b = [4, 5]
&gt;&gt;&gt; list(itertools.product(a, b))
[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; tensor_b = torch.tensor(b)
&gt;&gt;&gt; torch.cartesian_prod(tensor_a, tensor_b)
tensor([[1, 4],
        [1, 5],
        [2, 4],
        [2, 5],
        [3, 4],
        [3, 5]])
</code></pre><hr>
<pre><code>torch.cdist(x1, x2, p=2, compute_mode=&apos;use_mm_for_euclid_dist_if_necessary&apos;)&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x6279;&#x5904;&#x7406;&#x884C;&#x5411;&#x91CF;&#x7684;&#x4E24;&#x4E2A;&#x96C6;&#x5408;&#x7684;&#x6BCF;&#x5BF9;&#x4E4B;&#x95F4;&#x7684; p &#x8303;&#x6570;&#x8DDD;&#x79BB;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>x1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F62;&#x72B6;&#x4E3A;<img src="img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>x2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F62;&#x72B6;&#x4E3A;<img src="img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>p</strong> -p &#x8303;&#x6570;&#x8DDD;&#x79BB;&#x7684; p &#x503C;&#xFF0C;&#x4EE5;&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A;&#x5411;&#x91CF;&#x5BF9;<img src="img/2ed3340d16bd283f671129abab392a0b.jpg" alt="">&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002;</p>
</li>
<li><p><strong>compute_mode</strong> &#x2013;&apos;use_mm_for_euclid_dist_if_necessary&apos;-&#x5982;&#x679C; P &gt; 25 &#x6216; R &gt; 25&apos;use_mm_for_euclid_dist&apos;-&#x5C06;&#x59CB;&#x7EC8;&#x4F7F;&#x7528;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x65B9;&#x6CD5;&#x6765;&#x8BA1;&#x7B97;&#x6B27;&#x51E0;&#x91CC;&#x5FB7;&#x8DDD;&#x79BB;(p = 2&#xFF09; &#x6B27;&#x5F0F;&#x8DDD;&#x79BB;(p = 2&#xFF09;&apos;donot_use_mm_for_euclid_dist&apos;-&#x6C38;&#x8FDC;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x65B9;&#x6CD5;&#x6765;&#x8BA1;&#x7B97;&#x6B27;&#x5F0F;&#x8DDD;&#x79BB;(p = 2&#xFF09;&#x9ED8;&#x8BA4;&#x503C;&#xFF1A;use_mm_for_euclid_dist_if_necessary&#x3002;</p>
</li>
</ul>
<p>&#x5982;&#x679C; x1 &#x5177;&#x6709;&#x5F62;&#x72B6;<img src="img/b34d73ac200b58cfa9bc0608c6e1658a.jpg" alt="">&#xFF0C;&#x800C; x2 &#x5177;&#x6709;&#x5F62;&#x72B6;<img src="img/a77a440e5ab82d234d8ad3c6f8c5dc73.jpg" alt="">&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5C06;&#x5177;&#x6709;&#x5F62;&#x72B6;<img src="img/48db404261f2466cb86616d6f920e894.jpg" alt="">&#x3002;</p>
<p>&#x5982;&#x679C;<img src="img/5323d331671967548fee2dd052f584b9.jpg" alt="">&#xFF0C;&#x5219;&#x6B64;&#x51FD;&#x6570;&#x7B49;&#x6548;&#x4E8E; &lt;cite&gt;scipy.spatial.distance.cdist(input&#xFF0C;&apos;minkowski&apos;&#xFF0C;p = p&#xFF09;&lt;/cite&gt;&#x3002; &#x5F53;<img src="img/0ed455324c64d9801ef75fe54be6b565.jpg" alt="">&#x7B49;&#x4E8E; &lt;cite&gt;scipy.spatial.distance.cdist(input&#xFF0C;&apos;hamming&apos;&#xFF09;* M&lt;/cite&gt; &#x3002; &#x5F53;<img src="img/2a680efba9b8e37cfffde015324eb07e.jpg" alt="">&#x65F6;&#xFF0C;&#x6700;&#x63A5;&#x8FD1;&#x7684; scipy &#x51FD;&#x6570;&#x662F; &lt;cite&gt;scipy.spatial.distance.cdist(xn&#xFF0C;lambda x&#xFF0C;y&#xFF1A;np.abs(x-y&#xFF09;.max(&#xFF09;&#xFF09;&lt;/cite&gt;&#x3002;</p>
<p>Example</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])
&gt;&gt;&gt; a
tensor([[ 0.9041,  0.0196],
        [-0.3108, -2.4423],
        [-0.4821,  1.0590]])
&gt;&gt;&gt; b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])
&gt;&gt;&gt; b
tensor([[-2.1763, -0.4713],
        [-0.6986,  1.3702]])
&gt;&gt;&gt; torch.cdist(a, b, p=2)
tensor([[3.1193, 2.0959],
        [2.7138, 3.8322],
        [2.2830, 0.3791]])
</code></pre><hr>
<pre><code>torch.combinations(input, r=2, with_replacement=False) &#x2192; seq&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x7ED9;&#x5B9A;&#x5F20;&#x91CF;&#x7684;&#x957F;&#x5EA6;<img src="img/a892400c6ed0045af4aedcc03b3e5fd5.jpg" alt="">&#x7684;&#x7EC4;&#x5408;&#x3002; &#x5F53; &lt;cite&gt;with_replacement&lt;/cite&gt; &#x8BBE;&#x7F6E;&#x4E3A; &lt;cite&gt;False&lt;/cite&gt; &#x65F6;&#xFF0C;&#x8BE5;&#x884C;&#x4E3A;&#x7C7B;&#x4F3C;&#x4E8E; python &#x7684; &lt;cite&gt;itertools.combinations&lt;/cite&gt; &#xFF1B;&#x5F53; &lt;cite&gt;with_replacement&lt;/cite&gt; &#x8BBE;&#x7F6E;&#x4E3A;&#x65F6;&#xFF0C;&#x8BE5;&#x884C;&#x4E3A;&#x4E0E; &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; &#x76F8;&#x4F3C;&#x3002; HTG10]&#x8BBE;&#x7F6E;&#x4E3A; &lt;cite&gt;True&lt;/cite&gt; &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x4E00;&#x7EF4;&#x77E2;&#x91CF;&#x3002;</p>
</li>
<li><p><strong>r</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8981;&#x7EC4;&#x5408;&#x7684;&#x5143;&#x7D20;&#x6570;</p>
</li>
<li><p><strong>with_replacement</strong> (<em>&#x5E03;&#x5C14;&#x503C;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x5426;&#x5141;&#x8BB8;&#x91CD;&#x590D;&#x590D;&#x5236;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x7B49;&#x4E8E;&#x5C06;&#x6240;&#x6709;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x8F6C;&#x6362;&#x4E3A;&#x5217;&#x8868;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5BF9;&#x8FD9;&#x4E9B;&#x5217;&#x8868;&#x6267;&#x884C; &lt;cite&gt;itertools.combinations&lt;/cite&gt; &#x6216; &lt;cite&gt;itertools.combinations_with_replacement&lt;/cite&gt; &#xFF0C;&#x6700;&#x540E;&#x5C06;&#x7ED3;&#x679C;&#x5217;&#x8868;&#x8F6C;&#x6362;&#x4E3A;&#x5F20;&#x91CF;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = [1, 2, 3]
&gt;&gt;&gt; list(itertools.combinations(a, r=2))
[(1, 2), (1, 3), (2, 3)]
&gt;&gt;&gt; list(itertools.combinations(a, r=3))
[(1, 2, 3)]
&gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2))
[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
&gt;&gt;&gt; tensor_a = torch.tensor(a)
&gt;&gt;&gt; torch.combinations(tensor_a)
tensor([[1, 2],
        [1, 3],
        [2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, r=3)
tensor([[1, 2, 3]])
&gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True)
tensor([[1, 1],
        [1, 2],
        [1, 3],
        [2, 2],
        [2, 3],
        [3, 3]])
</code></pre><hr>
<pre><code>torch.cross(input, other, dim=-1, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5411;&#x91CF;&#x5728;<code>input</code>&#x548C;<code>other</code>&#x7684;&#x7EF4;&#x5EA6;<code>dim</code>&#x4E2D;&#x7684;&#x53C9;&#x79EF;&#x3002;</p>
<p><code>input</code>&#x548C;<code>other</code>&#x7684;&#x5927;&#x5C0F;&#x5FC5;&#x987B;&#x76F8;&#x540C;&#xFF0C;&#x5E76;&#x4E14;<code>dim</code>&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x5E94;&#x4E3A; 3&#x3002;</p>
<p>&#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B;<code>dim</code>&#xFF0C;&#x5219;&#x9ED8;&#x8BA4;&#x4E3A;&#x627E;&#x5230;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x4E3A; 3 &#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>other</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second input tensor</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53D6;&#x53C9;&#x79EF;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)
&gt;&gt;&gt; a
tensor([[-0.3956,  1.1455,  1.6895],
        [-0.5849,  1.3672,  0.3599],
        [-1.1626,  0.7180, -0.0521],
        [-0.1339,  0.9902, -2.0225]])
&gt;&gt;&gt; b = torch.randn(4, 3)
&gt;&gt;&gt; b
tensor([[-0.0257, -1.4725, -1.2251],
        [-1.1479, -0.7005, -1.9757],
        [-1.3904,  0.3726, -1.1836],
        [-0.9688, -0.7153,  0.2159]])
&gt;&gt;&gt; torch.cross(a, b, dim=1)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
&gt;&gt;&gt; torch.cross(a, b)
tensor([[ 1.0844, -0.5281,  0.6120],
        [-2.4490, -1.5687,  1.9792],
        [-0.8304, -1.3037,  0.5650],
        [-1.2329,  1.9883,  1.0551]])
</code></pre><hr>
<pre><code>torch.cumprod(input, dim, out=None, dtype=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7EF4;&#x5EA6;&#x4E3A;<code>dim</code>&#x7684;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x7D2F;&#x79EF;&#x79EF;&#x3002;</p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A; N &#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x4E5F;&#x5C06;&#x662F;&#x5927;&#x5C0F;&#x4E3A; N &#x7684;&#x5411;&#x91CF;(&#x5E26;&#x6709;&#x5143;&#x7D20;&#xFF09;&#x3002;</p>
<p><img src="img/89cdfc25537850f458deb6fa7119b4b4.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x6267;&#x884C;&#x64CD;&#x4F5C;&#x7684;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
        -0.2129, -0.4206,  0.1968])
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
         0.0014, -0.0006, -0.0001])

&gt;&gt;&gt; a[5] = 0.0
&gt;&gt;&gt; torch.cumprod(a, dim=0)
tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
         0.0000, -0.0000, -0.0000])
</code></pre><hr>
<pre><code>torch.cumsum(input, dim, out=None, dtype=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x7EF4;&#x5EA6;&#x4E3A;<code>dim</code>&#x7684;<code>input</code>&#x5143;&#x7D20;&#x7684;&#x7D2F;&#x79EF;&#x548C;&#x3002;</p>
<p>For example, if <code>input</code> is a vector of size N, the result will also be a vector of size N, with elements.</p>
<p><img src="img/0a2d62fcc8f81c2e2b3da579ec8cbccb.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; the dimension to do the operation over</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(10)
&gt;&gt;&gt; a
tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
         0.1850, -1.1571, -0.4243])
&gt;&gt;&gt; torch.cumsum(a, dim=0)
tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
        -1.8209, -2.9780, -3.4022])
</code></pre><hr>
<pre><code>torch.diag(input, diagonal=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><ul>
<li><p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x77E2;&#x91CF;(1-D &#x5F20;&#x91CF;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4EE5;<code>input</code>&#x7684;&#x5143;&#x7D20;&#x4E3A;&#x5BF9;&#x89D2;&#x7EBF;&#x7684; 2-D &#x65B9;&#x5F62;&#x5F20;&#x91CF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x77E9;&#x9635;(2-D &#x5F20;&#x91CF;&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5E26;&#x6709;<code>input</code>&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x7684; 1-D &#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>&#x53C2;&#x6570; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0&#xFF0C;&#x5219;&#x5B83;&#x662F;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &gt; 0&#xFF0C;&#x5219;&#x5B83;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &lt; 0&#xFF0C;&#x5219;&#x5B83;&#x4F4D;&#x4E8E;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x3002;</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x5BF9;&#x89D2;&#x7EBF;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5BF9;&#x89D2;&#x7EBF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>See also</p>
<p><a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> &#x59CB;&#x7EC8;&#x8FD4;&#x56DE;&#x5176;&#x8F93;&#x5165;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
<p><a href="#torch.diagflat" title="torch.diagflat"><code>torch.diagflat()</code></a> &#x59CB;&#x7EC8;&#x4F7F;&#x7528;&#x8F93;&#x5165;&#x6307;&#x5B9A;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x5143;&#x7D20;&#x6784;&#x9020;&#x5F20;&#x91CF;&#x3002;</p>
<p>Examples:</p>
<p>&#x83B7;&#x53D6;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4E3A;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x65B9;&#x9635;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([ 0.5950,-0.0872, 2.3298])
&gt;&gt;&gt; torch.diag(a)
tensor([[ 0.5950, 0.0000, 0.0000],
        [ 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 2.3298]])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
        [ 0.0000, 0.0000,-0.0872, 0.0000],
        [ 0.0000, 0.0000, 0.0000, 2.3298],
        [ 0.0000, 0.0000, 0.0000, 0.0000]])
</code></pre><p>&#x83B7;&#x53D6;&#x7ED9;&#x5B9A;&#x77E9;&#x9635;&#x7684;&#x7B2C; k &#x4E2A;&#x5BF9;&#x89D2;&#x7EBF;&#xFF1A;</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-0.4264, 0.0255,-0.1064],
        [ 0.8795,-0.2429, 0.1374],
        [ 0.1029,-0.6482,-1.6300]])
&gt;&gt;&gt; torch.diag(a, 0)
tensor([-0.4264,-0.2429,-1.6300])
&gt;&gt;&gt; torch.diag(a, 1)
tensor([ 0.0255, 0.1374])
</code></pre><hr>
<pre><code>torch.diag_embed(input, offset=0, dim1=-2, dim2=-1) &#x2192; Tensor&#xB6;
</code></pre><p>&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x67D0;&#x4E9B; 2D &#x5E73;&#x9762;(&#x7531;<code>dim1</code>&#x548C;<code>dim2</code>&#x6307;&#x5B9A;&#xFF09;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x7531;<code>input</code>&#x586B;&#x5145;&#x3002; &#x4E3A;&#x4E86;&#x4FBF;&#x4E8E;&#x521B;&#x5EFA;&#x6279;&#x5904;&#x7406;&#x5BF9;&#x89D2;&#x77E9;&#x9635;&#xFF0C;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#x9009;&#x62E9;&#x7531;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6700;&#x540E;&#x4E24;&#x4E2A;&#x7EF4;&#x6784;&#x6210;&#x7684; 2D &#x5E73;&#x9762;&#x3002;</p>
<p>&#x53C2;&#x6570;<code>offset</code>&#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C;<code>offset</code> = 0&#xFF0C;&#x5219;&#x5B83;&#x662F;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>offset</code> &gt;&#x4E3A; 0&#xFF0C;&#x5219;&#x5B83;&#x200B;&#x200B;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;<code>offset</code> &lt;&#x4E3A; 0&#xFF0C;&#x5219;&#x5B83;&#x200B;&#x200B;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x3002;</p>
</li>
</ul>
<p>&#x5C06;&#x8BA1;&#x7B97;&#x65B0;&#x77E9;&#x9635;&#x7684;&#x5927;&#x5C0F;&#xFF0C;&#x4EE5;&#x4F7F;&#x6307;&#x5B9A;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x6210;&#x4E3A;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x8F93;&#x5165;&#x5C3A;&#x5BF8;&#x7684;&#x5927;&#x5C0F;&#x3002; &#x6CE8;&#x610F;&#xFF0C;&#x5BF9;&#x4E8E;<img src="img/e647c6371faf8b6dd9327515ae95cbdb.jpg" alt="">&#x4EE5;&#x5916;&#x7684;<code>offset</code>&#xFF0C;<code>dim1</code>&#x548C;<code>dim2</code>&#x7684;&#x987A;&#x5E8F;&#x5F88;&#x91CD;&#x8981;&#x3002; &#x4EA4;&#x6362;&#x5B83;&#x4EEC;&#x7B49;&#x6548;&#x4E8E;&#x66F4;&#x6539;<code>offset</code>&#x7684;&#x7B26;&#x53F7;&#x3002;</p>
<p>&#x5C06; <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> &#x5E94;&#x7528;&#x4E8E;&#x5177;&#x6709;&#x76F8;&#x540C;&#x53C2;&#x6570;&#x7684;&#x8BE5;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5C06;&#x4EA7;&#x751F;&#x4E0E;&#x8F93;&#x5165;&#x76F8;&#x540C;&#x7684;&#x77E9;&#x9635;&#x3002; &#x4F46;&#x662F;&#xFF0C; <a href="#torch.diagonal" title="torch.diagonal"><code>torch.diagonal()</code></a> &#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x9ED8;&#x8BA4;&#x5C3A;&#x5BF8;&#xFF0C;&#x56E0;&#x6B64;&#x9700;&#x8981;&#x660E;&#x786E;&#x6307;&#x5B9A;&#x8FD9;&#x4E9B;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002; &#x5FC5;&#x987B;&#x81F3;&#x5C11;&#x4E3A;&#x4E00;&#x7EF4;&#x3002;</p>
</li>
<li><p><strong>&#x504F;&#x79FB;&#x91CF;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0(&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF09;&#x3002;</p>
</li>
<li><p><strong>dim1</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x7B2C;&#x4E00;&#x7EF4;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;-2&#x3002;</p>
</li>
<li><p><strong>dim2</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;-1&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 3)
&gt;&gt;&gt; torch.diag_embed(a)
tensor([[[ 1.5410,  0.0000,  0.0000],
         [ 0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -2.1788]],

        [[ 0.5684,  0.0000,  0.0000],
         [ 0.0000, -1.0845,  0.0000],
         [ 0.0000,  0.0000, -1.3986]]])

&gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2)
tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],
         [ 0.0000,  0.5684,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000, -0.2934,  0.0000],
         [ 0.0000,  0.0000, -1.0845,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000, -2.1788],
         [ 0.0000,  0.0000,  0.0000, -1.3986]],

        [[ 0.0000,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000]]])
</code></pre><hr>
<pre><code>torch.diagflat(input, offset=0) &#x2192; Tensor&#xB6;
</code></pre><ul>
<li><p>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor with the elements of <code>input</code> as the diagonal.</p>
</li>
<li><p>&#x5982;&#x679C;<code>input</code>&#x662F;&#x4E00;&#x7EF4;&#x4EE5;&#x4E0A;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x5BF9;&#x89D2;&#x7EBF;&#x5143;&#x7D20;&#x7B49;&#x4E8E;&#x5C55;&#x5E73;&#x7684;<code>input</code>&#x3002;</p>
</li>
</ul>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li><p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li><p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li><p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x504F;&#x79FB;&#x91CF;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0(&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#xFF09;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3)
&gt;&gt;&gt; a
tensor([-0.2956, -0.9068,  0.1695])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[-0.2956,  0.0000,  0.0000],
        [ 0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.1695]])
&gt;&gt;&gt; torch.diagflat(a, 1)
tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.9068,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.1695],
        [ 0.0000,  0.0000,  0.0000,  0.0000]])

&gt;&gt;&gt; a = torch.randn(2, 2)
&gt;&gt;&gt; a
tensor([[ 0.2094, -0.3018],
        [-0.1516,  1.9342]])
&gt;&gt;&gt; torch.diagflat(a)
tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.3018,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1516,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  1.9342]])
</code></pre><hr>
<pre><code>torch.diagonal(input, offset=0, dim1=0, dim2=1) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;<code>input</code>&#x7684;&#x5C40;&#x90E8;&#x89C6;&#x56FE;&#xFF0C;&#x5176;&#x5BF9;&#x89D2;&#x7EBF;&#x5143;&#x7D20;&#x76F8;&#x5BF9;&#x4E8E;<code>dim1</code>&#x548C;<code>dim2</code>&#x9644;&#x52A0;&#x4E3A;&#x5C3A;&#x5BF8;&#x7684;&#x672B;&#x7AEF;&#x5F62;&#x72B6;&#x3002;</p>
<p>The argument <code>offset</code> controls which diagonal to consider:</p>
<ul>
<li><p>If <code>offset</code> = 0, it is the main diagonal.</p>
</li>
<li><p>If <code>offset</code> &gt; 0, it is above the main diagonal.</p>
</li>
<li><p>If <code>offset</code> &lt; 0, it is below the main diagonal.</p>
</li>
</ul>
<p>&#x5C06; <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> &#x5E94;&#x7528;&#x4E8E;&#x5177;&#x6709;&#x76F8;&#x540C;&#x53C2;&#x6570;&#x7684;&#x6B64;&#x51FD;&#x6570;&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x5C06;&#x4EA7;&#x751F;&#x4E00;&#x4E2A;&#x5E26;&#x6709;&#x8F93;&#x5165;&#x5BF9;&#x89D2;&#x7EBF;&#x9879;&#x7684;&#x5BF9;&#x89D2;&#x77E9;&#x9635;&#x3002; &#x4F46;&#x662F;&#xFF0C; <a href="#torch.diag_embed" title="torch.diag_embed"><code>torch.diag_embed()</code></a> &#x5177;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x9ED8;&#x8BA4;&#x5C3A;&#x5BF8;&#xFF0C;&#x56E0;&#x6B64;&#x9700;&#x8981;&#x660E;&#x786E;&#x6307;&#x5B9A;&#x8FD9;&#x4E9B;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x3002; &#x5FC5;&#x987B;&#x81F3;&#x5C11;&#x4E3A;&#x4E8C;&#x7EF4;&#x3002;</p>
</li>
<li><p><strong>offset</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; which diagonal to consider. Default: 0 (main diagonal).</p>
</li>
<li><p><strong>dim1</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x7B2C;&#x4E00;&#x7EF4;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;0</p>
</li>
<li><p><strong>dim2</strong>  (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53D6;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x7B2C;&#x4E8C;&#x7EF4;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x8981;&#x53D6;&#x4E00;&#x6279;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x8BF7;&#x4F20;&#x5165; dim1 = -2&#xFF0C;dim2 = -1&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0854,  1.1431, -0.1752],
        [ 0.8536, -0.0905,  0.0360],
        [ 0.6927, -0.3735, -0.4945]])

&gt;&gt;&gt; torch.diagonal(a, 0)
tensor([-1.0854, -0.0905, -0.4945])

&gt;&gt;&gt; torch.diagonal(a, 1)
tensor([ 1.1431,  0.0360])

&gt;&gt;&gt; x = torch.randn(2, 5, 4, 2)
&gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2)
tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
         [-1.1065,  1.0401, -0.2235, -0.7938]],

        [[-1.7325, -0.3081,  0.6166,  0.2335],
         [ 1.0500,  0.7336, -0.3836, -1.1015]]])
</code></pre><hr>
<pre><code>torch.einsum(equation, *operands) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6B64;&#x51FD;&#x6570;&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x79CD;&#x4F7F;&#x7528;&#x7231;&#x56E0;&#x65AF;&#x5766;&#x6C42;&#x548C;&#x7EA6;&#x5B9A;&#x6765;&#x8BA1;&#x7B97;&#x591A;&#x7EBF;&#x6027;&#x8868;&#x8FBE;&#x5F0F;(&#x5373;&#x4E58;&#x79EF;&#x548C;&#xFF09;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x516C;&#x5F0F;</strong>(<em>&#x5B57;&#x7B26;&#x4E32;</em>&#xFF09;&#x2013;&#x8BE5;&#x516C;&#x5F0F;&#x4EE5;&#x4E0E;&#x64CD;&#x4F5C;&#x6570;&#x548C;&#x7ED3;&#x679C;&#x7684;&#x6BCF;&#x4E2A;&#x7EF4;&#x76F8;&#x5173;&#x8054;&#x7684;&#x5C0F;&#x5199;&#x5B57;&#x6BCD;(&#x7D22;&#x5F15;&#xFF09;&#x5F62;&#x5F0F;&#x7ED9;&#x51FA;&#x3002; &#x5DE6;&#x4FA7;&#x5217;&#x51FA;&#x4E86;&#x64CD;&#x4F5C;&#x6570;&#x7EF4;&#xFF0C;&#x4EE5;&#x9017;&#x53F7;&#x5206;&#x9694;&#x3002; &#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x7EF4;&#x5E94;&#x8BE5;&#x6709;&#x4E00;&#x4E2A;&#x7D22;&#x5F15;&#x5B57;&#x6BCD;&#x3002; &#x53F3;&#x4FA7;&#x7D27;&#x968F;&lt;cite&gt;-&gt;&lt;/cite&gt; &#x4E4B;&#x540E;&#xFF0C;&#x5E76;&#x7ED9;&#x51FA;&#x8F93;&#x51FA;&#x7684;&#x7D22;&#x5F15;&#x3002; &#x5982;&#x679C;&#x7701;&#x7565;&lt;cite&gt;-&gt;&lt;/cite&gt; &#x548C;&#x53F3;&#x4FA7;&#xFF0C;&#x5219;&#x5C06;&#x5176;&#x9690;&#x5F0F;&#x5B9A;&#x4E49;&#x4E3A;&#x6240;&#x6709;&#x7D22;&#x5F15;&#x7684;&#x6309;&#x5B57;&#x6BCD;&#x987A;&#x5E8F;&#x6392;&#x5E8F;&#x7684;&#x5217;&#x8868;&#xFF0C;&#x8FD9;&#x4E9B;&#x5217;&#x8868;&#x5728;&#x5DE6;&#x4FA7;&#x4EC5;&#x51FA;&#x73B0;&#x4E00;&#x6B21;&#x3002; &#x5728;&#x5C06;&#x64CD;&#x4F5C;&#x6570;&#x6761;&#x76EE;&#x76F8;&#x4E58;&#x540E;&#xFF0C;&#x5C06;&#x8F93;&#x51FA;&#x4E2D;&#x4E0D;&#x7B49;&#x4E8E;&#x7684;&#x7D22;&#x5F15;&#x76F8;&#x52A0;&#x3002; &#x5982;&#x679C;&#x540C;&#x4E00;&#x64CD;&#x4F5C;&#x6570;&#x7684;&#x7D22;&#x5F15;&#x51FA;&#x73B0;&#x591A;&#x6B21;&#xFF0C;&#x5219;&#x91C7;&#x7528;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x692D;&#x5706;&lt;cite&gt;&#x2026;&lt;/cite&gt;&#x4EE3;&#x8868;&#x56FA;&#x5B9A;&#x6570;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x5982;&#x679C;&#x63A8;&#x65AD;&#x51FA;&#x53F3;&#x4FA7;&#xFF0C;&#x5219;&#x7701;&#x7565;&#x53F7;&#x5C3A;&#x5BF8;&#x4F4D;&#x4E8E;&#x8F93;&#x51FA;&#x7684;&#x5F00;&#x5934;&#x3002;</p>
</li>
<li><p><strong>&#x64CD;&#x4F5C;&#x6570;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8BA1;&#x7B97;&#x7231;&#x56E0;&#x65AF;&#x5766;&#x603B;&#x548C;&#x7684;&#x64CD;&#x4F5C;&#x6570;&#x3002;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; y = torch.randn(4)
&gt;&gt;&gt; torch.einsum(&apos;i,j-&gt;ij&apos;, x, y)  # outer product
tensor([[-0.0570, -0.0286, -0.0231,  0.0197],
        [ 1.2616,  0.6335,  0.5113, -0.4351],
        [ 1.4452,  0.7257,  0.5857, -0.4984],
        [-0.4647, -0.2333, -0.1883,  0.1603],
        [-1.1130, -0.5588, -0.4510,  0.3838]])

&gt;&gt;&gt; A = torch.randn(3,5,4)
&gt;&gt;&gt; l = torch.randn(2,5)
&gt;&gt;&gt; r = torch.randn(2,4)
&gt;&gt;&gt; torch.einsum(&apos;bn,anm,bm-&gt;ba&apos;, l, A, r) # compare torch.nn.functional.bilinear
tensor([[-0.3430, -5.2405,  0.4494],
        [ 0.3311,  5.5201, -3.0356]])

&gt;&gt;&gt; As = torch.randn(3,2,5)
&gt;&gt;&gt; Bs = torch.randn(3,5,4)
&gt;&gt;&gt; torch.einsum(&apos;bij,bjk-&gt;bik&apos;, As, Bs) # batch matrix multiplication
tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
         [-1.6706, -0.8097, -0.8025, -2.1183]],

        [[ 4.2239,  0.3107, -0.5756, -0.2354],
         [-1.4558, -0.3460,  1.5087, -0.8530]],

        [[ 2.8153,  1.8787, -4.3839, -1.2112],
         [ 0.3728, -2.1131,  0.0921,  0.8305]]])

&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.einsum(&apos;ii-&gt;i&apos;, A) # diagonal
tensor([-0.7825,  0.8291, -0.1936])

&gt;&gt;&gt; A = torch.randn(4, 3, 3)
&gt;&gt;&gt; torch.einsum(&apos;...ii-&gt;...i&apos;, A) # batch diagonal
tensor([[-1.0864,  0.7292,  0.0569],
        [-0.9725, -1.0270,  0.6493],
        [ 0.5832, -1.1716, -1.5084],
        [ 0.4041, -1.1690,  0.8570]])

&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; torch.einsum(&apos;...ij-&gt;...ji&apos;, A).shape # batch permute
torch.Size([2, 3, 5, 4])
</code></pre><hr>
<pre><code>torch.flatten(input, start_dim=0, end_dim=-1) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C55;&#x5E73;&#x5F20;&#x91CF;&#x4E2D;&#x8FDE;&#x7EED;&#x7684;&#x6697;&#x6DE1;&#x8303;&#x56F4;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>start_dim</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x7B2C;&#x4E00;&#x4E2A;&#x53D8;&#x6697;&#x7684;&#x50CF;&#x7D20;</p>
</li>
<li><p><strong>end_dim</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x53D8;&#x6697;&#x7684;&#x50CF;&#x7D20;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; t = torch.tensor([[[1, 2],
                       [3, 4]],
                      [[5, 6],
                       [7, 8]]])
&gt;&gt;&gt; torch.flatten(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
&gt;&gt;&gt; torch.flatten(t, start_dim=1)
tensor([[1, 2, 3, 4],
        [5, 6, 7, 8]])
</code></pre><hr>
<pre><code>torch.flip(input, dims) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6CBF;&#x7ED9;&#x5B9A;&#x8F74;&#x53CD;&#x8F6C; n-D &#x5F20;&#x91CF;&#x7684;&#x987A;&#x5E8F;&#xFF0C;&#x4EE5;&#x6697;&#x6DE1;&#x8868;&#x793A;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x4F7F;</strong>&#x53D8;&#x6697;(<em>&#x5217;&#x8868;</em> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em>&#xFF09;&#x2013;&#x7FFB;&#x8F6C;&#x8F74;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[ 0,  1],
         [ 2,  3]],

        [[ 4,  5],
         [ 6,  7]]])
&gt;&gt;&gt; torch.flip(x, [0, 1])
tensor([[[ 6,  7],
         [ 4,  5]],

        [[ 2,  3],
         [ 0,  1]]])
</code></pre><hr>
<pre><code>torch.rot90(input, k, dims) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5728;&#x8C03;&#x5149;&#x8F74;&#x6307;&#x5B9A;&#x7684;&#x5E73;&#x9762;&#x4E2D;&#x5C06; n-D &#x5F20;&#x91CF;&#x65CB;&#x8F6C; 90 &#x5EA6;&#x3002; &#x5982;&#x679C; k &gt; 0&#xFF0C;&#x5219;&#x65CB;&#x8F6C;&#x65B9;&#x5411;&#x662F;&#x4ECE;&#x7B2C;&#x4E00;&#x4E2A;&#x8F74;&#x5230;&#x7B2C;&#x4E8C;&#x4E2A;&#x8F74;&#xFF0C;&#x5BF9;&#x4E8E; k &lt; 0&#xFF0C;&#x65CB;&#x8F6C;&#x65B9;&#x5411;&#x662F;&#x4ECE;&#x7B2C;&#x4E8C;&#x4E2A;&#x8F74;&#x5230;&#x7B2C;&#x4E00;&#x4E2A;&#x8F74;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>k</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x65CB;&#x8F6C;&#x6B21;&#x6570;</p>
</li>
<li><p><strong>&#x4F7F;</strong>&#x53D8;&#x6697;(<em>&#x5217;&#x8868;</em> <em>&#x6216;</em> <em>&#x5143;&#x7EC4;</em>&#xFF09;&#x2013;&#x65CB;&#x8F6C;&#x8F74;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(4).view(2, 2)
&gt;&gt;&gt; x
tensor([[0, 1],
        [2, 3]])
&gt;&gt;&gt; torch.rot90(x, 1, [0, 1])
tensor([[1, 3],
        [0, 2]])

&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[0, 1],
         [2, 3]],

        [[4, 5],
         [6, 7]]])
&gt;&gt;&gt; torch.rot90(x, 1, [1, 2])
tensor([[[1, 3],
         [0, 2]],

        [[5, 7],
         [4, 6]]])
</code></pre><hr>
<pre><code>torch.histc(input, bins=100, min=0, max=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5F20;&#x91CF;&#x7684;&#x76F4;&#x65B9;&#x56FE;&#x3002;</p>
<p>&#x5143;&#x7D20;&#x88AB;&#x5206;&#x7C7B;&#x4E3A; <a href="#torch.min" title="torch.min"><code>min</code></a> &#x548C; <a href="#torch.max" title="torch.max"><code>max</code></a> &#x4E4B;&#x95F4;&#x7684;&#x7B49;&#x5BBD;&#x5355;&#x5143;&#x3002; &#x5982;&#x679C; <a href="#torch.min" title="torch.min"><code>min</code></a> &#x548C; <a href="#torch.max" title="torch.max"><code>max</code></a> &#x5747;&#x4E3A;&#x96F6;&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x6570;&#x636E;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x548C;&#x6700;&#x5927;&#x503C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x7BB1;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x76F4;&#x65B9;&#x56FE;&#x7BB1;&#x6570;</p>
</li>
<li><p><strong>min</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x8303;&#x56F4;&#x7684;&#x4E0B;&#x9650;(&#x5305;&#x62EC;&#xFF09;</p>
</li>
<li><p><strong>&#x6700;&#x5927;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x8303;&#x56F4;&#x7684;&#x4E0A;&#x9650;(&#x5305;&#x62EC;&#xFF09;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Returns</p>
<p>&#x76F4;&#x65B9;&#x56FE;&#x8868;&#x793A;&#x4E3A;&#x5F20;&#x91CF;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
</code></pre><hr>
<pre><code>torch.meshgrid(*tensors, **kwargs)&#xB6;
</code></pre><p>&#x53D6;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5F20;&#x91CF;(&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x53EF;&#x4EE5;&#x662F;&#x6807;&#x91CF;&#x6216;&#x4E00;&#x7EF4;&#x5411;&#x91CF;&#xFF09;&#xFF0C;&#x5E76;&#x521B;&#x5EFA;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt=""> N &#x7EF4;&#x7F51;&#x683C;&#xFF0C;&#x5176;&#x4E2D;&#x901A;&#x8FC7;&#x6269;&#x5C55;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt="">&#x5B9A;&#x4E49;<img src="img/db26d1a59be5965889bd4d5533b7be61.jpg" alt=""> &lt;sup&gt;&#x7B2C;&lt;/sup&gt;&#x7F51;&#x683C;&#x3002; &lt;sup&gt;&#x548C;&lt;/sup&gt;&#x8F93;&#x5165;&#x8D85;&#x51FA;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x5B9A;&#x4E49;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<blockquote>
<pre><code>Args:
</code></pre><p>&#x5F20;&#x91CF;(&#x5F20;&#x91CF;&#x5217;&#x8868;&#xFF09;&#xFF1A;&#x6807;&#x91CF;&#x6216;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x7684;&#x5217;&#x8868;&#x3002; &#x6807;&#x91CF;&#x5C06;&#x88AB;&#x81EA;&#x52A8;&#x89C6;&#x4E3A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/fc55934714b3777971b760dd3cf42978.jpg" alt="">&#x7684;&#x5F20;&#x91CF;</p>
<pre><code>Returns:
</code></pre><p>seq(&#x5F20;&#x91CF;&#x5E8F;&#x5217;&#xFF09;&#xFF1A;&#x5982;&#x679C;&#x8F93;&#x5165;&#x7684;<img src="img/678502bee29f9b13e7115b18864a5822.jpg" alt="">&#x5F20;&#x91CF;&#x4E3A;<img src="img/dc2a28f0eb5d798e9f246215dbb10795.jpg" alt="">&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x4E5F;&#x5C06;&#x5177;&#x6709;<img src="img/678502bee29f9b13e7115b18864a5822.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x6240;&#x6709;&#x5F20;&#x91CF;&#x5747;&#x4E3A;<img src="img/3a092c0d2d4874edbfe18b4364b8a48d.jpg" alt="">&#x3002;</p>
<p>Example:</p>
<pre><code>&amp;gt;&amp;gt;&amp;gt; x = torch.tensor([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; y = torch.tensor([4, 5, 6])
&amp;gt;&amp;gt;&amp;gt; grid_x, grid_y = torch.meshgrid(x, y)
&amp;gt;&amp;gt;&amp;gt; grid_x
tensor([[1, 1, 1],
        [2, 2, 2],
        [3, 3, 3]])
&amp;gt;&amp;gt;&amp;gt; grid_y
tensor([[4, 5, 6],
        [4, 5, 6],
        [4, 5, 6]])
</code></pre></blockquote>
<hr>
<pre><code>torch.renorm(input, p, dim, maxnorm, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<code>input</code>&#x6CBF;&#x7EF4;&#x5EA6;<code>dim</code>&#x7684;&#x6BCF;&#x4E2A;&#x5B50;&#x5F20;&#x91CF;&#x5747;&#x88AB;&#x89C4;&#x8303;&#x5316;&#xFF0C;&#x4EE5;&#x4F7F;&#x5B50;&#x5F20;&#x91CF;&#x7684; &lt;cite&gt;p&lt;/cite&gt; -norm &#x5C0F;&#x4E8E;&#x503C;<code>maxnorm</code></p>
<p>Note</p>
<p>&#x5982;&#x679C;&#x67D0;&#x884C;&#x7684;&#x8303;&#x6570;&#x4F4E;&#x4E8E; &lt;cite&gt;maxnorm&lt;/cite&gt; &#xFF0C;&#x5219;&#x8BE5;&#x884C;&#x4E0D;&#x53D8;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>p</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x8303;&#x6570;&#x8BA1;&#x7B97;&#x7684;&#x80FD;&#x529B;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x5207;&#x7247;&#x4EE5;&#x83B7;&#x5F97;&#x5B50;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x5EA6;</p>
</li>
<li><p><strong>maxnorm</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x4FDD;&#x6301;&#x6BCF;&#x4E2A;&#x5B50;&#x5F20;&#x91CF;&#x4F4E;&#x4E8E;&#x7684;&#x6700;&#x5927;&#x89C4;&#x8303;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)
&gt;&gt;&gt; x[1].fill_(2)
tensor([ 2.,  2.,  2.])
&gt;&gt;&gt; x[2].fill_(3)
tensor([ 3.,  3.,  3.])
&gt;&gt;&gt; x
tensor([[ 1.,  1.,  1.],
        [ 2.,  2.,  2.],
        [ 3.,  3.,  3.]])
&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)
tensor([[ 1.0000,  1.0000,  1.0000],
        [ 1.6667,  1.6667,  1.6667],
        [ 1.6667,  1.6667,  1.6667]])
</code></pre><hr>
<pre><code>torch.repeat_interleave()&#xB6;
</code></pre><hr>
<pre><code>torch.repeat_interleave(input, repeats, dim=None) &#x2192; Tensor
</code></pre><p>&#x91CD;&#x590D;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>Warning</p>
<p>&#x8FD9;&#x4E0E;<code>torch.repeat()</code>&#x4E0D;&#x540C;&#xFF0C;&#x4F46;&#x4E0E; &lt;cite&gt;numpy.repeat&lt;/cite&gt; &#x76F8;&#x4F3C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x91CD;&#x590D;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>python&#xFF1A;int</em> )&#x2013;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x91CD;&#x590D;&#x6B21;&#x6570;&#x3002; &#x91CD;&#x590D;&#x64AD;&#x653E;&#x4EE5;&#x9002;&#x5408;&#x7ED9;&#x5B9A;&#x8F74;&#x7684;&#x5F62;&#x72B6;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6CBF;&#x5176;&#x91CD;&#x590D;&#x503C;&#x7684;&#x5C3A;&#x5BF8;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528;&#x5C55;&#x5E73;&#x7684;&#x8F93;&#x5165;&#x6570;&#x7EC4;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x5C55;&#x5E73;&#x7684;&#x8F93;&#x51FA;&#x6570;&#x7EC4;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<pre><code>Repeated tensor which has the same shape as input, except along the
</code></pre><p>&#x7ED9;&#x5B9A;&#x7684;&#x8F74;&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])
&gt;&gt;&gt; x.repeat_interleave(2)
tensor([1, 1, 2, 2, 3, 3])
&gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, 2)
tensor([1, 1, 2, 2, 3, 3, 4, 4])
&gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]])
&gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],
        [3, 4]])
</code></pre><hr>
<pre><code>torch.repeat_interleave(repeats) &#x2192; Tensor
</code></pre><p>&#x5982;&#x679C;&lt;cite&gt;&#x91CD;&#x590D;&lt;/cite&gt;&#x4E3A;&lt;cite&gt;&#x5F20;&#x91CF;([n1&#xFF0C;n2&#xFF0C;n3&#xFF0C;&#x2026;]&#xFF09;&lt;/cite&gt;&#xFF0C;&#x5219;&#x8F93;&#x51FA;&#x5C06;&#x4E3A;&lt;cite&gt;&#x5F20;&#x91CF;([0&#xFF0C;0&#xFF0C;&#x2026;&#xFF0C;1&#xFF0C;1&#xFF0C; &#x2026;&#xFF0C;2&#xFF0C;2&#xFF0C;&#x2026;&#xFF0C;&#x2026;]&#xFF09;&lt;/cite&gt;&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;0&lt;/cite&gt; &#x51FA;&#x73B0; &lt;cite&gt;n1&lt;/cite&gt; &#x6B21;&#xFF0C; &lt;cite&gt;1&lt;/cite&gt; &#x51FA;&#x73B0; &lt;cite&gt;n2&lt;/cite&gt; &#x6B21;&#xFF0C;[ &lt;cite&gt;2&lt;/cite&gt; &#x51FA;&#x73B0; &lt;cite&gt;n3&lt;/cite&gt; &#x6B21;&#xFF0C;&#x7B49;&#x7B49;&#x3002;</p>
<hr>
<pre><code>torch.roll(input, shifts, dims=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6CBF;&#x7ED9;&#x5B9A;&#x5C3A;&#x5BF8;&#x6EDA;&#x52A8;&#x5F20;&#x91CF;&#x3002; &#x79FB;&#x51FA;&#x6700;&#x540E;&#x4F4D;&#x7F6E;&#x7684;&#x5143;&#x7D20;&#x5C06;&#x91CD;&#x65B0;&#x5F15;&#x5165;&#x7B2C;&#x4E00;&#x4E2A;&#x4F4D;&#x7F6E;&#x3002; &#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;&#x5C3A;&#x5BF8;&#xFF0C;&#x5219;&#x5F20;&#x91CF;&#x5C06;&#x5728;&#x6EDA;&#x52A8;&#x4E4B;&#x524D;&#x53D8;&#x5E73;&#xFF0C;&#x7136;&#x540E;&#x6062;&#x590D;&#x4E3A;&#x539F;&#x59CB;&#x5F62;&#x72B6;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>&#x79FB;&#x4F4D;</strong> (<em>python&#xFF1A;int</em> <em>&#x6216;</em> <em>python&#xFF1A;ints</em> &#x7684;&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x5F20;&#x91CF;&#x5143;&#x7D20;&#x79FB;&#x4F4D;&#x7684;&#x4F4D;&#x6570; &#x3002; &#x5982;&#x679C; shifts &#x662F;&#x4E00;&#x4E2A;&#x5143;&#x7EC4;&#xFF0C;&#x5219; dims &#x5FC5;&#x987B;&#x662F;&#x76F8;&#x540C;&#x5927;&#x5C0F;&#x7684;&#x5143;&#x7EC4;&#xFF0C;&#x5E76;&#x4E14;&#x6BCF;&#x4E2A;&#x7EF4;&#x5EA6;&#x5C06;&#x6EDA;&#x52A8;&#x76F8;&#x5E94;&#x7684;&#x503C;</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> (<em>python&#xFF1A;int</em> <em>&#x6216;</em> <em>tuple of python&#xFF1A;ints</em> )&#x2013;&#x6EDA;&#x52A8;&#x8F74;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
&gt;&gt;&gt; x
tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
&gt;&gt;&gt; torch.roll(x, 1, 0)
tensor([[7, 8],
        [1, 2],
        [3, 4],
        [5, 6]])
&gt;&gt;&gt; torch.roll(x, -1, 0)
tensor([[3, 4],
        [5, 6],
        [7, 8],
        [1, 2]])
&gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1))
tensor([[6, 5],
        [8, 7],
        [2, 1],
        [4, 3]])
</code></pre><hr>
<pre><code>torch.tensordot(a, b, dims=2)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE; a &#x548C; b &#x5728;&#x591A;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x6536;&#x7F29;&#x3002;</p>
<p><a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> &#x5B9E;&#x73B0;&#x4E86;&#x5E7F;&#x4E49;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>a</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5DE6;&#x5F20;&#x91CF;&#x6536;&#x7F29;</p>
</li>
<li><p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x53F3;&#x5F20;&#x91CF;&#x6536;&#x7F29;</p>
</li>
<li><p><strong>&#x53D8;&#x6697;</strong> (<em>python&#xFF1A;int</em> <em>&#x6216;</em> <em>python&#xFF1A;integers</em> &#x7684;&#x4E24;&#x4E2A;&#x5217;&#x8868;&#x7684;&#x5143;&#x7EC4;&#xFF09;&#x2013;&#x8981;&#x6536;&#x7F29;&#x7684;&#x5C3A;&#x5BF8;&#x6570;&#x6216;&#x5C3A;&#x5BF8;&#x7684;&#x663E;&#x5F0F;&#x5217;&#x8868; &#x5206;&#x522B;&#x7528;&#x4E8E;<code>a</code>&#x548C;<code>b</code></p>
</li>
</ul>
<p>&#x5F53;&#x4F7F;&#x7528;&#x6574;&#x6570;&#x53C2;&#x6570;<code>dims</code> = <img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt="">&#x8C03;&#x7528;&#x5E76;&#x4E14;<code>a</code>&#x548C;<code>b</code>&#x7684;&#x7EF4;&#x6570;&#x5206;&#x522B;&#x4E3A;<img src="img/03327e7b697db30918e96b2209927929.jpg" alt="">&#x548C;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x65F6;&#xFF0C;&#x5B83;&#x5C06;&#x8BA1;&#x7B97;</p>
<p><img src="img/1a4ce18109f30700be1f1dea87661f3e.jpg" alt=""></p>
<p>&#x5F53;&#x4F7F;&#x7528;&#x5217;&#x8868;&#x5F62;&#x5F0F;&#x7684;<code>dims</code>&#x8C03;&#x7528;&#x65F6;&#xFF0C;&#x7ED9;&#x5B9A;&#x7684;&#x5C3A;&#x5BF8;&#x5C06;&#x4EE3;&#x66FF;<code>a</code>&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;<img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt="">&#x548C;<img src="img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" alt="">&#x7684;&#x7B2C;&#x4E00;&#x4E2A;<img src="img/95d07e221a9b945feb93422a44ac3d42.jpg" alt="">&#x6536;&#x7F29;&#x3002; &#x8FD9;&#x4E9B;&#x5C3A;&#x5BF8;&#x7684;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x5339;&#x914D;&#xFF0C;&#x4F46;&#x662F; <a href="#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a> &#x5C06;&#x5904;&#x7406;&#x5E7F;&#x64AD;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)
&gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)
&gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))
tensor([[4400., 4730.],
        [4532., 4874.],
        [4664., 5018.],
        [4796., 5162.],
        [4928., 5306.]])

&gt;&gt;&gt; a = torch.randn(3, 4, 5, device=&apos;cuda&apos;)
&gt;&gt;&gt; b = torch.randn(4, 5, 6, device=&apos;cuda&apos;)
&gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()
tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])
</code></pre><hr>
<pre><code>torch.trace(input) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x8F93;&#x5165;&#x4E8C;&#x7EF4;&#x77E9;&#x9635;&#x5BF9;&#x89D2;&#x7EBF;&#x5143;&#x7D20;&#x7684;&#x603B;&#x548C;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3)
&gt;&gt;&gt; x
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.]])
&gt;&gt;&gt; torch.trace(x)
tensor(15.)
</code></pre><hr>
<pre><code>torch.tril(input, diagonal=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x77E9;&#x9635;(2-D &#x5F20;&#x91CF;&#xFF09;&#x6216;&#x77E9;&#x9635;&#x6279;&#x6B21;<code>input</code>&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#xFF0C;&#x7ED3;&#x679C;&#x5F20;&#x91CF;<code>out</code>&#x7684;&#x5176;&#x4ED6;&#x5143;&#x7D20;&#x8BBE;&#x7F6E;&#x4E3A; 0&#x3002;</p>
<p>&#x77E9;&#x9635;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x5B9A;&#x4E49;&#x4E3A;&#x5BF9;&#x89D2;&#x7EBF;&#x4E4B;&#x4E0A;&#x548C;&#x4E4B;&#x4E0B;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x6570; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x5982;&#x679C; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0&#xFF0C;&#x5219;&#x4FDD;&#x7559;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x548C;&#x4E0B;&#x65B9;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002; &#x6B63;&#x503C;&#x5305;&#x62EC;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x540C;&#x6837;&#xFF0C;&#x8D1F;&#x503C;&#x6392;&#x9664;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;<img src="img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" alt="">&#x7684;&#x7D22;&#x5F15;&#x96C6;<img src="img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" alt="">&#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; the diagonal to consider</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[-1.0813, -0.8619,  0.7105],
        [ 0.0935,  0.1380,  2.2112],
        [-0.3409, -0.9828,  0.0289]])
&gt;&gt;&gt; torch.tril(a)
tensor([[-1.0813,  0.0000,  0.0000],
        [ 0.0935,  0.1380,  0.0000],
        [-0.3409, -0.9828,  0.0289]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],
        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])
&gt;&gt;&gt; torch.tril(b, diagonal=1)
tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])
&gt;&gt;&gt; torch.tril(b, diagonal=-1)
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],
        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])
</code></pre><hr>
<pre><code>torch.tril_indices(row, col, offset=0, dtype=torch.long, device=&apos;cpu&apos;, layout=torch.strided) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE; 2&#xD7;N &#x5F20;&#x91CF;&#x4E2D;<code>row</code>-<code>col</code>&#x77E9;&#x9635;&#x7684;&#x4E0B;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x884C;&#x5305;&#x542B;&#x6240;&#x6709;&#x7D22;&#x5F15;&#x7684;&#x884C;&#x5750;&#x6807;&#xFF0C;&#x7B2C;&#x4E8C;&#x884C;&#x5305;&#x542B;&#x5217;&#x5750;&#x6807;&#x3002; &#x7D22;&#x5F15;&#x662F;&#x6839;&#x636E;&#x884C;&#x7136;&#x540E;&#x6309;&#x5217;&#x6392;&#x5E8F;&#x7684;&#x3002;</p>
<p>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</p>
<p>&#x53C2;&#x6570;<code>offset</code>&#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x5982;&#x679C;<code>offset</code> = 0&#xFF0C;&#x5219;&#x4FDD;&#x7559;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x548C;&#x4E0B;&#x65B9;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002; &#x6B63;&#x503C;&#x5305;&#x62EC;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x540C;&#x6837;&#xFF0C;&#x8D1F;&#x503C;&#x6392;&#x9664;&#x5728;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;<img src="img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" alt="">&#x7684;&#x7D22;&#x5F15;&#x96C6;<img src="img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" alt="">&#x662F;&#x77E9;&#x9635;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>&#x6CE8;&#x610F;&#xFF1A;&#x5728;&#x201C; cuda&#x201D;&#x4E0A;&#x8FD0;&#x884C;&#x65F6;&#xFF0C;&#x884C;* col &#x5FC5;&#x987B;&#x5C0F;&#x4E8E;<img src="img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" alt="">&#xFF0C;&#x4EE5;&#x9632;&#x6B62;&#x8BA1;&#x7B97;&#x671F;&#x95F4;&#x6EA2;&#x51FA;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x884C;</strong>(<code>int</code>&#xFF09;&#x2013;&#x4E8C;&#x7EF4;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x884C;&#x6570;&#x3002;</p>
</li>
<li><p><strong>col</strong> (<code>int</code>&#xFF09;&#x2013;&#x4E8C;&#x7EF4;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x5217;&#x6570;&#x3002;</p>
</li>
<li><p><strong>&#x504F;&#x79FB;&#x91CF;</strong>(<code>int</code>&#xFF09;&#x2013;&#x4E0E;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x504F;&#x79FB;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;&#x672A;&#x63D0;&#x4F9B;&#xFF0C;&#x5219;&#x4E3A; 0&#x3002;</p>
</li>
<li><p><strong>dtype</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x6240;&#x9700;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;&#x5982;&#x679C;<code>None</code>&#xFF0C;<code>torch.long</code>&#x3002;</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>&#x5E03;&#x5C40;</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a> &#xFF0C;&#x53EF;&#x9009;&#xFF09;&#x2013;&#x5F53;&#x524D;&#x4EC5;&#x652F;&#x6301;<code>torch.strided</code>&#x3002;</p>
</li>
</ul>
<pre><code>Example::
</code></pre><pre><code>&gt;&gt;&gt; a = torch.tril_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 1, 1, 2, 2, 2],
        [0, 0, 1, 0, 1, 2]])
</code></pre><pre><code>&gt;&gt;&gt; a = torch.tril_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[1, 2, 2, 3, 3, 3],
        [0, 0, 1, 0, 1, 2]])
</code></pre><pre><code>&gt;&gt;&gt; a = torch.tril_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])
</code></pre><hr>
<pre><code>torch.triu(input, diagonal=0, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x77E9;&#x9635;(2-D &#x5F20;&#x91CF;&#xFF09;&#x6216;&#x77E9;&#x9635;&#x6279;&#x6B21;<code>input</code>&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#xFF0C;&#x7ED3;&#x679C;&#x5F20;&#x91CF;<code>out</code>&#x7684;&#x5176;&#x4ED6;&#x5143;&#x7D20;&#x8BBE;&#x7F6E;&#x4E3A; 0&#x3002;</p>
<p>&#x77E9;&#x9635;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x5B9A;&#x4E49;&#x4E3A;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x548C;&#x4E0A;&#x65B9;&#x7684;&#x5143;&#x7D20;&#x3002;</p>
<p>&#x53C2;&#x6570; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> &#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x5982;&#x679C; <a href="#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a> = 0&#xFF0C;&#x5219;&#x4FDD;&#x7559;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x548C;&#x4E0A;&#x65B9;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002; &#x6B63;&#x503C;&#x6392;&#x9664;&#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x540C;&#x6837;&#xFF0C;&#x8D1F;&#x503C;&#x5305;&#x62EC;&#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;<img src="img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" alt="">&#x7684;&#x7D22;&#x5F15;&#x96C6;<img src="img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" alt="">&#x662F;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>diagonal</strong> (<em>python:int__,</em> <em>optional</em>) &#x2013; the diagonal to consider</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.3480, -0.5211, -0.4573]])
&gt;&gt;&gt; torch.triu(a)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.0000, -1.0680,  0.6602],
        [ 0.0000,  0.0000, -0.4573]])
&gt;&gt;&gt; torch.triu(a, diagonal=1)
tensor([[ 0.0000,  0.5207,  2.0049],
        [ 0.0000,  0.0000,  0.6602],
        [ 0.0000,  0.0000,  0.0000]])
&gt;&gt;&gt; torch.triu(a, diagonal=-1)
tensor([[ 0.2309,  0.5207,  2.0049],
        [ 0.2072, -1.0680,  0.6602],
        [ 0.0000, -0.5211, -0.4573]])

&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; b
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=1)
tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])
&gt;&gt;&gt; torch.triu(b, diagonal=-1)
tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],
        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
</code></pre><hr>
<pre><code>torch.triu_indices(row, col, offset=0, dtype=torch.long, device=&apos;cpu&apos;, layout=torch.strided) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE; 2&#xD7;N &#x5F20;&#x91CF;&#x4E2D;<code>row</code> x <code>col</code>&#x77E9;&#x9635;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x7684;&#x7D22;&#x5F15;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x884C;&#x5305;&#x542B;&#x6240;&#x6709;&#x7D22;&#x5F15;&#x7684;&#x884C;&#x5750;&#x6807;&#xFF0C;&#x7B2C;&#x4E8C;&#x884C;&#x5305;&#x542B;&#x5217;&#x5750;&#x6807;&#x3002; &#x7D22;&#x5F15;&#x662F;&#x6839;&#x636E;&#x884C;&#x7136;&#x540E;&#x6309;&#x5217;&#x6392;&#x5E8F;&#x7684;&#x3002;</p>
<p>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</p>
<p>&#x53C2;&#x6570;<code>offset</code>&#x63A7;&#x5236;&#x8981;&#x8003;&#x8651;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x5982;&#x679C;<code>offset</code> = 0&#xFF0C;&#x5219;&#x4FDD;&#x7559;&#x4E3B;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x548C;&#x4E0A;&#x65B9;&#x7684;&#x6240;&#x6709;&#x5143;&#x7D20;&#x3002; &#x6B63;&#x503C;&#x6392;&#x9664;&#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0A;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#xFF0C;&#x540C;&#x6837;&#xFF0C;&#x8D1F;&#x503C;&#x5305;&#x62EC;&#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x4E0B;&#x65B9;&#x7684;&#x5BF9;&#x89D2;&#x7EBF;&#x3002; &#x4E3B;&#x8981;&#x5BF9;&#x89D2;&#x7EBF;&#x662F;<img src="img/f1dac6cf1b08b41a3c94272830a3eaba.jpg" alt="">&#x7684;&#x7D22;&#x5F15;&#x96C6;<img src="img/efdb6cba2dff4626df6ecbd8c9117b62.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/a760ba0f4e82b4f6875dd8b8db1bd3c4.jpg" alt="">&#x662F;&#x77E9;&#x9635;&#x7684;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>NOTE: when running on &#x2018;cuda&#x2019;, row * col must be less than <img src="img/62e45bcf8fe738ca1f051a4f7e9a7845.jpg" alt=""> to prevent overflow during calculation.</p>
<p>Parameters</p>
<ul>
<li><p><strong>row</strong> (<code>int</code>) &#x2013; number of rows in the 2-D matrix.</p>
</li>
<li><p><strong>col</strong> (<code>int</code>) &#x2013; number of columns in the 2-D matrix.</p>
</li>
<li><p><strong>offset</strong> (<code>int</code>) &#x2013; diagonal offset from the main diagonal. Default: if not provided, 0.</p>
</li>
<li><p><strong>dtype</strong> (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) &#x2013; the desired data type of returned tensor. Default: if <code>None</code>, <code>torch.long</code>.</p>
</li>
<li><p><strong>device</strong> (<a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) &#x2013; the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</li>
<li><p><strong>layout</strong> (<a href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code>torch.layout</code></a>, optional) &#x2013; currently only support <code>torch.strided</code>.</p>
</li>
</ul>
<pre><code>Example::
</code></pre><pre><code>&gt;&gt;&gt; a = torch.triu_indices(3, 3)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 2],
        [0, 1, 2, 1, 2, 2]])
</code></pre><pre><code>&gt;&gt;&gt; a = torch.triu_indices(4, 3, -1)
&gt;&gt;&gt; a
tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],
        [0, 1, 2, 0, 1, 2, 1, 2, 2]])
</code></pre><pre><code>&gt;&gt;&gt; a = torch.triu_indices(4, 3, 1)
&gt;&gt;&gt; a
tensor([[0, 0, 1],
        [1, 2, 2]])
</code></pre><h3 id="blas-&#x548C;-lapack-&#x64CD;&#x4F5C;">BLAS &#x548C; LAPACK &#x64CD;&#x4F5C;</h3>
<hr>
<pre><code>torch.addbmm(beta=1, input, alpha=1, batch1, batch2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6267;&#x884C;&#x5B58;&#x50A8;&#x5728;<code>batch1</code>&#x548C;<code>batch2</code>&#x4E2D;&#x7684;&#x77E9;&#x9635;&#x7684;&#x6279;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#xFF0C;&#x5E76;&#x51CF;&#x5C11;&#x52A0;&#x6CD5;&#x6B65;&#x9AA4;(&#x6240;&#x6709;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x6CBF;&#x7B2C;&#x4E00;&#x7EF4;&#x7D2F;&#x79EF;&#xFF09;&#x3002; <code>input</code>&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x4E2D;&#x3002;</p>
<p><code>batch1</code>&#x548C;<code>batch2</code>&#x5FC5;&#x987B;&#x662F; 3D &#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;<code>batch1</code>&#x662F;<img src="img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>batch2</code>&#x662F;<img src="img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>input</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E26;&#x6709;<img src="img/80b2c8de6d028b93a22dfe571079ee9c.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x800C;<code>out</code>&#x5C06;&#x662F;<img src="img/80b2c8de6d028b93a22dfe571079ee9c.jpg" alt="">&#x5F20;&#x91CF;&#x3002; &#x3002;</p>
<p><img src="img/2c2652d014606e2ab1a85bdd3c257494.jpg" alt=""></p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53C2;&#x6570;<code>beta</code>&#x548C;<code>alpha</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> (<em>&#x6570;&#x5B57;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <code>input</code>(<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6DFB;&#x52A0;&#x7684;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &lt;cite&gt;batch1 @ batch2&lt;/cite&gt; (<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>batch1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E00;&#x6279;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>batch2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E8C;&#x6279;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)
tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
</code></pre><hr>
<pre><code>torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5BF9;&#x77E9;&#x9635;<code>mat1</code>&#x548C;<code>mat2</code>&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002; &#x77E9;&#x9635;<code>input</code>&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x4E2D;&#x3002;</p>
<p>&#x5982;&#x679C;<code>mat1</code>&#x662F;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>mat2</code>&#x662F;<img src="img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x90A3;&#x4E48;<code>input</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E26;&#x6709;<img src="img/80b2c8de6d028b93a22dfe571079ee9c.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x800C;<code>out</code>&#x5C06;&#x662F;<img src="img/80b2c8de6d028b93a22dfe571079ee9c.jpg" alt=""> &#x5F20;&#x91CF;&#x3002;</p>
<p><code>alpha</code>&#x548C;<code>beta</code>&#x5206;&#x522B;&#x662F;<code>mat1</code>&#x548C;<code>mat2</code>&#x4E0E;&#x6DFB;&#x52A0;&#x7684;&#x77E9;&#x9635;<code>input</code>&#x4E4B;&#x95F4;&#x7684;&#x77E9;&#x9635;&#x5411;&#x91CF;&#x4E58;&#x79EF;&#x7684;&#x6BD4;&#x4F8B;&#x56E0;&#x5B50;&#x3002;</p>
<p><img src="img/9acb7c0c73f7a2456fcd58168e607d32.jpg" alt=""></p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53C2;&#x6570;<code>beta</code>&#x548C;<code>alpha</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) &#x2013; multiplier for <code>input</code> (<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt="">)</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; matrix to be added</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/6a8a2e3415f1a12e8299e3d32d94728d.jpg" alt="">(<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>mat1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)
&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
        [ 0.7573, -3.9555, -2.8681]])
</code></pre><hr>
<pre><code>torch.addmv(beta=1, input, alpha=1, mat, vec, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>mat</code>&#x4E0E;&#x5411;&#x91CF;<code>vec</code>&#x7684;&#x77E9;&#x9635;&#x5411;&#x91CF;&#x79EF;&#x3002; &#x5411;&#x91CF;<code>input</code>&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x4E2D;&#x3002;</p>
<p>&#x5982;&#x679C;<code>mat</code>&#x662F;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>vec</code>&#x662F;&#x5927;&#x5C0F; &lt;cite&gt;m&lt;/cite&gt; &#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>input</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;</a>&#xFF0C;&#x4E14;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x4E3A; &lt;cite&gt;n&lt;/cite&gt; &#x548C;<code>out</code>&#x5927;&#x5C0F;&#x5C06;&#x662F; &lt;cite&gt;n&lt;/cite&gt; &#x5927;&#x5C0F;&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#x3002;</p>
<p><code>alpha</code>&#x548C;<code>beta</code>&#x5206;&#x522B;&#x662F;<code>mat</code>&#x548C;<code>vec</code>&#x4E0E;&#x6DFB;&#x52A0;&#x7684;&#x5F20;&#x91CF;<code>input</code>&#x4E4B;&#x95F4;&#x7684;&#x77E9;&#x9635;&#x5411;&#x91CF;&#x4E58;&#x79EF;&#x7684;&#x6BD4;&#x4F8B;&#x56E0;&#x5B50;&#x3002;</p>
<p><img src="img/c267729def81e95d252835fd4b3c0885.jpg" alt=""></p>
<p>&#x5BF9;&#x4E8E;&#x7C7B;&#x578B;&#x4E3A; &lt;cite&gt;FloatTensor&lt;/cite&gt; &#x6216; &lt;cite&gt;DoubleTensor&lt;/cite&gt; &#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53C2;&#x6570;<code>beta</code>&#x548C;<code>alpha</code>&#x5FC5;&#x987B;&#x4E3A;&#x5B9E;&#x6570;&#xFF0C;&#x5426;&#x5219;&#x5E94;&#x4E3A;&#x6574;&#x6570;</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) &#x2013; multiplier for <code>input</code> (<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt="">)</p>
</li>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x6DFB;&#x52A0;&#x7684;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/7906de3cf6f1147a41fa843b63151dce.jpg" alt="">(<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>&#x57AB;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>vec</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(2)
&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.addmv(M, mat, vec)
tensor([-0.3768, -5.5565])
</code></pre><hr>
<pre><code>torch.addr(beta=1, input, alpha=1, vec1, vec2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6267;&#x884C;&#x5411;&#x91CF;<code>vec1</code>&#x548C;<code>vec2</code>&#x7684;&#x5916;&#x79EF;&#x5E76;&#x5C06;&#x5176;&#x6DFB;&#x52A0;&#x5230;&#x77E9;&#x9635;<code>input</code>&#x4E2D;&#x3002;</p>
<p>&#x53EF;&#x9009;&#x503C;<code>beta</code>&#x548C;<code>alpha</code>&#x5206;&#x522B;&#x662F;<code>vec1</code>&#x548C;<code>vec2</code>&#x4E0E;&#x6DFB;&#x52A0;&#x77E9;&#x9635;<code>input</code>&#x4E4B;&#x95F4;&#x7684;&#x5916;&#x90E8;&#x4E58;&#x79EF;&#x7684;&#x6BD4;&#x4F8B;&#x56E0;&#x5B50;&#x3002;</p>
<p><img src="img/d41c12ed61e56a7f779894121105461d.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>vec1</code>&#x662F; &lt;cite&gt;n&lt;/cite&gt; &#x5927;&#x5C0F;&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x800C;<code>vec2</code>&#x662F; &lt;cite&gt;m&lt;/cite&gt; &#x5927;&#x5C0F;&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5219;<code>input</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;</a>&#x4E14;&#x77E9;&#x9635;&#x4E3A; <img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x548C;<code>out</code>&#x5927;&#x5C0F;&#x5C06;&#x662F;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x5927;&#x5C0F;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) &#x2013; multiplier for <code>input</code> (<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt="">)</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; matrix to be added</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/8b4088eb8d72b6ef9fe78d97a3149b5b.jpg" alt="">(<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>vec1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5916;&#x79EF;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5916;&#x79EF;&#x7684;&#x7B2C;&#x4E8C;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; vec1 = torch.arange(1., 4.)
&gt;&gt;&gt; vec2 = torch.arange(1., 3.)
&gt;&gt;&gt; M = torch.zeros(3, 2)
&gt;&gt;&gt; torch.addr(M, vec1, vec2)
tensor([[ 1.,  2.],
        [ 2.,  4.],
        [ 3.,  6.]])
</code></pre><hr>
<pre><code>torch.baddbmm(beta=1, input, alpha=1, batch1, batch2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5728;<code>batch1</code>&#x548C;<code>batch2</code>&#x4E2D;&#x6267;&#x884C;&#x77E9;&#x9635;&#x7684;&#x6279;&#x5904;&#x7406;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002; <code>input</code>&#x88AB;&#x6DFB;&#x52A0;&#x5230;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x4E2D;&#x3002;</p>
<p><code>batch1</code>&#x548C;<code>batch2</code>&#x5FC5;&#x987B;&#x662F; 3D &#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;<code>batch1</code>&#x662F;<img src="img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>batch2</code>&#x662F;<img src="img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x90A3;&#x4E48;<code>input</code>&#x5FC5;&#x987B;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x53EF;&#x5E7F;&#x64AD;&#x7684;</a>&#xFF0C;&#x5E26;&#x6709;<img src="img/f827b00456c04e393fa94ba4df1c7e08.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x800C;<code>out</code>&#x5C06;&#x662F;<img src="img/f827b00456c04e393fa94ba4df1c7e08.jpg" alt=""> &#x5F20;&#x91CF;&#x3002; <code>alpha</code>&#x548C;<code>beta</code>&#x7684;&#x542B;&#x4E49;&#x5747;&#x4E0E; <a href="#torch.addbmm" title="torch.addbmm"><code>torch.addbmm()</code></a> &#x4E2D;&#x4F7F;&#x7528;&#x7684;&#x7F29;&#x653E;&#x56E0;&#x5B50;&#x76F8;&#x540C;&#x3002;</p>
<p><img src="img/63aed732a8b14d4e5804e77ade6d9340.jpg" alt=""></p>
<p>For inputs of type &lt;cite&gt;FloatTensor&lt;/cite&gt; or &lt;cite&gt;DoubleTensor&lt;/cite&gt;, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers.</p>
<p>Parameters</p>
<ul>
<li><p><strong>beta</strong> (<em>Number__,</em> <em>optional</em>) &#x2013; multiplier for <code>input</code> (<img src="img/53a496ec7d546e2af9595a7055dd6a7e.jpg" alt="">)</p>
</li>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the tensor to be added</p>
</li>
<li><p><strong>alpha</strong> (<em>&#x7F16;&#x53F7;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/7b5e33d60c908d236e66d3aba3044f4f.jpg" alt="">(<img src="img/5b9866fb35b01c553ed3e738e3972ae9.jpg" alt="">&#xFF09;&#x7684;&#x4E58;&#x6570;</p>
</li>
<li><p><strong>batch1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the first batch of matrices to be multiplied</p>
</li>
<li><p><strong>batch2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second batch of matrices to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)
&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
</code></pre><hr>
<pre><code>torch.bmm(input, mat2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5BF9;<code>input</code>&#x548C;<code>mat2</code>&#x4E2D;&#x5B58;&#x50A8;&#x7684;&#x77E9;&#x9635;&#x6267;&#x884C;&#x6279;&#x5904;&#x7406;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
<p><code>input</code>&#x548C;<code>mat2</code>&#x5FC5;&#x987B;&#x662F; 3D &#x5F20;&#x91CF;&#xFF0C;&#x6BCF;&#x4E2A;&#x5F20;&#x91CF;&#x5305;&#x542B;&#x76F8;&#x540C;&#x6570;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x662F;<img src="img/6bcc61944ebb068b264d492e9d3a9e9c.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>mat2</code>&#x662F;<img src="img/30e4202e8cde7f489d43fdfbf59e0c46.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>out</code>&#x5C06;&#x662F;<img src="img/f827b00456c04e393fa94ba4df1c7e08.jpg" alt="">&#x5F20;&#x91CF;&#x3002;</p>
<p><img src="img/582f347178ea996d728f2eb71d21ae1d.jpg" alt=""></p>
<p>Note</p>
<p>&#x8BE5;&#x529F;&#x80FD;&#x4E0D;<a href="notes/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;</a>&#x3002; &#x6709;&#x5173;&#x5E7F;&#x64AD;&#x77E9;&#x9635;&#x4EA7;&#x54C1;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x6279;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>mat2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7B2C;&#x4E8C;&#x6279;&#x77E9;&#x9635;&#x76F8;&#x4E58;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(10, 3, 4)
&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; res = torch.bmm(input, mat2)
&gt;&gt;&gt; res.size()
torch.Size([10, 3, 5])
</code></pre><hr>
<pre><code>torch.chain_matmul(*matrices)&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt=""> 2-D &#x5F20;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002; &#x4F7F;&#x7528;&#x77E9;&#x9635;&#x94FE;&#x987A;&#x5E8F;&#x7B97;&#x6CD5;&#x53EF;&#x4EE5;&#x6709;&#x6548;&#x5730;&#x8BA1;&#x7B97;&#x8BE5;&#x4E58;&#x79EF;&#xFF0C;&#x8BE5;&#x7B97;&#x6CD5;&#x9009;&#x62E9;&#x4EE5;&#x7B97;&#x672F;&#x8FD0;&#x7B97; (<a href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition" target="_blank">[CLRS]</a>)&#x4EA7;&#x751F;&#x6700;&#x4F4E;&#x6210;&#x672C;&#x7684;&#x987A;&#x5E8F;&#x3002; &#x6CE8;&#x610F;&#xFF0C;&#x7531;&#x4E8E;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x8BA1;&#x7B97;&#x4E58;&#x79EF;&#x7684;&#x51FD;&#x6570;&#xFF0C;&#x56E0;&#x6B64;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5FC5;&#x987B;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; 2&#xFF1B;&#x56E0;&#x6B64;&#xFF0C;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x5FC5;&#x987B;&#x5927;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; 2&#x3002; &#x5982;&#x679C;&#x7B49;&#x4E8E; 2&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x5E73;&#x51E1;&#x7684;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002; &#x5982;&#x679C;<img src="img/7ea0d6aa290d37c8bc57957206bafe80.jpg" alt="">&#x4E3A; 1&#xFF0C;&#x5219;&#x4E3A;&#x7A7A;&#x64CD;&#x4F5C;-&#x539F;&#x59CB;&#x77E9;&#x9635;&#x6309;&#x539F;&#x6837;&#x8FD4;&#x56DE;&#x3002;</p>
<p>Parameters</p>
<p><strong>&#x77E9;&#x9635;</strong>(<em>&#x5F20;&#x91CF;...</em> )&#x2013;&#x7531; 2 &#x4E2A;&#x6216;&#x591A;&#x4E2A; 2D &#x5F20;&#x91CF;&#x786E;&#x5B9A;&#x5176;&#x4E58;&#x79EF;&#x7684;&#x5E8F;&#x5217;&#x3002;</p>
<p>Returns</p>
<p>&#x5982;&#x679C;<img src="img/10b8c0924577b4e47a4e8b6537a6ac9a.jpg" alt="">&#x5F20;&#x91CF;&#x7684;&#x5C3A;&#x5BF8;&#x4E3A;<img src="img/9342f0f16c0cf8e65f9e7baf672bc083.jpg" alt="">&#xFF0C;&#x5219;&#x4E58;&#x79EF;&#x5C06;&#x4E3A;&#x5C3A;&#x5BF8;<img src="img/169badb20d1f38fab1c101fc7c5c674f.jpg" alt="">&#x3002;</p>
<p>Return type</p>
<p><a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 4)
&gt;&gt;&gt; b = torch.randn(4, 5)
&gt;&gt;&gt; c = torch.randn(5, 6)
&gt;&gt;&gt; d = torch.randn(6, 7)
&gt;&gt;&gt; torch.chain_matmul(a, b, c, d)
tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])
</code></pre><hr>
<pre><code>torch.cholesky(input, upper=False, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x6216;&#x4E00;&#x6279;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#x7684; Cholesky &#x5206;&#x89E3;&#x3002;</p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;<code>U</code>&#x4E3A;&#x4E0A;&#x4E09;&#x89D2;&#xFF0C;&#x5206;&#x89E3;&#x5F62;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p><img src="img/917cbfcd41dc365c78ee1bb4651d5a3a.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;<code>L</code>&#x4E3A;&#x4E0B;&#x4E09;&#x89D2;&#xFF0C;&#x5206;&#x89E3;&#x5F62;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p><img src="img/3f3a6cdf9c276b261c15da651aff430e.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5E76;&#x4E14;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x4E3A;&#x4E00;&#x6279;&#x5BF9;&#x79F0;&#x7684;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x7531;&#x5404;&#x4E2A;&#x77E9;&#x9635;&#x7684;&#x4E0A;&#x4E09;&#x89D2; Cholesky &#x56E0;&#x5B50;&#x7EC4;&#x6210;&#x3002; &#x540C;&#x6837;&#xFF0C;&#x5F53;<code>upper</code>&#x4E3A;<code>False</code>&#x65F6;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x5C06;&#x7531;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x77E9;&#x9635;&#x7684;&#x4E0B;&#x4E09;&#x89D2; Cholesky &#x56E0;&#x5B50;&#x7EC4;&#x6210;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/aa6a866e7977a9ee67a53687003d3821.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;*&lt;/cite&gt; &#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x591A;&#x4E2A;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#xFF0C;&#x5305;&#x62EC; &#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;&#x3002;</p>
</li>
<li><p><strong>&#x4E0A;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6307;&#x793A;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x7684;&#x6807;&#x5FD7;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
<li><p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x77E9;&#x9635;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; l
tensor([[ 1.5528,  0.0000,  0.0000],
        [-0.4821,  1.0592,  0.0000],
        [ 0.9371,  0.5487,  0.7023]])
&gt;&gt;&gt; torch.mm(l, l.t())
tensor([[ 2.4112, -0.7486,  1.4551],
        [-0.7486,  1.3544,  0.1294],
        [ 1.4551,  0.1294,  1.6724]])
&gt;&gt;&gt; a = torch.randn(3, 2, 2)
&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite
&gt;&gt;&gt; l = torch.cholesky(a)
&gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2))
&gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)
</code></pre><hr>
<pre><code>torch.cholesky_inverse(input, upper=False, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4F7F;&#x7528;&#x5176; Cholesky &#x56E0;&#x5B50;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#x8BA1;&#x7B97;&#x5BF9;&#x79F0;&#x6B63;&#x5B9A;&#x77E9;&#x9635;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x7684;&#x9006;&#xFF1A;&#x8FD4;&#x56DE;&#x77E9;&#x9635;<code>inv</code>&#x3002; &#x4F7F;&#x7528; LAPACK &#x4F8B;&#x7A0B;<code>dpotri</code>&#x548C;<code>spotri</code>(&#x4EE5;&#x53CA;&#x76F8;&#x5E94;&#x7684; MAGMA &#x4F8B;&#x7A0B;&#xFF09;&#x8BA1;&#x7B97;&#x9006;&#x3002;</p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#x4E3A;&#x4E0B;&#x4E09;&#x89D2;&#xFF0C;&#x8FD9;&#x6837;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E3A;</p>
<p><img src="img/68e4a9138b1537d38f0e11634c0f068b.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>True</code>&#xFF0C;&#x6216;&#x672A;&#x63D0;&#x4F9B;&#xFF0C;&#x5219;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#x4E3A;&#x4E0A;&#x4E09;&#x89D2;&#xFF0C;&#x4F7F;&#x5F97;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x4E3A;</p>
<p><img src="img/c4e7afdf52ea3475cb80b4d812b87a16.jpg" alt=""></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#xFF0C;&#x4E0A;&#x6216;&#x4E0B;&#x4E09;&#x89D2; Cholesky &#x56E0;&#x5B50;</p>
</li>
<li><p><strong>&#x4E0A;&#x90E8;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x5426;&#x8FD4;&#x56DE;&#x4E0B;&#x90E8;(&#x9ED8;&#x8BA4;&#xFF09;&#x6216;&#x4E0A;&#x90E8;&#x4E09;&#x89D2;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>out</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &lt;cite&gt;inv&lt;/cite&gt; &#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[  0.9935,  -0.6353,   1.5806],
        [ -0.6353,   0.8769,  -1.7183],
        [  1.5806,  -1.7183,  10.6618]])
&gt;&gt;&gt; torch.cholesky_inverse(u)
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
&gt;&gt;&gt; a.inverse()
tensor([[ 1.9314,  1.2251, -0.0889],
        [ 1.2251,  2.4439,  0.2122],
        [-0.0889,  0.2122,  0.1412]])
</code></pre><hr>
<pre><code>torch.cholesky_solve(input, input2, upper=False, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x7ED9;&#x5B9A;&#x5176; Cholesky &#x56E0;&#x5B50;&#x77E9;&#x9635;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#xFF0C;&#x4EE5;&#x6B63;&#x534A;&#x5B9A;&#x77E9;&#x9635;&#x89E3;&#x7EBF;&#x6027;&#x65B9;&#x7A0B;&#x7EC4;&#x3002;</p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#x4E3A;&#x4E14;&#x4E0B;&#x90E8;&#x4E09;&#x89D2;&#x5F62;&#xFF0C;&#x5E76;&#x4E14;&#x8FD4;&#x56DE; &lt;cite&gt;c&lt;/cite&gt; &#x4F7F;&#x5F97;&#xFF1A;</p>
<p><img src="img/fbdb7ce9ef686ee96dd7242ca863ee3c.jpg" alt=""></p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x4E0D;&#x63D0;&#x4F9B;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#x4E3A;&#x4E0A;&#x4E09;&#x89D2;&#x5F62;&#xFF0C;&#x5E76;&#x4E14;&#x8FD4;&#x56DE; &lt;cite&gt;c&lt;/cite&gt; &#xFF0C;&#x4F7F;&#x5F97;&#xFF1A;</p>
<p><img src="img/b63d89883b7ac400b9b862e2f05738f7.jpg" alt=""></p>
<p>&lt;cite&gt;torch.cholesky_solve(b&#xFF0C;u&#xFF09;&lt;/cite&gt;&#x53EF;&#x4EE5;&#x63A5;&#x53D7; 2D &#x8F93;&#x5165; &lt;cite&gt;b&#xFF0C;u&lt;/cite&gt; &#x6216;&#x4E00;&#x6279; 2D &#x77E9;&#x9635;&#x7684;&#x8F93;&#x5165;&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x4E3A;&#x6279;&#x6B21;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6210;&#x6279;&#x8F93;&#x51FA; &lt;cite&gt;c&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/8ebb9d35b764041761ddb4e3436e265d.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x77E9;&#x9635;<img src="img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/10ea424983a008dbf72dc86628a2813d.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x77E9;&#x9635;<img src="img/83ebacfa515af65e0d7e683595edbde1.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x7531;&#x4E0A;&#x6216;&#x4E0B;&#x4E09;&#x89D2;&#x7EC4;&#x6210;&#x7684;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8; &#x80C6;&#x56FA;&#x9187;&#x7CFB;&#x6570;</p>
</li>
<li><p><strong>&#x4E0A;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x5426;&#x8003;&#x8651;&#x5C06; Cholesky &#x56E0;&#x5B50;&#x89C6;&#x4E3A;&#x4E0B;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x8FD8;&#x662F;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &lt;cite&gt;c&lt;/cite&gt; &#x7684;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite
&gt;&gt;&gt; u = torch.cholesky(a)
&gt;&gt;&gt; a
tensor([[ 0.7747, -1.9549,  1.3086],
        [-1.9549,  6.7546, -5.4114],
        [ 1.3086, -5.4114,  4.8733]])
&gt;&gt;&gt; b = torch.randn(3, 2)
&gt;&gt;&gt; b
tensor([[-0.6355,  0.9891],
        [ 0.1974,  1.4706],
        [-0.4115, -0.6225]])
&gt;&gt;&gt; torch.cholesky_solve(b, u)
tensor([[ -8.1625,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
&gt;&gt;&gt; torch.mm(a.inverse(), b)
tensor([[ -8.1626,  19.6097],
        [ -5.8398,  14.2387],
        [ -4.3771,  10.4173]])
</code></pre><hr>
<pre><code>torch.dot(input, tensor) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x70B9;&#x79EF;(&#x5185;&#x79EF;&#xFF09;&#x3002;</p>
<p>Note</p>
<p>&#x8BE5;&#x529F;&#x80FD;&#x4E0D;<a href="notes/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;</a>&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))
tensor(7)
</code></pre><hr>
<pre><code>torch.eig(input, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5B9E;&#x65B9;&#x77E9;&#x9635;&#x7684;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x7531;&#x4E8E;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x53EF;&#x80FD;&#x5F88;&#x590D;&#x6742;&#xFF0C;&#x56E0;&#x6B64;&#x4EC5; <a href="#torch.symeig" title="torch.symeig"><code>torch.symeig()</code></a> &#x652F;&#x6301;&#x53CD;&#x5411;&#x4F20;&#x9012;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5F62;&#x72B6;&#x4E3A;<img src="img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" alt="">&#x7684;&#x65B9;&#x9635;&#xFF0C;&#x5C06;&#x4E3A;&#x5176;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>&#x7279;&#x5F81;&#x5411;&#x91CF;</strong> (<em>bool</em> )&#x2013; <code>True</code>&#x4EE5;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF1B; &#x5426;&#x5219;&#xFF0C;&#x5C06;&#x4EC5;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x503C;</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8F93;&#x51FA;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x7684; namedtuple(&#x7279;&#x5F81;&#x503C;&#xFF0C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF09;</p>
<blockquote>
<ul>
<li><strong>&#x7279;&#x5F81;&#x503C;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x5F62;&#x72B6;<img src="img/f274e719751329e2ef63ca92533b23da.jpg" alt="">&#x3002; &#x6BCF;&#x884C;&#x662F;<code>input</code>&#x7684;&#x7279;&#x5F81;&#x503C;&#xFF0C;&#x5176;&#x4E2D;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x5B9E;&#x90E8;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x865A;&#x90E8;&#x3002; &#x7279;&#x5F81;&#x503C;&#x4E0D;&#x4E00;&#x5B9A;&#x662F;&#x6709;&#x5E8F;&#x7684;&#x3002;</li>
</ul>
<ul>
<li><strong>&#x7279;&#x5F81;&#x5411;&#x91CF;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x5982;&#x679C;<code>eigenvectors=False</code>&#x4E3A;&#x7A7A;&#xFF0C;&#x5219;&#x4E3A;&#x5F20;&#x91CF;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x5F62;&#x72B6;<img src="img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" alt="">&#x7684;&#x5F20;&#x91CF;&#x6765;&#x8BA1;&#x7B97;&#x5BF9;&#x5E94;&#x7279;&#x5F81;&#x503C;&#x7684;&#x5F52;&#x4E00;&#x5316;(&#x5355;&#x4F4D;&#x957F;&#x5EA6;&#xFF09;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF0C;&#x5982;&#x4E0B;&#x6240;&#x793A;&#x3002; &#x5982;&#x679C;&#x5BF9;&#x5E94;&#x7684;&lt;cite&gt;&#x7279;&#x5F81;&#x503C;[j]&lt;/cite&gt; &#x662F;&#x5B9E;&#x6570;&#xFF0C;&#x5219;&lt;cite&gt;&#x7279;&#x5F81;&#x5411;&#x91CF;[&#xFF1A;&#xFF0C;j]&lt;/cite&gt; &#x5217;&#x662F;&#x4E0E;&lt;cite&gt;&#x7279;&#x5F81;&#x503C;[j]&lt;/cite&gt; &#x76F8;&#x5BF9;&#x5E94;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002; &#x5982;&#x679C;&#x76F8;&#x5E94;&#x7684;&lt;cite&gt;&#x7279;&#x5F81;&#x503C;[j]&lt;/cite&gt; &#x548C;&lt;cite&gt;&#x7279;&#x5F81;&#x503C;[j + 1]&lt;/cite&gt; &#x5F62;&#x6210;&#x590D;&#x5171;&#x8F6D;&#x5BF9;&#xFF0C;&#x5219;&#x771F;&#x5B9E;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x53EF;&#x4EE5;&#x8BA1;&#x7B97;&#x4E3A;<img src="img/cf872428156c87dba5ec860dd64ff692.jpg" alt="">&#xFF0C;<img src="img/3a205492a67d1307528fb311bfba7d2a.jpg" alt="">&#x3002;</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF09;</p>
<hr>
<pre><code>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x8FD9;&#x662F;&#x76F4;&#x63A5;&#x8C03;&#x7528; LAPACK &#x7684;&#x5E95;&#x5C42;&#x51FD;&#x6570;&#x3002; &#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;[eqg0f] &#x7684; <a href="https://software.intel.com/en-us/node/521004" target="_blank">LAPACK &#x6587;&#x6863;&#x4E2D;&#x5B9A;&#x4E49;&#x7684; namedtuple(a&#xFF0C;tau&#xFF09;&#x3002;</a></p>
<p>&#x901A;&#x5E38;&#xFF0C;&#x60A8;&#x901A;&#x5E38;&#x8981;&#x4F7F;&#x7528; <a href="#torch.qr" title="torch.qr"><code>torch.qr()</code></a> &#x3002;</p>
<p>&#x8BA1;&#x7B97;<code>input</code>&#x7684; QR &#x5206;&#x89E3;&#xFF0C;&#x4F46;&#x4E0D;&#x5C06;<img src="img/7f927fea3856ca4796aab74326229f61.jpg" alt="">&#x548C;<img src="img/fd6855baddb0a56aeca293dd58a9758d.jpg" alt="">&#x6784;&#x9020;&#x4E3A;&#x660E;&#x786E;&#x7684;&#x5355;&#x72EC;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x800C;&#x662F;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x57FA;&#x7840;&#x7684; LAPACK &#x51FD;&#x6570;&lt;cite&gt;&#xFF1F;geqrf&lt;/cite&gt; &#xFF0C;&#x8BE5;&#x51FD;&#x6570;&#x4EA7;&#x751F;&#x4E00;&#x7CFB;&#x5217;&#x201C;&#x57FA;&#x672C;&#x53CD;&#x5C04;&#x5668;&#x201D;&#x3002;</p>
<p>&#x6709;&#x5173;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; geqrf &#x7684; <a href="https://software.intel.com/en-us/node/521004" target="_blank">LAPACK &#x6587;&#x6863;&#x3002;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;(&#x5F20;&#x91CF;&#xFF0C;&#x5F20;&#x91CF;&#xFF09;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;</p>
</li>
</ul>
<hr>
<pre><code>torch.ger(input, vec2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p><code>input</code>&#x548C;<code>vec2</code>&#x7684;&#x5916;&#x90E8;&#x4E58;&#x79EF;&#x3002; &#x5982;&#x679C;<code>input</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x800C;<code>vec2</code>&#x662F;&#x5927;&#x5C0F;&#x4E3A;<img src="img/03327e7b697db30918e96b2209927929.jpg" alt="">&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5219;<code>out</code>&#x5FC5;&#x987B;&#x662F;&#x5927;&#x5C0F;&#x4E3A;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x7684;&#x77E9;&#x9635;&#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x4E00;&#x7EF4;&#x8F93;&#x5165;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>vec2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x4E00;&#x7EF4;&#x8F93;&#x5165;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x9009;&#x8F93;&#x51FA;&#x77E9;&#x9635;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; v1 = torch.arange(1., 5.)
&gt;&gt;&gt; v2 = torch.arange(1., 4.)
&gt;&gt;&gt; torch.ger(v1, v2)
tensor([[  1.,   2.,   3.],
        [  2.,   4.,   6.],
        [  3.,   6.,   9.],
        [  4.,   8.,  12.]])
</code></pre><hr>
<pre><code>torch.inverse(input, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x53D6;&#x65B9;&#x9635;<code>input</code>&#x7684;&#x9006;&#x3002; <code>input</code>&#x53EF;&#x4EE5;&#x662F; 2D &#x65B9;&#x5F62;&#x5F20;&#x91CF;&#x7684;&#x6279;&#x5904;&#x7406;&#xFF0C;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6B64;&#x51FD;&#x6570;&#x5C06;&#x8FD4;&#x56DE;&#x7531;&#x5404;&#x4E2A;&#x9006;&#x7EC4;&#x6210;&#x7684;&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>&#x65E0;&#x8BBA;&#x539F;&#x59CB;&#x6B65;&#x5E45;&#x5982;&#x4F55;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x90FD;&#x5C06;&#x88AB;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x4F7F;&#x7528; &lt;cite&gt;input.contiguous(&#xFF09;&#x3002;transpose(-2&#xFF0C;-1&#xFF09;.stride(&#xFF09;&lt;/cite&gt;&#x4E4B;&#x7C7B;&#x7684;&#x6B65;&#x5E45;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/aa6a866e7977a9ee67a53687003d3821.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;*&lt;/cite&gt; &#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; x = torch.rand(4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z
tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000, -0.0000, -0.0000,  1.0000]])
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero
tensor(1.1921e-07)
&gt;&gt;&gt; # Batched inverse example
&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)
&gt;&gt;&gt; y = torch.inverse(x)
&gt;&gt;&gt; z = torch.matmul(x, y)
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero
tensor(1.9073e-06)
</code></pre><hr>
<pre><code>torch.det(input) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x6216;&#x6279;&#x6B21;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x7684;&#x884C;&#x5217;&#x5F0F;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>input</code>&#x4E0D;&#x53EF;&#x9006;&#x65F6;&#xFF0C;&#x5411;&#x540E;&#x901A;&#x8FC7; <a href="#torch.det" title="torch.det"><code>det()</code></a> &#x5185;&#x90E8;&#x4F7F;&#x7528; SVD &#x7ED3;&#x679C;&#x3002; &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x6CA1;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5947;&#x5F02;&#x503C;&#xFF0C;&#x5219;&#x901A;&#x8FC7; <a href="#torch.det" title="torch.det"><code>det()</code></a> &#x5411;&#x540E;&#x7FFB;&#x500D;&#x5C06;&#x4E0D;&#x7A33;&#x5B9A;&#x3002; &#x6709;&#x5173;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> &#x3002;</p>
<p>Parameters</p>
<p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<code>(*, n, n)</code>&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<code>*</code>&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x91CF;&#x5C3A;&#x5BF8;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(3.7641)

&gt;&gt;&gt; A = torch.randn(3, 2, 2)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])
</code></pre><hr>
<pre><code>torch.logdet(input) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x6216;&#x6279;&#x6B21;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x7684;&#x5BF9;&#x6570;&#x884C;&#x5217;&#x5F0F;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x5BF9;&#x6570;&#x884C;&#x5217;&#x5F0F;&#x4E3A; 0&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x4E3A;<code>-inf</code>&#xFF1B;&#x5982;&#x679C;<code>input</code>&#x7684;&#x884C;&#x5217;&#x5F0F;&#x4E3A;&#x8D1F;&#x6570;&#xFF0C;&#x5219;&#x7ED3;&#x679C;&#x4E3A;<code>nan</code>&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>input</code>&#x4E0D;&#x53EF;&#x9006;&#x65F6;&#xFF0C;&#x5411;&#x540E;&#x901A;&#x8FC7; <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> &#x5185;&#x90E8;&#x4F7F;&#x7528; SVD &#x7ED3;&#x679C;&#x3002; &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x6CA1;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5947;&#x5F02;&#x503C;&#xFF0C;&#x5219;&#x901A;&#x8FC7; <a href="#torch.logdet" title="torch.logdet"><code>logdet()</code></a> &#x5411;&#x540E;&#x7FFB;&#x500D;&#x5C06;&#x4E0D;&#x7A33;&#x5B9A;&#x3002; &#x6709;&#x5173;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> &#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; torch.det(A)
tensor(0.2611)
&gt;&gt;&gt; torch.logdet(A)
tensor(-1.3430)
&gt;&gt;&gt; A
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; A.det()
tensor([1.1990, 0.4099, 0.7386])
&gt;&gt;&gt; A.det().log()
tensor([ 0.1815, -0.8917, -0.3031])
</code></pre><hr>
<pre><code>torch.slogdet(input) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x6216;&#x4E00;&#x6279;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x7684;&#x884C;&#x5217;&#x5F0F;&#x7684;&#x6B63;&#x8D1F;&#x53F7;&#x548C;&#x5BF9;&#x6570;&#x7EDD;&#x5BF9;&#x503C;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x884C;&#x5217;&#x5F0F;&#x4E3A;&#x96F6;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;<code>(0, -inf)</code>&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>input</code>&#x4E0D;&#x53EF;&#x9006;&#x65F6;&#xFF0C;&#x5411;&#x540E;&#x901A;&#x8FC7; <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> &#x5185;&#x90E8;&#x4F7F;&#x7528; SVD &#x7ED3;&#x679C;&#x3002; &#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x6CA1;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5947;&#x5F02;&#x503C;&#xFF0C;&#x5219;&#x901A;&#x8FC7; <a href="#torch.slogdet" title="torch.slogdet"><code>slogdet()</code></a> &#x5411;&#x540E;&#x7FFB;&#x500D;&#x5C06;&#x4E0D;&#x7A33;&#x5B9A;&#x3002; &#x6709;&#x5173;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> &#x3002;</p>
<p>Parameters</p>
<p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor of size <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions.</p>
<p>Returns</p>
<p>&#x5305;&#x542B;&#x884C;&#x5217;&#x5F0F;&#x7684;&#x7B26;&#x53F7;&#x548C;&#x7EDD;&#x5BF9;&#x884C;&#x5217;&#x5F0F;&#x7684;&#x5BF9;&#x6570;&#x503C;&#x7684; namedtuple(&#x7B26;&#x53F7;&#xFF0C;logabsdet&#xFF09;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; A
tensor([[ 0.0032, -0.2239, -1.1219],
        [-0.6690,  0.1161,  0.4053],
        [-1.6218, -0.9273, -0.0082]])
&gt;&gt;&gt; torch.det(A)
tensor(-0.7576)
&gt;&gt;&gt; torch.logdet(A)
tensor(nan)
&gt;&gt;&gt; torch.slogdet(A)
torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))
</code></pre><hr>
<pre><code>torch.lstsq(input, A, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x5927;&#x5C0F;&#x4E3A;<img src="img/88bf1d4bce0ef2568c0d7c879f26ce08.jpg" alt="">&#x7684;&#x6EE1;&#x79E9;&#x77E9;&#x9635;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x548C;&#x5927;&#x5C0F;&#x4E3A;<img src="img/811a1fadb1118d551ad2e0f7515b3ab2.jpg" alt="">&#x7684;&#x77E9;&#x9635;<img src="img/041cf842180f19a622690f37ed9f70d2.jpg" alt="">&#x7684;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x548C;&#x6700;&#x5C0F;&#x8303;&#x6570;&#x95EE;&#x9898;&#x7684;&#x89E3;&#x3002;</p>
<p>&#x5982;&#x679C;<img src="img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" alt="">&#xFF0C; <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> &#x89E3;&#x51B3;&#x4E86;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x95EE;&#x9898;&#xFF1A;</p>
<p><img src="img/9d1018603f258178ba5a9db4739d49ff.jpg" alt=""></p>
<p>&#x5982;&#x679C;<img src="img/30ee994fb1857c1f8d6540a60056fe45.jpg" alt="">&#xFF0C; <a href="#torch.lstsq" title="torch.lstsq"><code>lstsq()</code></a> &#x89E3;&#x51B3;&#x4E86;&#x6700;&#x5C0F;&#x8303;&#x6570;&#x95EE;&#x9898;&#xFF1A;</p>
<p><img src="img/cb7af0f918cae78e16fd68223243e29c.jpg" alt=""></p>
<p>&#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;<img src="img/1dc567019b272fda0c5051c472dac2b7.jpg" alt="">&#x5177;&#x6709;&#x5F62;&#x72B6;<img src="img/2e7f8c37553bcb25b33aa412a4d78219.jpg" alt="">&#x3002; <img src="img/1dc567019b272fda0c5051c472dac2b7.jpg" alt="">&#x7684;&#x524D;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x884C;&#x5305;&#x542B;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x3002; &#x5982;&#x679C;&#x4E3A;<img src="img/7cbd71a05144eeca3bb1f22a22c30c3a.jpg" alt="">&#xFF0C;&#x5219;&#x6BCF;&#x5217;&#x4E2D;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x7684;&#x5269;&#x4F59;&#x5E73;&#x65B9;&#x548C;&#x7531;&#x8BE5;&#x5217;&#x5176;&#x4F59;<img src="img/6d15ee60528ade686ac8933bcac404a4.jpg" alt="">&#x884C;&#x4E2D;&#x5143;&#x7D20;&#x7684;&#x5E73;&#x65B9;&#x548C;&#x5F97;&#x51FA;&#x3002;</p>
<p>Note</p>
<p>GPU &#x4E0D;&#x652F;&#x6301;<img src="img/30ee994fb1857c1f8d6540a60056fe45.jpg" alt="">&#x7684;&#x60C5;&#x51B5;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x77E9;&#x9635;<img src="img/041cf842180f19a622690f37ed9f70d2.jpg" alt=""></p>
</li>
<li><p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x7531;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x77E9;&#x9635;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x6784;&#x6210;&#x7684;<img src="img/03327e7b697db30918e96b2209927929.jpg" alt=""></p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x9009;&#x76EE;&#x6807;&#x5F20;&#x91CF;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;(&#x89E3;&#x51B3;&#x65B9;&#x6848;&#xFF0C;QR&#xFF09;&#xFF0C;&#x5176;&#x4E2D;&#x5305;&#x542B;&#xFF1A;</p>
<blockquote>
<ul>
<li><strong>&#x89E3;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x89E3;</li>
</ul>
<ul>
<li><strong>QR</strong>  (<em>Tensor</em> )&#xFF1A;QR &#x56E0;&#x5F0F;&#x5206;&#x89E3;&#x7684;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Note</p>
<p>&#x65E0;&#x8BBA;&#x8F93;&#x5165;&#x77E9;&#x9635;&#x7684;&#x8DE8;&#x5EA6;&#x5982;&#x4F55;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;&#x5C06;&#x59CB;&#x7EC8;&#x8FDB;&#x884C;&#x8F6C;&#x7F6E;&#x3002; &#x5373;&#xFF0C;&#x4ED6;&#x4EEC;&#x5C06;&#x5177;&#x6709;&lt;cite&gt;(1&#xFF0C;m&#xFF09;&lt;/cite&gt;&#x800C;&#x4E0D;&#x662F;&lt;cite&gt;(m&#xFF0C;1&#xFF09;&lt;/cite&gt;&#x7684;&#x6B65;&#x5E45;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[1., 1, 1],
                      [2, 3, 4],
                      [3, 5, 2],
                      [4, 2, 5],
                      [5, 4, 3]])
&gt;&gt;&gt; B = torch.tensor([[-10., -3],
                      [ 12, 14],
                      [ 14, 12],
                      [ 16, 16],
                      [ 18, 16]])
&gt;&gt;&gt; X, _ = torch.lstsq(B, A)
&gt;&gt;&gt; X
tensor([[  2.0000,   1.0000],
        [  1.0000,   1.0000],
        [  1.0000,   2.0000],
        [ 10.9635,   4.8501],
        [  8.9332,   5.2418]])
</code></pre><hr>
<pre><code>torch.lu(A, pivot=True, get_infos=False, out=None)&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x6216;&#x77E9;&#x9635;&#x6279;&#x6B21;&#x7684; LU &#x5206;&#x89E3;<code>A</code>&#x3002; &#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5305;&#x542B; LU &#x5206;&#x89E3;&#x548C;<code>A</code>&#x7684;&#x67A2;&#x8F74;&#x7684;&#x5143;&#x7EC4;&#x3002; &#x5982;&#x679C;<code>pivot</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x5B8C;&#x6210;&#x65CB;&#x8F6C;&#x3002;</p>
<p>Note</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7684;&#x67A2;&#x8F74;&#x4E3A; 1 &#x7D22;&#x5F15;&#x3002; &#x5982;&#x679C;<code>pivot</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684;&#x67A2;&#x8F74;&#x662F;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x586B;&#x5145;&#x6709;&#x9002;&#x5F53;&#x5927;&#x5C0F;&#x7684;&#x96F6;&#x3002;</p>
<p>Note</p>
<p><code>pivot</code> = <code>False</code>&#x7684; LU &#x5206;&#x89E3;&#x4E0D;&#x9002;&#x7528;&#x4E8E; CPU&#xFF0C;&#x5C1D;&#x8BD5;&#x8FD9;&#x6837;&#x505A;&#x4F1A;&#x5F15;&#x53D1;&#x9519;&#x8BEF;&#x3002; &#x4F46;&#x662F;&#xFF0C;CUDA &#x53EF;&#x4F7F;&#x7528;<code>pivot</code> = <code>False</code>&#x7684; LU &#x5206;&#x89E3;&#x3002;</p>
<p>Note</p>
<p>&#x8BE5;&#x51FD;&#x6570;&#x4E0D;&#x4F1A;&#x68C0;&#x67E5;&#x5206;&#x89E3;&#x662F;&#x5426;&#x6210;&#x529F;&#xFF0C;&#x56E0;&#x4E3A;<code>get_infos</code>&#x4E3A;<code>True</code>&#xFF0C;&#x56E0;&#x4E3A;&#x8FD4;&#x56DE;&#x5143;&#x7EC4;&#x7684;&#x7B2C;&#x4E09;&#x4E2A;&#x5143;&#x7D20;&#x4E2D;&#x5B58;&#x5728;&#x5206;&#x89E3;&#x7684;&#x72B6;&#x6001;&#x3002;</p>
<p>Note</p>
<p>&#x5728; CUDA &#x8BBE;&#x5907;&#x4E0A;&#x6279;&#x91CF;&#x5904;&#x7406;&#x5927;&#x5C0F;&#x5C0F;&#x4E8E;&#x6216;&#x7B49;&#x4E8E; 32 &#x7684;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x7531;&#x4E8E; MAGMA &#x5E93;&#x4E2D;&#x7684;&#x9519;&#x8BEF;&#xFF0C;&#x5BF9;&#x5947;&#x5F02;&#x77E9;&#x9635;&#x91CD;&#x590D;&#x8FDB;&#x884C; LU &#x56E0;&#x5F0F;&#x5206;&#x89E3;(&#x8BF7;&#x53C2;&#x89C1;&#x5CA9;&#x6D46;&#x95EE;&#x9898; 13&#xFF09;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x7684;&#x5F20;&#x91CF;<img src="img/6217ff594f4af540b27d0cc551ab742c.jpg" alt=""></p>
</li>
<li><p><strong>&#x67A2;&#x8F74;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x5B8C;&#x6210;&#x67A2;&#x8F74;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code></p>
</li>
<li><p><strong>get_infos</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5982;&#x679C;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4FE1;&#x606F; IntTensor&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong>(<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x9009;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x3002; &#x5982;&#x679C;<code>get_infos</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x4E3A; Tensor&#xFF0C;IntTensor &#x548C; IntTensor&#x3002; &#x5982;&#x679C;<code>get_infos</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x5143;&#x7EC4;&#x4E2D;&#x7684;&#x5143;&#x7D20;&#x4E3A; Tensor&#xFF0C;IntTensor&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code></p>
</li>
</ul>
<p>Returns</p>
<p>&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#x5305;&#x542B;</p>
<blockquote>
<ul>
<li><strong>&#x5206;&#x89E3;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x5927;&#x5C0F;<img src="img/6217ff594f4af540b27d0cc551ab742c.jpg" alt="">&#x7684;&#x5206;&#x89E3;</li>
</ul>
<ul>
<li><strong>&#x67A2;&#x8F74;</strong> (<em>IntTensor</em> )&#xFF1A;&#x5927;&#x5C0F;&#x4E3A;<img src="img/74dd7138867888df79f198ead03a8f07.jpg" alt="">&#x7684;&#x67A2;&#x8F74;</li>
</ul>
<ul>
<li><strong>&#x4FE1;&#x606F;</strong> (<em>IntTensor</em> &#xFF0C;<em>&#x53EF;&#x9009;</em>&#xFF09;&#xFF1A;&#x5982;&#x679C;<code>get_infos</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x6B64;&#x5F20;&#x91CF;&#x4E3A;<img src="img/39f05b69c0ea6b9b6b228f948944c4f6.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;&#x975E;&#x96F6;&#x503C;&#x8868;&#x793A;&#x662F;&#x5426; &#x77E9;&#x9635;&#x5206;&#x89E3;&#x6216;&#x6BCF;&#x4E2A;&#x5C0F;&#x6279;&#x91CF;&#x6210;&#x529F;&#x6216;&#x5931;&#x8D25;</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">&#x5F20;&#x91CF;</a>&#xFF0C;IntTensor&#xFF0C;IntTensor(&#x53EF;&#x9009;&#xFF09;&#xFF09;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
&gt;&gt;&gt; A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
&gt;&gt;&gt; pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
&gt;&gt;&gt; if info.nonzero().size(0) == 0:
...   print(&apos;LU factorization succeeded for all samples!&apos;)
LU factorization succeeded for all samples!
</code></pre><hr>
<pre><code>torch.lu_solve(input, LU_data, LU_pivots, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4F7F;&#x7528; <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> &#x4E2D; A &#x7684;&#x90E8;&#x5206;&#x67A2;&#x8F74; LU &#x5206;&#x89E3;&#xFF0C;&#x8FD4;&#x56DE;&#x7EBF;&#x6027;&#x7CFB;&#x7EDF;<img src="img/328fdc7aa24f647110fc0900733a006f.jpg" alt="">&#x7684; LU &#x89E3;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>b</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5C3A;&#x5BF8;&#x4E3A;<img src="img/8ebb9d35b764041761ddb4e3436e265d.jpg" alt="">&#x7684; RHS &#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x91CF;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013; A &#x4ECE; <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> &#x5927;&#x5C0F;&#x4E3A;<img src="img/10ea424983a008dbf72dc86628a2813d.jpg" alt="">&#x7684; A &#x7684;&#x900F;&#x89C6; LU &#x5206;&#x89E3;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A; &#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>LU_pivots</strong>  (<em>IntTensor</em> )&#x2013; LU &#x5206;&#x89E3;&#x7684;&#x67A2;&#x8F74;&#x6765;&#x81EA; <a href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> &#xFF0C;&#x5927;&#x5C0F;&#x4E3A;<img src="img/74dd7138867888df79f198ead03a8f07.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x751F;&#x4EA7;&#x5C3A;&#x5BF8;&#x3002; <code>LU_pivots</code>&#x7684;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#x5FC5;&#x987B;&#x7B49;&#x4E8E;<code>LU_data</code>&#x7684;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(2, 3, 1)
&gt;&gt;&gt; A_LU = torch.lu(A)
&gt;&gt;&gt; x = torch.lu_solve(b, *A_LU)
&gt;&gt;&gt; torch.norm(torch.bmm(A, x) - b)
tensor(1.00000e-07 *
       2.8312)
</code></pre><hr>
<pre><code>torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)&#xB6;
</code></pre><p>&#x89E3;&#x538B;&#x7F29;&#x6570;&#x636E;&#x5E76;&#x4ECE;&#x5F20;&#x91CF;&#x7684; LU &#x5206;&#x89E3;&#x4E2D;&#x67A2;&#x8F6C;&#x3002;</p>
<p>&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4;&#x4E3A;<code>(the pivots, the L tensor, the U tensor)</code>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>LU_data</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6253;&#x5305; LU &#x5206;&#x89E3;&#x6570;&#x636E;</p>
</li>
<li><p><strong>LU_pivots</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x538B;&#x7F29; LU &#x5206;&#x89E3;&#x67A2;&#x8F74;</p>
</li>
<li><p><strong>unpack_data</strong>  (<em>bool</em> )&#x2013;&#x6307;&#x793A;&#x662F;&#x5426;&#x5E94;&#x62C6;&#x5305;&#x6570;&#x636E;&#x7684;&#x6807;&#x5FD7;</p>
</li>
<li><p><strong>unpack_pivots</strong>  (<em>bool</em> )&#x2013;&#x6307;&#x793A;&#x662F;&#x5426;&#x5E94;&#x62C6;&#x5F00;&#x67A2;&#x8F74;&#x7684;&#x6807;&#x5FD7;</p>
</li>
</ul>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = A.lu()
&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
&gt;&gt;&gt;
&gt;&gt;&gt; # can recover A from factorization
&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))

&gt;&gt;&gt; # LU factorization of a rectangular matrix:
&gt;&gt;&gt; A = torch.randn(2, 3, 2)
&gt;&gt;&gt; A_LU, pivots = A.lu()
&gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
&gt;&gt;&gt; P
tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]],

        [[0., 0., 1.],
         [0., 1., 0.],
         [1., 0., 0.]]])
&gt;&gt;&gt; A_L
tensor([[[ 1.0000,  0.0000],
         [ 0.4763,  1.0000],
         [ 0.3683,  0.1135]],

        [[ 1.0000,  0.0000],
         [ 0.2957,  1.0000],
         [-0.9668, -0.3335]]])
&gt;&gt;&gt; A_U
tensor([[[ 2.1962,  1.0881],
         [ 0.0000, -0.8681]],

        [[-1.0947,  0.3736],
         [ 0.0000,  0.5718]]])
&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))
&gt;&gt;&gt; torch.norm(A_ - A)
tensor(2.9802e-08)
</code></pre><hr>
<pre><code>torch.matmul(input, other, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x7684;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
<p>&#x884C;&#x4E3A;&#x53D6;&#x51B3;&#x4E8E;&#x5F20;&#x91CF;&#x7684;&#x7EF4;&#x6570;&#xFF0C;&#x5982;&#x4E0B;&#x6240;&#x793A;&#xFF1A;</p>
<ul>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x5F20;&#x91CF;&#x90FD;&#x662F;&#x4E00;&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x70B9;&#x79EF;(&#x6807;&#x91CF;&#xFF09;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#x90FD;&#x662F;&#x4E8C;&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x77E9;&#x9635;&#x77E9;&#x9635;&#x4E58;&#x79EF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x81EA;&#x53D8;&#x91CF;&#x662F;&#x4E00;&#x7EF4;&#x7684;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x81EA;&#x53D8;&#x91CF;&#x662F;&#x4E8C;&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x4E3A;&#x4E86;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#xFF0C;&#x4F1A;&#x5C06; 1 &#x9644;&#x52A0;&#x5230;&#x5176;&#x7EF4;&#x4E0A;&#x3002; &#x77E9;&#x9635;&#x76F8;&#x4E58;&#x540E;&#xFF0C;&#x5C06;&#x5220;&#x9664;&#x524D;&#x7F6E;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x4E3A; 2 &#x7EF4;&#xFF0C;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x4E3A; 1 &#x7EF4;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x77E9;&#x9635;&#x5411;&#x91CF;&#x4E58;&#x79EF;&#x3002;</p>
</li>
<li><p>&#x5982;&#x679C;&#x4E24;&#x4E2A;&#x81EA;&#x53D8;&#x91CF;&#x81F3;&#x5C11;&#x4E3A;&#x4E00;&#x7EF4;&#x4E14;&#x81F3;&#x5C11;&#x4E00;&#x4E2A;&#x81EA;&#x53D8;&#x91CF;&#x4E3A; N &#x7EF4;(&#x5176;&#x4E2D; N &gt; 2&#xFF09;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6279;&#x5904;&#x7406;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002; &#x5982;&#x679C;&#x7B2C;&#x4E00;&#x4E2A;&#x81EA;&#x53D8;&#x91CF;&#x662F;&#x4E00;&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x5C06; 1 &#x9644;&#x52A0;&#x5230;&#x5176;&#x7EF4;&#x7684;&#x524D;&#x9762;&#xFF0C;&#x4EE5;&#x5B9E;&#x73B0;&#x6279;&#x91CF;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x5E76;&#x5728;&#x4E4B;&#x540E;&#x5C06;&#x5176;&#x5220;&#x9664;&#x3002; &#x5982;&#x679C;&#x7B2C;&#x4E8C;&#x4E2A;&#x53C2;&#x6570;&#x662F;&#x4E00;&#x7EF4;&#x7684;&#xFF0C;&#x5219;&#x5C06; 1 &#x9644;&#x52A0;&#x5230;&#x5176;&#x7EF4;&#x4E0A;&#xFF0C;&#x4EE5;&#x5B9E;&#x73B0;&#x6210;&#x6279;&#x77E9;&#x9635;&#x500D;&#x6570;&#x7684;&#x76EE;&#x7684;&#xFF0C;&#x7136;&#x540E;&#x5C06;&#x5176;&#x5220;&#x9664;&#x3002; &#x975E;&#x77E9;&#x9635;(&#x5373;&#x6279;&#x5904;&#x7406;&#xFF09;&#x5C3A;&#x5BF8;&#x662F;<a href="notes/broadcasting.html#broadcasting-semantics">&#x5E7F;&#x64AD;&#x7684;</a>(&#x56E0;&#x6B64;&#x5FC5;&#x987B;&#x662F;&#x53EF;&#x5E7F;&#x64AD;&#x7684;&#xFF09;&#x3002; &#x4F8B;&#x5982;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x662F;<img src="img/48e81addd1b15307d101249324d93372.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>other</code>&#x662F;<img src="img/561dfed20b3d8014c77c118b8904ec66.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>out</code>&#x5C06;&#x662F;<img src="img/cb41125b3953740f3a7f50204cd20d7f.jpg" alt="">&#x5F20;&#x91CF;&#x3002;</p>
</li>
</ul>
<p>Note</p>
<p>&#x6B64;&#x529F;&#x80FD;&#x7684;&#x4E00;&#x7EF4;&#x70B9;&#x79EF;&#x4EA7;&#x54C1;&#x7248;&#x672C;&#x4E0D;&#x652F;&#x6301;<code>out</code>&#x53C2;&#x6570;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>&#x5176;&#x4ED6;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E8C;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; # vector x vector
&gt;&gt;&gt; tensor1 = torch.randn(3)
&gt;&gt;&gt; tensor2 = torch.randn(3)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([])
&gt;&gt;&gt; # matrix x vector
&gt;&gt;&gt; tensor1 = torch.randn(3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([3])
&gt;&gt;&gt; # batched matrix x broadcasted vector
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
&gt;&gt;&gt; # batched matrix x batched matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
&gt;&gt;&gt; # batched matrix x broadcasted matrix
&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)
&gt;&gt;&gt; tensor2 = torch.randn(4, 5)
&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
</code></pre><hr>
<pre><code>torch.matrix_power(input, n) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5E73;&#x65B9;&#x77E9;&#x9635;&#x4E58;&#x5E42;<code>n</code>&#x7684;&#x77E9;&#x9635;&#x3002; &#x5BF9;&#x4E8E;&#x4E00;&#x6279;&#x77E9;&#x9635;&#xFF0C;&#x5C06;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x77E9;&#x9635;&#x63D0;&#x9AD8;&#x5230;&#x5E42;<code>n</code>&#x3002;</p>
<p>&#x5982;&#x679C;<code>n</code>&#x4E3A;&#x8D1F;&#xFF0C;&#x5219;&#x77E9;&#x9635;&#x7684;&#x9006;(&#x5982;&#x679C;&#x662F;&#x53EF;&#x9006;&#x7684;&#xFF09;&#x63D0;&#x9AD8;&#x5230;&#x5E42;<code>n</code>&#x3002; &#x5BF9;&#x4E8E;&#x4E00;&#x6279;&#x77E9;&#x9635;&#xFF0C;&#x5C06;&#x6210;&#x6279;&#x7684;&#x9006;(&#x5982;&#x679C;&#x53EF;&#x9006;&#xFF09;&#x63D0;&#x9AD8;&#x5230;&#x5E42;<code>n</code>&#x3002; &#x5982;&#x679C;<code>n</code>&#x4E3A; 0&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x5355;&#x4F4D;&#x77E9;&#x9635;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the input tensor.</p>
</li>
<li><p><strong>n</strong>  (<em>python&#xFF1A;int</em> )&#x2013;&#x5C06;&#x77E9;&#x9635;&#x63D0;&#x5347;&#x4E3A;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(2, 2, 2)
&gt;&gt;&gt; a
tensor([[[-1.9975, -1.9610],
         [ 0.9592, -2.3364]],

        [[-1.2534, -1.3429],
         [ 0.4153, -1.4664]]])
&gt;&gt;&gt; torch.matrix_power(a, 3)
tensor([[[  3.9392, -23.9916],
         [ 11.7357,  -0.2070]],

        [[  0.2468,  -6.7168],
         [  2.0774,  -0.8187]]])
</code></pre><hr>
<pre><code>torch.matrix_rank(input, tol=None, symmetric=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;&#x7684;&#x6570;&#x503C;&#x7B49;&#x7EA7;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528; SVD &#x5B8C;&#x6210;&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x7B49;&#x7EA7;&#x7684;&#x65B9;&#x6CD5;&#x3002; &#x5982;&#x679C;<code>symmetric</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x5047;&#x5B9A;<code>input</code>&#x662F;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x901A;&#x8FC7;&#x83B7;&#x5F97;&#x7279;&#x5F81;&#x503C;&#x6765;&#x5B8C;&#x6210;&#x79E9;&#x7684;&#x8BA1;&#x7B97;&#x3002;</p>
<p><code>tol</code>&#x662F;&#x9608;&#x503C;&#xFF0C;&#x4F4E;&#x4E8E;&#x8BE5;&#x9608;&#x503C;&#x7684;&#x5947;&#x5F02;&#x503C;(&#x6216;&#x5F53;<code>symmetric</code>&#x4E3A;<code>True</code>&#x65F6;&#x7684;&#x7279;&#x5F81;&#x503C;&#xFF09;&#x88AB;&#x89C6;&#x4E3A; 0&#x3002;&#x5982;&#x679C;&#x672A;&#x6307;&#x5B9A;<code>tol</code>&#xFF0C;&#x5219;<code>tol</code>&#x8BBE;&#x7F6E;&#x4E3A;<code>S.max() * max(S.size()) * eps</code>&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;S&lt;/cite&gt; &#x662F;&#x5947;&#x5F02;&#x503C;(&#x6216;<code>symmetric</code>&#x662F;<code>True</code>&#x65F6;&#x7684;&#x7279;&#x5F81;&#x503C;&#xFF09;&#xFF0C;<code>eps</code>&#x662F;<code>input</code>&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684; epsilon &#x503C;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8F93;&#x5165;&#x4E8C;&#x7EF4;&#x5F20;&#x91CF;</p>
</li>
<li><p><strong>tol</strong>  (<em>python&#xFF1A;float</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x516C;&#x5DEE;&#x503C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>None</code></p>
</li>
<li><p><strong>&#x5BF9;&#x79F0;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x6307;&#x793A;<code>input</code>&#x662F;&#x5426;&#x5BF9;&#x79F0;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code></p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.eye(10)
&gt;&gt;&gt; torch.matrix_rank(a)
tensor(10)
&gt;&gt;&gt; b = torch.eye(10)
&gt;&gt;&gt; b[0, 0] = 0
&gt;&gt;&gt; torch.matrix_rank(b)
tensor(9)
</code></pre><hr>
<pre><code>torch.mm(input, mat2, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5BF9;&#x77E9;&#x9635;<code>input</code>&#x548C;<code>mat2</code>&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x662F;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>mat2</code>&#x662F;<img src="img/d145985cd9c2b23f68a55b0d5429c2ac.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>out</code>&#x5C06;&#x662F;<img src="img/80b2c8de6d028b93a22dfe571079ee9c.jpg" alt="">&#x5F20;&#x91CF;&#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>. For broadcasting matrix products, see <a href="#torch.matmul" title="torch.matmul"><code>torch.matmul()</code></a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x7B2C;&#x4E00;&#x4E2A;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>mat2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the second matrix to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)
&gt;&gt;&gt; mat2 = torch.randn(3, 3)
&gt;&gt;&gt; torch.mm(mat1, mat2)
tensor([[ 0.4851,  0.5037, -0.3633],
        [-0.0760, -3.6705,  2.4784]])
</code></pre><hr>
<pre><code>torch.mv(input, vec, out=None) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6267;&#x884C;&#x77E9;&#x9635;<code>input</code>&#x4E0E;&#x5411;&#x91CF;<code>vec</code>&#x7684;&#x77E9;&#x9635;&#x5411;&#x91CF;&#x79EF;&#x3002;</p>
<p>&#x5982;&#x679C;<code>input</code>&#x662F;<img src="img/95760d62046dcfa418c3b7ffea4caefc.jpg" alt="">&#x5F20;&#x91CF;&#xFF0C;<code>vec</code>&#x662F;&#x5927;&#x5C0F;<img src="img/03327e7b697db30918e96b2209927929.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x5F20;&#x91CF;&#xFF0C;&#x5219;<code>out</code>&#x5C06;&#x662F;&#x5927;&#x5C0F;<img src="img/5f0051b0454fa75ca446b59b47eff6f6.jpg" alt="">&#x7684;&#x4E00;&#x7EF4;&#x3002;</p>
<p>Note</p>
<p>This function does not <a href="notes/broadcasting.html#broadcasting-semantics">broadcast</a>.</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;</p>
</li>
<li><p><strong>vec</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; vector to be multiplied</p>
</li>
<li><p><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) &#x2013; the output tensor.</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)
&gt;&gt;&gt; vec = torch.randn(3)
&gt;&gt;&gt; torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])
</code></pre><hr>
<pre><code>torch.orgqr(input, input2) &#x2192; Tensor&#xB6;
</code></pre><p>&#x6839;&#x636E; <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> &#x8FD4;&#x56DE;&#x7684;&lt;cite&gt;(&#x8F93;&#x5165;&#xFF0C;input2&#xFF09;&lt;/cite&gt;&#x5143;&#x7EC4;&#xFF0C;&#x8BA1;&#x7B97; QR &#x5206;&#x89E3;&#x7684;&#x6B63;&#x4EA4;&#x77E9;&#x9635; &lt;cite&gt;Q&lt;/cite&gt; &#x3002;</p>
<p>&#x8FD9;&#x5C06;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x57FA;&#x7840;&#x7684; LAPACK &#x51FD;&#x6570;&lt;cite&gt;&#xFF1F;orgqr&lt;/cite&gt; &#x3002; &#x6709;&#x5173;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; orgqr &#x7684; <a href="https://software.intel.com/en-us/mkl-developer-reference-c-orgqr" target="_blank">LAPACK &#x6587;&#x6863;&#x3002;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6765;&#x81EA; <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> &#x7684; &lt;cite&gt;a&lt;/cite&gt; &#x3002;</p>
</li>
<li><p><strong>input2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x6765;&#x81EA; <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> &#x7684; &lt;cite&gt;tau&lt;/cite&gt; &#x3002;</p>
</li>
</ul>
<hr>
<pre><code>torch.ormqr(input, input2, input3, left=True, transpose=False) &#x2192; Tensor&#xB6;
</code></pre><p>&#x5C06;&lt;cite&gt;&#x57AB;&lt;/cite&gt;(&#x7531;<code>input3</code>&#x8D4B;&#x4E88;&#xFF09;&#x4E58;&#x4EE5; <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a> &#x8868;&#x793A;&#x7684; QR &#x56E0;&#x5F0F;&#x5206;&#x89E3;&#x7684;&#x6B63;&#x4EA4; &lt;cite&gt;Q&lt;/cite&gt; &#x77E9;&#x9635;&#xFF0C;&#x8BE5;&#x77E9;&#x9635;&#x7531;&lt;cite&gt;(a (tau&#xFF09;&lt;/cite&gt;(&#x7531;[<code>input</code>&#xFF0C;<code>input2</code>&#x7ED9;&#x4E88;&#xFF09;&#xFF09;&#x3002;</p>
<p>&#x8FD9;&#x5C06;&#x76F4;&#x63A5;&#x8C03;&#x7528;&#x57FA;&#x7840;&#x7684; LAPACK &#x51FD;&#x6570;&lt;cite&gt;&#xFF1F;ormqr&lt;/cite&gt; &#x3002; &#x6709;&#x5173;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; ormqr &#x7684; <a href="https://software.intel.com/en-us/mkl-developer-reference-c-ormqr" target="_blank">LAPACK &#x6587;&#x6863;&#x3002;</a></p>
<p>Parameters</p>
<ul>
<li><p><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the &lt;cite&gt;a&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li><p><strong>input2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; the &lt;cite&gt;tau&lt;/cite&gt; from <a href="#torch.geqrf" title="torch.geqrf"><code>torch.geqrf()</code></a>.</p>
</li>
<li><p><strong>input3</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x8981;&#x76F8;&#x4E58;&#x7684;&#x77E9;&#x9635;&#x3002;</p>
</li>
</ul>
<hr>
<pre><code>torch.pinverse(input, rcond=1e-15) &#x2192; Tensor&#xB6;
</code></pre><p>&#x8BA1;&#x7B97; 2D &#x5F20;&#x91CF;&#x7684;&#x4F2A;&#x9006;(&#x4E5F;&#x79F0;&#x4E3A; Moore-Penrose &#x9006;&#xFF09;&#x3002; &#x8BF7;&#x67E5;&#x770B; <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank">Moore-Penrose &#x9006;</a>&#x4E86;&#x89E3;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;</p>
<p>Note</p>
<p>&#x4F7F;&#x7528;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#x5B9E;&#x73B0;&#x6B64;&#x65B9;&#x6CD5;&#x3002;</p>
<p>Note</p>
<p>&#x5728;&#x77E9;&#x9635; <a href="https://epubs.siam.org/doi/10.1137/0117004" target="_blank">[1]</a> &#x7684;&#x5143;&#x7D20;&#x4E2D;&#xFF0C;&#x4F2A;&#x9006;&#x4E0D;&#x4E00;&#x5B9A;&#x662F;&#x8FDE;&#x7EED;&#x51FD;&#x6570;&#x3002; &#x56E0;&#x6B64;&#xFF0C;&#x5BFC;&#x6570;&#x5E76;&#x4E0D;&#x603B;&#x662F;&#x5B58;&#x5728;&#xFF0C;&#x5E76;&#x4E14;&#x4EC5;&#x4EE5;&#x6052;&#x5B9A;&#x7B49;&#x7EA7;&#x5B58;&#x5728; <a href="https://www.jstor.org/stable/2156365" target="_blank">[2]</a> &#x3002; &#x4F46;&#x662F;&#xFF0C;&#x7531;&#x4E8E;&#x4F7F;&#x7528; SVD &#x7ED3;&#x679C;&#x5B9E;&#x73B0;&#xFF0C;&#x56E0;&#x6B64;&#x8BE5;&#x65B9;&#x6CD5;&#x53EF;&#x5411;&#x540E;&#x4F20;&#x64AD;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x80FD;&#x4E0D;&#x7A33;&#x5B9A;&#x3002; &#x7531;&#x4E8E;&#x5185;&#x90E8;&#x4F7F;&#x7528; SVD&#xFF0C;&#x56E0;&#x6B64;&#x53CC;&#x5411;&#x540E;&#x9000;&#x4E5F;&#x4F1A;&#x53D8;&#x5F97;&#x4E0D;&#x7A33;&#x5B9A;&#x3002; &#x6709;&#x5173;&#x66F4;&#x591A;&#x8BE6;&#x7EC6;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1; <a href="#torch.svd" title="torch.svd"><code>svd()</code></a> &#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/6217ff594f4af540b27d0cc551ab742c.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>rcond</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x4E00;&#x4E2A;&#x6D6E;&#x70B9;&#x503C;&#xFF0C;&#x7528;&#x4E8E;&#x786E;&#x5B9A;&#x5C0F;&#x7684;&#x5947;&#x5F02;&#x503C;&#x7684;&#x622A;&#x6B62;&#x503C;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;1e-15</p>
</li>
</ul>
<p>Returns</p>
<p>&#x5C3A;&#x5BF8;&#x4E3A;<img src="img/decace38e1e20419be05da9c4eac4b78.jpg" alt="">&#x7684;<code>input</code>&#x7684;&#x4F2A;&#x9006;&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; input = torch.randn(3, 5)
&gt;&gt;&gt; input
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
&gt;&gt;&gt; torch.pinverse(input)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])
&gt;&gt;&gt; # Batched pinverse example
&gt;&gt;&gt; a = torch.randn(2,6,3)
&gt;&gt;&gt; b = torch.pinverse(a)
&gt;&gt;&gt; torch.matmul(b, a)
tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],
        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],
        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],

        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],
        [-2.2352e-07,  1.0000e+00,  1.1921e-07],
        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])
</code></pre><hr>
<pre><code>torch.qr(input, some=True, out=None) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x8BA1;&#x7B97;&#x77E9;&#x9635;&#x6216;&#x4E00;&#x6279;&#x77E9;&#x9635;<code>input</code>&#x7684; QR &#x5206;&#x89E3;&#xFF0C;&#x5E76;&#x8FD4;&#x56DE;&#x5F20;&#x91CF;&#x7684;&#x547D;&#x540D;&#x5143;&#x7EC4;(Q&#xFF0C;R&#xFF09;&#xFF0C;&#x4F7F;&#x5F97;<img src="img/41222a3dad86eb0798f47e735d4eb15b.jpg" alt="">&#x5176;&#x4E2D;<img src="img/7f927fea3856ca4796aab74326229f61.jpg" alt="">&#x662F;&#x6B63;&#x4EA4;&#x77E9;&#x9635;&#x6216;&#x4E00;&#x6279;&#x6B63;&#x4EA4;&#x77E9;&#x9635;&#xFF0C;&#x800C;<img src="img/fd6855baddb0a56aeca293dd58a9758d.jpg" alt="">&#x662F; &#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x6216;&#x4E00;&#x6279;&#x4E0A;&#x4E09;&#x89D2;&#x77E9;&#x9635;&#x3002;</p>
<p>&#x5982;&#x679C;<code>some</code>&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x7626;(&#x200B;&#x200B;&#x7CBE;&#x7B80;&#xFF09;QR &#x56E0;&#x5F0F;&#x5206;&#x89E3;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x5982;&#x679C;<code>some</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5B8C;&#x6574;&#x7684; QR &#x56E0;&#x5F0F;&#x5206;&#x89E3;&#x3002;</p>
<p>Note</p>
<p>&#x5982;&#x679C;<code>input</code>&#x7684;&#x5143;&#x7D20;&#x7684;&#x5E45;&#x5EA6;&#x8F83;&#x5927;&#xFF0C;&#x5219;&#x53EF;&#x80FD;&#x4F1A;&#x5931;&#x53BB;&#x7CBE;&#x5EA6;</p>
<p>Note</p>
<p>&#x5C3D;&#x7BA1;&#x5B83;&#x59CB;&#x7EC8;&#x53EF;&#x4EE5;&#x4E3A;&#x60A8;&#x63D0;&#x4F9B;&#x6709;&#x6548;&#x7684;&#x5206;&#x89E3;&#xFF0C;&#x4F46;&#x5728;&#x5404;&#x4E2A;&#x5E73;&#x53F0;&#x4E0A;&#x53EF;&#x80FD;&#x4E0D;&#x4F1A;&#x7ED9;&#x60A8;&#x76F8;&#x540C;&#x7684;&#x5206;&#x89E3;-&#x8FD9;&#x53D6;&#x51B3;&#x4E8E;&#x60A8;&#x7684; LAPACK &#x5B9E;&#x73B0;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/6217ff594f4af540b27d0cc551ab742c.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;*&lt;/cite&gt; &#x4E3A;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#xFF0C;&#x5305;&#x62EC;&#x5C3A;&#x5BF8;&#x77E9;&#x9635; <img src="img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" alt="">&#x3002;</p>
</li>
<li><p><strong>&#x4E00;&#x4E9B;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x8BBE;&#x7F6E;&#x4E3A;<code>True</code>&#x53EF;&#x51CF;&#x5C11; QR &#x5206;&#x89E3;&#xFF0C;&#x5C06;<code>False</code>&#x8FDB;&#x884C;&#x5B8C;&#x5168; QR &#x5206;&#x89E3;&#x3002;</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; &lt;cite&gt;Q&lt;/cite&gt; &#x548C; &lt;cite&gt;R&lt;/cite&gt; &#x5F20;&#x91CF;&#x7684;&#x5143;&#x7EC4; <code>input = torch.matmul(Q, R)</code>&#x3002; &lt;cite&gt;Q&lt;/cite&gt; &#x548C; &lt;cite&gt;R&lt;/cite&gt; &#x7684;&#x5C3A;&#x5BF8;&#x5206;&#x522B;&#x4E3A;<img src="img/8ebb9d35b764041761ddb4e3436e265d.jpg" alt="">&#x548C;<img src="img/d158d0764e1a57ef7842c747aafc64dc.jpg" alt="">&#xFF0C;&#x5982;&#x679C;<code>some:</code>&#x4E3A;<code>True</code>&#x5219;&#x4E3A;<img src="img/c7ea35623a8622e24f04c76bdf3f47c5.jpg" alt="">&#xFF0C;&#x5426;&#x5219;&#x4E3A;<img src="img/632a48f07f9deba36cb4ed068721ae09.jpg" alt="">&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
&gt;&gt;&gt; r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
&gt;&gt;&gt; torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
&gt;&gt;&gt; torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
&gt;&gt;&gt; a = torch.randn(3, 4, 5)
&gt;&gt;&gt; q, r = torch.qr(a, some=False)
&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
True
&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
</code></pre><hr>
<pre><code>torch.solve(input, A, out=None) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x6B64;&#x51FD;&#x6570;&#x5C06;&#x6C42;&#x89E3;&#x8FD4;&#x56DE;&#x5230;&#x7531;<img src="img/d8a7525921c25fbc9d6af3a53d546be8.jpg" alt="">&#x8868;&#x793A;&#x7684;&#x7EBF;&#x6027;&#x65B9;&#x7A0B;&#x7EC4;&#x548C; A &#x7684; LU &#x5206;&#x89E3;&#xFF0C;&#x4EE5;&#x4FBF;&#x5C06;&#x5176;&#x4F5C;&#x4E3A;&#x547D;&#x540D;&#x5143;&lt;cite&gt;&#x89E3;&#x51B3;&#x65B9;&#x6848; LU&lt;/cite&gt; &#x3002;</p>
<p>&lt;cite&gt;LU&lt;/cite&gt; &#x5305;&#x542B; &lt;cite&gt;L&lt;/cite&gt; &#x548C; &lt;cite&gt;U&lt;/cite&gt; &#x56E0;&#x7D20;&#xFF0C;&#x7528;&#x4E8E; &lt;cite&gt;A&lt;/cite&gt; &#x7684; LU &#x5206;&#x89E3;&#x3002;</p>
<p>&lt;cite&gt;torch.solve(B&#xFF0C;A&#xFF09;&lt;/cite&gt;&#x53EF;&#x4EE5;&#x63A5;&#x53D7; 2D &#x8F93;&#x5165; &lt;cite&gt;B&#xFF0C;A&lt;/cite&gt; &#x6216;&#x4E00;&#x6279; 2D &#x77E9;&#x9635;&#x7684;&#x8F93;&#x5165;&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x662F;&#x6279;&#x6B21;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6279;&#x6B21;&#x8F93;&#x51FA;&lt;cite&gt;&#x89E3;&#x51B3;&#x65B9;&#x6848; LU&lt;/cite&gt; &#x3002;</p>
<p>Note</p>
<p>&#x4E0D;&#x7BA1;&#x539F;&#x59CB;&#x6B65;&#x5E45;&#x5982;&#x4F55;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635;&lt;cite&gt;&#x89E3;&#x51B3;&#x65B9;&#x6848;&lt;/cite&gt;&#x548C; &lt;cite&gt;LU&lt;/cite&gt; &#x90FD;&#x5C06;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#xFF0C;&#x6B65;&#x5E45;&#x7C7B;&#x4F3C; &lt;cite&gt;B.contiguous(&#xFF09;&#x3002;transpose(-1&#xFF0C;-2&#xFF09;&#x3002; stride(&#xFF09;&lt;/cite&gt;&#x548C; &lt;cite&gt;A.contiguous(&#xFF09;&#x3002;transpose(-1&#xFF0C;-2&#xFF09;.stride(&#xFF09;&lt;/cite&gt;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/8ebb9d35b764041761ddb4e3436e265d.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x77E9;&#x9635;<img src="img/041cf842180f19a622690f37ed9f70d2.jpg" alt="">&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/10ea424983a008dbf72dc86628a2813d.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x65B9;&#x9635;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>&#x8F93;&#x51FA;</strong>(<em>(</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF0C;</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#xFF09;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x53EF;&#x9009;&#x8F93;&#x51FA;&#x5143;&#x7EC4;&#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],
                      [-6.05, -3.30,  5.36, -4.44,  1.08],
                      [-0.45,  2.58, -2.70,  0.27,  9.04],
                      [8.32,  2.71,  4.35,  -7.17,  2.14],
                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()
&gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],
                      [-1.56,  4.00, -8.67,  1.75,  2.86],
                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))
tensor(1.00000e-06 *
       7.0977)

&gt;&gt;&gt; # Batched solver example
&gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4)
&gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6)
&gt;&gt;&gt; X, LU = torch.solve(B, A)
&gt;&gt;&gt; torch.dist(B, A.matmul(X))
tensor(1.00000e-06 *
   3.6386)
</code></pre><hr>
<pre><code>torch.svd(input, some=True, compute_uv=True, out=None) -&gt; (Tensor, Tensor, Tensor)&#xB6;
</code></pre><p>&#x8BE5;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;<code>(U, S, V)</code>&#xFF0C;&#x5B83;&#x662F;&#x8F93;&#x5165;&#x5B9E;&#x6570;&#x77E9;&#x9635;&#x6216;&#x4E00;&#x6279;&#x5B9E;&#x6570;&#x77E9;&#x9635;<code>input</code>&#x8FD9;&#x6837;<img src="img/2a4f9c13d7085a30a536b8751c0e3e4c.jpg" alt="">&#x7684;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#x3002;</p>
<p>&#x5982;&#x679C;<code>some</code>&#x4E3A;<code>True</code>(&#x9ED8;&#x8BA4;&#x503C;&#xFF09;&#xFF0C;&#x5219;&#x8BE5;&#x65B9;&#x6CD5;&#x8FD4;&#x56DE;&#x7B80;&#x5316;&#x540E;&#x7684;&#x5947;&#x5F02;&#x503C;&#x5206;&#x89E3;&#xFF0C;&#x5373;&#xFF0C;&#x5982;&#x679C;<code>input</code>&#x7684;&#x6700;&#x540E;&#x4E24;&#x4E2A;&#x7EF4;&#x4E3A;<code>m</code>&#x548C;<code>n</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#x77E9;&#x9635;&#x5C06;&#x4EC5;&#x5305;&#x542B;<img src="img/cbc4cab078465d304683e6b32e90ce2e.jpg" alt="">&#x6B63;&#x4EA4;&#x5217;&#x3002;</p>
<p>&#x5982;&#x679C;<code>compute_uv</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x7684; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#x77E9;&#x9635;&#x5C06;&#x5206;&#x522B;&#x4E3A;&#x5F62;&#x72B6;&#x4E3A;<img src="img/b9c3d9c79c4cd656bf4c904908b8c189.jpg" alt="">&#x548C;<img src="img/c64d7c7677227e8b59da7d7cfb2466d8.jpg" alt="">&#x7684;&#x96F6;&#x77E9;&#x9635;&#x3002; <code>some</code>&#x5728;&#x8FD9;&#x91CC;&#x5C06;&#x88AB;&#x5FFD;&#x7565;&#x3002;</p>
<p>Note</p>
<p>&#x5947;&#x5F02;&#x503C;&#x4EE5;&#x964D;&#x5E8F;&#x8FD4;&#x56DE;&#x3002; &#x5982;&#x679C;<code>input</code>&#x662F;&#x4E00;&#x6279;&#x77E9;&#x9635;&#xFF0C;&#x5219;&#x8BE5;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x77E9;&#x9635;&#x7684;&#x5947;&#x5F02;&#x503C;&#x5C06;&#x6309;&#x964D;&#x5E8F;&#x8FD4;&#x56DE;&#x3002;</p>
<p>Note</p>
<p>SVD &#x5728; CPU &#x4E0A;&#x7684;&#x5B9E;&#x73B0;&#x4F7F;&#x7528; LAPACK &#x4F8B;&#x7A0B;&lt;cite&gt;&#xFF1F;gesdd&lt;/cite&gt; (&#x5206;&#x6CBB;&#x7B97;&#x6CD5;&#xFF09;&#x4EE3;&#x66FF;&lt;cite&gt;&#xFF1F;gesvd&lt;/cite&gt; &#x6765;&#x63D0;&#x9AD8;&#x901F;&#x5EA6;&#x3002; &#x7C7B;&#x4F3C;&#x5730;&#xFF0C;GPU &#x4E0A;&#x7684; SVD &#x4E5F;&#x4F7F;&#x7528; MAGMA &#x4F8B;&#x7A0B; &lt;cite&gt;gesdd&lt;/cite&gt; &#x3002;</p>
<p>Note</p>
<p>&#x65E0;&#x8BBA;&#x539F;&#x59CB;&#x6B65;&#x5E45;&#x5982;&#x4F55;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635; &lt;cite&gt;U&lt;/cite&gt; &#x90FD;&#x5C06;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x6B65;&#x5E45;&#x4E3A;<code>U.contiguous().transpose(-2, -1).stride()</code></p>
<p>Note</p>
<p>&#x5411;&#x540E;&#x901A;&#x8FC7; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#x8F93;&#x51FA;&#x65F6;&#xFF0C;&#x9700;&#x8981;&#x683C;&#x5916;&#x5C0F;&#x5FC3;&#x3002; &#x4EC5;&#x5F53;<code>input</code>&#x5177;&#x6709;&#x6240;&#x6709;&#x4E0D;&#x540C;&#x7684;&#x5947;&#x5F02;&#x503C;&#x7684;&#x5B8C;&#x6574;&#x7B49;&#x7EA7;&#x65F6;&#xFF0C;&#x6B64;&#x7C7B;&#x64CD;&#x4F5C;&#x624D;&#x771F;&#x6B63;&#x7A33;&#x5B9A;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x7531;&#x4E8E;&#x672A;&#x6B63;&#x786E;&#x5B9A;&#x4E49;&#x6E10;&#x53D8;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x51FA;&#x73B0;<code>NaN</code>&#x3002; &#x53E6;&#x5916;&#xFF0C;&#x8BF7;&#x6CE8;&#x610F;&#xFF0C;&#x5373;&#x4F7F;&#x539F;&#x59CB;&#x540E;&#x9000;&#x4EC5;&#x51FA;&#x73B0;&#x5728; &lt;cite&gt;S&lt;/cite&gt; &#x4E0A;&#xFF0C;&#x4E24;&#x6B21;&#x540E;&#x9000;&#x901A;&#x5E38;&#x4E5F;&#x4F1A;&#x901A;&#x8FC7; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#x8FDB;&#x884C;&#x989D;&#x5916;&#x7684;&#x540E;&#x9000;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>some</code> = <code>False</code>&#x65F6;&#xFF0C;<code>U[..., :, min(m, n):]</code>&#x548C;<code>V[..., :, min(m, n):]</code>&#x4E0A;&#x7684;&#x68AF;&#x5EA6;&#x5C06;&#x5411;&#x540E;&#x5FFD;&#x7565;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD9;&#x4E9B;&#x5411;&#x91CF;&#x53EF;&#x4EE5;&#x662F;&#x5B50;&#x7A7A;&#x95F4;&#x7684;&#x4EFB;&#x610F;&#x57FA;&#x3002;</p>
<p>Note</p>
<p>&#x5F53;<code>compute_uv</code> = <code>False</code>&#x65F6;&#xFF0C;&#x7531;&#x4E8E;&#x5411;&#x540E;&#x64CD;&#x4F5C;&#x9700;&#x8981;&#x6765;&#x81EA;&#x6B63;&#x5411;&#x7684; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#xFF0C;&#x56E0;&#x6B64;&#x65E0;&#x6CD5;&#x6267;&#x884C;&#x53CD;&#x5411;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/6217ff594f4af540b27d0cc551ab742c.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;*&lt;/cite&gt; &#x662F;&#x96F6;&#x4E2A;&#x6216;&#x591A;&#x4E2A;&#x7531;<img src="img/5324b7a0d3b2c1767998bc499b6dfb11.jpg" alt="">&#x7EC4;&#x6210;&#x7684;&#x6279;&#x91CF; &#x77E9;&#x9635;&#x3002;</p>
</li>
<li><p><strong>&#x4E00;&#x4E9B;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x8FD4;&#x56DE;&#x7684; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt;</p>
</li>
<li><p><strong>compute_uv</strong>  (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x9009;&#x62E9;&#x662F;&#x5426;&#x8BA1;&#x7B97; &lt;cite&gt;U&lt;/cite&gt; &#x548C; &lt;cite&gt;V&lt;/cite&gt; &#x6216;&#x4E0D;</p>
</li>
<li><p><strong>out</strong> (<em>&#x5143;&#x7EC4;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5F20;&#x91CF;&#x7684;&#x8F93;&#x51FA;&#x5143;&#x7EC4;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
&gt;&gt;&gt; s
tensor([2.3289, 2.0315, 0.7806])
&gt;&gt;&gt; v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, v = torch.svd(a_big)
&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
tensor(2.6503e-06)
</code></pre><hr>
<pre><code>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x6B64;&#x51FD;&#x6570;&#x8FD4;&#x56DE;&#x5B9E;&#x6570;&#x5BF9;&#x79F0;&#x77E9;&#x9635;<code>input</code>&#x6216;&#x4E00;&#x6279;&#x5B9E;&#x6570;&#x5BF9;&#x79F0;&#x77E9;&#x9635;&#x7684;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF0C;&#x7531;&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;(&#x7279;&#x5F81;&#x503C;&#xFF0C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF09;&#x8868;&#x793A;&#x3002;</p>
<p>&#x6B64;&#x51FD;&#x6570;&#x8BA1;&#x7B97;<code>input</code>&#x7684;&#x6240;&#x6709;&#x7279;&#x5F81;&#x503C;(&#x548C;&#x5411;&#x91CF;&#xFF09;&#xFF0C;&#x4F7F;&#x5F97;<img src="img/cb16e8778bbe7f58582093ac9fd7dd36.jpg" alt="">&#x3002;</p>
<p>&#x5E03;&#x5C14;&#x53C2;&#x6570;<code>eigenvectors</code>&#x5B9A;&#x4E49;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x548C;&#x7279;&#x5F81;&#x503C;&#x6216;&#x4EC5;&#x7279;&#x5F81;&#x503C;&#x7684;&#x8BA1;&#x7B97;&#x3002;</p>
<p>&#x5982;&#x679C;&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x4EC5;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x503C;&#x3002; &#x5982;&#x679C;&#x4E3A;<code>True</code>&#xFF0C;&#x5219;&#x540C;&#x65F6;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x503C;&#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</p>
<p>&#x7531;&#x4E8E;&#x5047;&#x5B9A;&#x8F93;&#x5165;&#x77E9;&#x9635;<code>input</code>&#x662F;&#x5BF9;&#x79F0;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#x4EC5;&#x4F7F;&#x7528;&#x4E0A;&#x4E09;&#x89D2;&#x90E8;&#x5206;&#x3002;</p>
<p>&#x5982;&#x679C;<code>upper</code>&#x4E3A;<code>False</code>&#xFF0C;&#x5219;&#x4F7F;&#x7528;&#x4E0B;&#x90E8;&#x4E09;&#x89D2;&#x5F62;&#x90E8;&#x5206;&#x3002;</p>
<p>Note</p>
<p>&#x7279;&#x5F81;&#x503C;&#x4EE5;&#x5347;&#x5E8F;&#x8FD4;&#x56DE;&#x3002; &#x5982;&#x679C;<code>input</code>&#x662F;&#x4E00;&#x6279;&#x77E9;&#x9635;&#xFF0C;&#x5219;&#x8BE5;&#x6279;&#x4E2D;&#x6BCF;&#x4E2A;&#x77E9;&#x9635;&#x7684;&#x7279;&#x5F81;&#x503C;&#x5C06;&#x4EE5;&#x5347;&#x5E8F;&#x8FD4;&#x56DE;&#x3002;</p>
<p>Note</p>
<p>&#x65E0;&#x8BBA;&#x539F;&#x59CB;&#x6B65;&#x5E45;&#x5982;&#x4F55;&#xFF0C;&#x8FD4;&#x56DE;&#x7684;&#x77E9;&#x9635; &lt;cite&gt;V&lt;/cite&gt; &#x90FD;&#x5C06;&#x8F6C;&#x7F6E;&#xFF0C;&#x5373;&#x4F7F;&#x7528;&#x6B65;&#x5E45; &lt;cite&gt;V.contiguous(&#xFF09;&#x3002;transpose(-1&#xFF0C;-2&#xFF09;.stride(&#xFF09;&lt;/cite&gt;&#x3002;</p>
<p>Note</p>
<p>&#x5411;&#x540E;&#x901A;&#x8FC7;&#x8F93;&#x51FA;&#x65F6;&#xFF0C;&#x9700;&#x8981;&#x683C;&#x5916;&#x5C0F;&#x5FC3;&#x3002; &#x53EA;&#x6709;&#x5F53;&#x6240;&#x6709;&#x7279;&#x5F81;&#x503C;&#x90FD;&#x4E0D;&#x540C;&#x65F6;&#xFF0C;&#x8FD9;&#x79CD;&#x64CD;&#x4F5C;&#x624D;&#x771F;&#x6B63;&#x7A33;&#x5B9A;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x51FA;&#x73B0;<code>NaN</code>&#xFF0C;&#x56E0;&#x4E3A;&#x672A;&#x6B63;&#x786E;&#x5B9A;&#x4E49;&#x6E10;&#x53D8;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/aa6a866e7977a9ee67a53687003d3821.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#xFF0C;&#x5176;&#x4E2D; &lt;cite&gt;*&lt;/cite&gt; &#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x591A;&#x7531;&#x5BF9;&#x79F0;&#x77E9;&#x9635;&#x7EC4;&#x6210;&#x7684;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
<li><p><strong>&#x7279;&#x5F81;&#x5411;&#x91CF;</strong>(<em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x5426;&#x5FC5;&#x987B;&#x8BA1;&#x7B97;&#x7279;&#x5F81;&#x5411;&#x91CF;</p>
</li>
<li><p><strong>&#x4E0A;&#x90E8;</strong>(<em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x63A7;&#x5236;&#x662F;&#x8003;&#x8651;&#x4E0A;&#x4E09;&#x89D2;&#x533A;&#x57DF;&#x8FD8;&#x662F;&#x4E0B;&#x4E09;&#x89D2;&#x533A;&#x57DF;</p>
</li>
<li><p><strong>out</strong> (<em>tuple__,</em> <em>optional</em>) &#x2013; the output tuple of (Tensor, Tensor)</p>
</li>
</ul>
<p>Returns</p>
<p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<ul>
<li><strong>&#x7279;&#x5F81;&#x503C;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x5F62;&#x72B6;<img src="img/74dd7138867888df79f198ead03a8f07.jpg" alt="">&#x3002; &#x7279;&#x5F81;&#x503C;&#x6309;&#x5347;&#x5E8F;&#x6392;&#x5217;&#x3002;</li>
</ul>
<ul>
<li><strong>&#x7279;&#x5F81;&#x5411;&#x91CF;</strong>(<em>tensor</em>&#xFF09;&#xFF1A;&#x5F62;&#x72B6;<img src="img/10ea424983a008dbf72dc86628a2813d.jpg" alt="">&#x3002; &#x5982;&#x679C;<code>eigenvectors=False</code>&#xFF0C;&#x5219;&#x4E3A;&#x5F20;&#x91CF;&#x4E3A;&#x7A7A;&#x3002; &#x5426;&#x5219;&#xFF0C;&#x8BE5;&#x5F20;&#x91CF;&#x5305;&#x542B;<code>input</code>&#x7684;&#x6B63;&#x4EA4;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x3002;</li>
</ul>
</blockquote>
<p>Return type</p>
<p>(<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; a = torch.randn(5, 5)
&gt;&gt;&gt; a = a + a.t()  # To make a symmetric
&gt;&gt;&gt; a
tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],
        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],
        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],
        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],
        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])
&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)
&gt;&gt;&gt; e
tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])
&gt;&gt;&gt; v
tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],
        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],
        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],
        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],
        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])
&gt;&gt;&gt; a_big = torch.randn(5, 2, 2)
&gt;&gt;&gt; a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric
&gt;&gt;&gt; e, v = a_big.symeig(eigenvectors=True)
&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True
</code></pre><hr>
<pre><code>torch.trapz()&#xB6;
</code></pre><hr>
<pre><code>torch.trapz(y, x, *, dim=-1) &#x2192; Tensor
</code></pre><p>&#x4F7F;&#x7528;&#x68AF;&#x5F62;&#x6CD5;&#x5219;&#x4F30;&#x8BA1;&lt;cite&gt;&#x6697;&lt;/cite&gt;&#x7684;<img src="img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" alt="">&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>y</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x79EF;&#x5206;&#x51FD;&#x6570;&#x7684;&#x503C;</p>
</li>
<li><p><strong>x</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x51FD;&#x6570; &lt;cite&gt;y&lt;/cite&gt; &#x7684;&#x91C7;&#x6837;&#x70B9;&#x3002; &#x5982;&#x679C; &lt;cite&gt;x&lt;/cite&gt; &#x4E0D;&#x6309;&#x5347;&#x5E8F;&#x6392;&#x5217;&#xFF0C;&#x5219;&#x5176;&#x51CF;&#x5C0F;&#x7684;&#x65F6;&#x95F4;&#x95F4;&#x9694;&#x5C06;&#x5BF9;&#x4F30;&#x8BA1;&#x7684;&#x79EF;&#x5206;&#x4EA7;&#x751F;&#x8D1F;&#x9762;&#x5F71;&#x54CD;(&#x5373;&#x9075;&#x5FAA;&#x60EF;&#x4F8B;<img src="img/7368567bca3b8caffef0c7639ae1ebd5.jpg" alt="">&#xFF09;&#x3002;</p>
</li>
<li><p><strong>&#x6697;&#x6DE1;&#x7684;</strong> (<em>python&#xFF1A;int</em> )&#x2013;&#x96C6;&#x6210;&#x6240;&#x6CBF;&#x7684;&#x7EF4;&#x5EA6;&#x3002; &#x9ED8;&#x8BA4;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x5C3A;&#x5BF8;&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;&#x4E0E;&#x8F93;&#x5165;&#x5F62;&#x72B6;&#x76F8;&#x540C;&#x7684;&#x5F20;&#x91CF;&#xFF0C;&#x9664;&#x4E86;&#x5220;&#x9664;&#x4E86;&lt;cite&gt;&#x6697;&#x6DE1;&#x7684;&lt;/cite&gt;&#x3002; &#x8FD4;&#x56DE;&#x7684;&#x5F20;&#x91CF;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x4EE3;&#x8868;&#x6CBF;&#x7740;&lt;cite&gt;&#x6697;&#x6DE1;&lt;/cite&gt;&#x7684;&#x4F30;&#x8BA1;&#x79EF;&#x5206;<img src="img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" alt="">&#x3002;</p>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; y = torch.randn((2, 3))
&gt;&gt;&gt; y
tensor([[-2.1156,  0.6857, -0.2700],
        [-1.2145,  0.5540,  2.0431]])
&gt;&gt;&gt; x = torch.tensor([[1, 3, 4], [1, 2, 3]])
&gt;&gt;&gt; torch.trapz(y, x)
tensor([-1.2220,  0.9683])
</code></pre><hr>
<pre><code>torch.trapz(y, *, dx=1, dim=-1) &#x2192; Tensor
</code></pre><p>&#x5982;&#x4E0A;&#x6240;&#x8FF0;&#xFF0C;&#x4F46;&#x662F;&#x91C7;&#x6837;&#x70B9;&#x4EE5; &lt;cite&gt;dx&lt;/cite&gt; &#x7684;&#x8DDD;&#x79BB;&#x5747;&#x5300;&#x95F4;&#x9694;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>y</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#x2013; The values of the function to integrate</p>
</li>
<li><p><strong>dx</strong>  (<em>python&#xFF1A;float</em> )&#x2013;&#x91C7;&#x6837; &lt;cite&gt;y&lt;/cite&gt; &#x7684;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#x3002;</p>
</li>
<li><p><strong>dim</strong> (<em>python:int</em>) &#x2013; The dimension along which to integrate. By default, use the last dimension.</p>
</li>
</ul>
<p>Returns</p>
<p>A Tensor with the same shape as the input, except with &lt;cite&gt;dim&lt;/cite&gt; removed. Each element of the returned tensor represents the estimated integral <img src="img/610dd7b4997e45b33ba79e39c75e1b9d.jpg" alt=""> along &lt;cite&gt;dim&lt;/cite&gt;.</p>
<hr>
<pre><code>torch.triangular_solve(input, A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)&#xB6;
</code></pre><p>&#x7528;&#x4E09;&#x89D2;&#x7CFB;&#x6570;&#x77E9;&#x9635;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x548C;&#x591A;&#x4E2A;&#x53F3;&#x4FA7;<img src="img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" alt="">&#x6C42;&#x89E3;&#x65B9;&#x7A0B;&#x7EC4;&#x3002;</p>
<p>&#x7279;&#x522B;&#x662F;&#xFF0C;&#x6C42;&#x89E3;<img src="img/998c4e0ada5e41efb8c632f644ba86f1.jpg" alt="">&#x5E76;&#x5047;&#x5B9A;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x4E3A;&#x5E26;&#x6709;&#x9ED8;&#x8BA4;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x3002;</p>
<p>&lt;cite&gt;torch.triangular_solve(b&#xFF0C;A&#xFF09;&lt;/cite&gt;&#x53EF;&#x4EE5;&#x63A5;&#x53D7; 2D &#x8F93;&#x5165; &lt;cite&gt;b&#xFF0C;A&lt;/cite&gt; &#x6216;&#x4E00;&#x6279; 2D &#x77E9;&#x9635;&#x7684;&#x8F93;&#x5165;&#x3002; &#x5982;&#x679C;&#x8F93;&#x5165;&#x4E3A;&#x6279;&#x6B21;&#xFF0C;&#x5219;&#x8FD4;&#x56DE;&#x6210;&#x6279;&#x8F93;&#x51FA; &lt;cite&gt;X&lt;/cite&gt;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x8F93;&#x5165;</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5C3A;&#x5BF8;&#x4E3A;<img src="img/8ebb9d35b764041761ddb4e3436e265d.jpg" alt="">&#x7684;&#x591A;&#x4E2A;&#x53F3;&#x4FA7;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x662F;&#x96F6;&#x4E2A;&#x4EE5;&#x4E0A;&#x7684;&#x6279;&#x6B21;&#x5C3A;&#x5BF8;(<img src="img/23a2cc697e4e1a8adeb60eb42ab51a6e.jpg" alt="">&#xFF09;</p>
</li>
<li><p><strong>A</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a>)&#x2013;&#x5927;&#x5C0F;&#x4E3A;<img src="img/10ea424983a008dbf72dc86628a2813d.jpg" alt="">&#x7684;&#x8F93;&#x5165;&#x4E09;&#x89D2;&#x7CFB;&#x6570;&#x77E9;&#x9635;&#xFF0C;&#x5176;&#x4E2D;<img src="img/dcef98688866c0d5a21137cf53bf228d.jpg" alt="">&#x4E3A;&#x96F6;&#x6216;&#x66F4;&#x5927;&#x7684;&#x6279;&#x5904;&#x7406;&#x5C3A;&#x5BF8;</p>
</li>
<li><p><strong>&#x4E0A;</strong> (<em>bool</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x662F;&#x6C42;&#x89E3;&#x65B9;&#x7A0B;&#x7684;&#x4E0A;&#x4E09;&#x89D2;&#x7CFB;&#x7EDF;(&#x9ED8;&#x8BA4;&#xFF09;&#x8FD8;&#x662F;&#x4E0B;&#x4E09;&#x89D2;&#x7CFB;&#x7EDF; &#x65B9;&#x7A0B;&#x7EC4;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>True</code>&#x3002;</p>
</li>
<li><p><strong>&#x6362;&#x4F4D;</strong>(<em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013;&#x5728;&#x5C06;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x53D1;&#x9001;&#x5230;&#x6C42;&#x89E3;&#x5668;&#x4E4B;&#x524D;&#x662F;&#x5426;&#x5E94;&#x8BE5;&#x5BF9;&#x5176;&#x8FDB;&#x884C;&#x6362;&#x4F4D;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
<li><p><strong>&#x5355;&#x8FB9;&#x5F62;</strong>(<em>&#x5E03;&#x5C14;</em> <em>&#xFF0C;</em> <em>&#x53EF;&#x9009;</em>&#xFF09;&#x2013; <img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x662F;&#x5426;&#x4E3A;&#x5355;&#x4F4D;&#x4E09;&#x89D2;&#x5F62;&#x3002; &#x5982;&#x679C;&#x4E3A; True&#xFF0C;&#x5219;&#x5047;&#x5B9A;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x7684;&#x5BF9;&#x89D2;&#x5143;&#x7D20;&#x4E3A; 1&#xFF0C;&#x5E76;&#x4E14;&#x672A;&#x4ECE;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x5F15;&#x7528;&#x3002; &#x9ED8;&#x8BA4;&#x503C;&#xFF1A;<code>False</code>&#x3002;</p>
</li>
</ul>
<p>Returns</p>
<p>&#x4E00;&#x4E2A;&#x547D;&#x540D;&#x5143;&#x7EC4;&lt;cite&gt;(&#x89E3;&#x51B3;&#x65B9;&#x6848;&#xFF0C;cloned_coefficient&#xFF09;&lt;/cite&gt;&#x5176;&#x4E2D; &lt;cite&gt;cloned_coefficient&lt;/cite&gt; &#x662F;<img src="img/8cab287af6acaf0838d1d67381a3716d.jpg" alt="">&#x7684;&#x514B;&#x9686;&#xFF0C;&#x800C;&lt;cite&gt;&#x89E3;&#x51B3;&#x65B9;&#x6848;&lt;/cite&gt;&#x662F;<img src="img/1dc567019b272fda0c5051c472dac2b7.jpg" alt="">&#x5230;<img src="img/998c4e0ada5e41efb8c632f644ba86f1.jpg" alt="">&#x7684;&#x89E3;&#x51B3;&#x65B9;&#x6848;(&#x6216;&#x5176;&#x4ED6;&#x53D8;&#x4F53;&#xFF09; &#x7684;&#x65B9;&#x7A0B;&#x7EC4;&#xFF0C;&#x5177;&#x4F53;&#x53D6;&#x51B3;&#x4E8E;&#x5173;&#x952E;&#x5B57;&#x53C2;&#x6570;&#x3002;&#xFF09;</p>
<p>Examples:</p>
<pre><code>&gt;&gt;&gt; A = torch.randn(2, 2).triu()
&gt;&gt;&gt; A
tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]])
&gt;&gt;&gt; b = torch.randn(2, 3)
&gt;&gt;&gt; b
tensor([[-0.0210,  2.3513, -1.5492],
        [ 1.5429,  0.7403, -1.0243]])
&gt;&gt;&gt; torch.triangular_solve(b, A)
torch.return_types.triangular_solve(
solution=tensor([[ 1.7841,  2.9046, -2.5405],
        [ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
        [ 0.0000,  0.7986]]))
</code></pre><h2 id="&#x5B9E;&#x7528;&#x5DE5;&#x5177;">&#x5B9E;&#x7528;&#x5DE5;&#x5177;</h2>
<hr>
<pre><code>torch.compiled_with_cxx11_abi()&#xB6;
</code></pre><p>&#x8FD4;&#x56DE; PyTorch &#x662F;&#x5426;&#x4F7F;&#x7528; _GLIBCXX_USE_CXX11_ABI = 1 &#x6784;&#x5EFA;</p>
<hr>
<pre><code>torch.result_type(tensor1, tensor2) &#x2192; dtype&#xB6;
</code></pre><p>&#x8FD4;&#x56DE; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x8FD9;&#x662F;&#x5BF9;&#x63D0;&#x4F9B;&#x7684;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6267;&#x884C;&#x7B97;&#x672F;&#x8FD0;&#x7B97;&#x5F97;&#x51FA;&#x7684;&#x3002; &#x6709;&#x5173;&#x7C7B;&#x578B;&#x5347;&#x7EA7;&#x903B;&#x8F91;&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x7C7B;&#x578B;&#x5347;&#x7EA7;<a href="tensor_attributes.html#type-promotion-doc">&#x6587;&#x6863;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>&#x5F20;&#x91CF; 1</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>&#x6570;&#x5B57;</em>&#xFF09;&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6216;&#x6570;&#x5B57;</p>
</li>
<li><p><strong>&#x5F20;&#x91CF; 2</strong>  (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>tensor</em></a> <em>&#x6216;</em> <em>&#x6570;&#x5B57;</em>&#xFF09;&#x2013;&#x8F93;&#x5165;&#x5F20;&#x91CF;&#x6216;&#x6570;&#x5B57;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)
torch.float32
&gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
torch.uint8
</code></pre><hr>
<pre><code>torch.can_cast(from, to) &#x2192; bool&#xB6;
</code></pre><p>&#x786E;&#x5B9A;&#x5728;&#x7C7B;&#x578B;&#x63D0;&#x5347;<a href="tensor_attributes.html#type-promotion-doc">&#x6587;&#x6863;</a>&#x4E2D;&#x63CF;&#x8FF0;&#x7684; PyTorch &#x8F6C;&#x6362;&#x89C4;&#x5219;&#x4E0B;&#x662F;&#x5426;&#x5141;&#x8BB8;&#x7C7B;&#x578B;&#x8F6C;&#x6362;&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p>&#x4E2D;&#x7684;<strong> (<em>dpython&#xFF1A;type</em> )&#x2013;&#x539F;&#x59CB;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</strong></p>
</li>
<li><p><strong>&#x5230;</strong> (<em>dpython&#xFF1A;type</em> )&#x2013;&#x76EE;&#x6807; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#x3002;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.can_cast(torch.double, torch.float)
True
&gt;&gt;&gt; torch.can_cast(torch.float, torch.int)
False
</code></pre><hr>
<pre><code>torch.promote_types(type1, type2) &#x2192; dtype&#xB6;
</code></pre><p>&#x8FD4;&#x56DE;&#x5C3A;&#x5BF8;&#x548C;&#x6807;&#x91CF;&#x79CD;&#x7C7B;&#x6700;&#x5C0F;&#x7684; <a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a> &#xFF0C;&#x5176;&#x5927;&#x5C0F;&#x4E0D;&#x5C0F;&#x4E8E; &lt;cite&gt;type1&lt;/cite&gt; &#x6216; &lt;cite&gt;type2&lt;/cite&gt; &#x3002; &#x6709;&#x5173;&#x7C7B;&#x578B;&#x5347;&#x7EA7;&#x903B;&#x8F91;&#x7684;&#x66F4;&#x591A;&#x4FE1;&#x606F;&#xFF0C;&#x8BF7;&#x53C2;&#x89C1;&#x7C7B;&#x578B;&#x5347;&#x7EA7;<a href="tensor_attributes.html#type-promotion-doc">&#x6587;&#x6863;</a>&#x3002;</p>
<p>Parameters</p>
<ul>
<li><p><strong>type1</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)&#x2013;</p>
</li>
<li><p><strong>type2</strong>  (<a href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>)&#x2013;</p>
</li>
</ul>
<p>Example:</p>
<pre><code>&gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32))
torch.float32
&gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long)
torch.long
</code></pre><p><hr></p>
<div align="center">
    <p><a href="http://www.apachecn.org" target="_blank"><font face="KaiTi" size="6" color="red">&#x6211;&#x4EEC;&#x4E00;&#x76F4;&#x5728;&#x52AA;&#x529B;</font></a></p>
    <p><a href="https://github.com/apachecn/pytorch-doc-zh/" target="_blank">apachecn/pytorch-doc-zh</a></p>
    <p><iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=watch&amp;count=true&amp;v=2" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=star&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <iframe align="middle" src="https://ghbtns.com/github-btn.html?user=apachecn&amp;repo=pytorch-doc-zh&amp;type=fork&amp;count=true" frameborder="0" scrolling="0" width="100px" height="25px"></iframe>
    <a target="_blank" href="shang.qq.com/wpa/qunwpa"><img border="0" src="http://data.apachecn.org/img/logo/ApacheCN-group.png" alt="ML | ApacheCN" title="ML | ApacheCN"></a></p>
</div>
 <div style="text-align:center;margin:0 0 10.5px;">
     <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-3565452474788507" data-ad-slot="2543897000">
     </ins>
     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?38525fdac4b5d4403900b943d4e7dd91";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-102475051-10"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-102475051-10');
    </script>
</div>

<p><meta name="google-site-verification" content="pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo"></p>
<p><iframe src="https://www.bilibili.com/read/cv2710377" style="display:none"></iframe>
<img src="http://t.cn/AiCoDHwb" hidden="hidden"></p>
<div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
        clientID: '2e62dee5b9896e2eede6',
        clientSecret: 'ca6819a54656af0d87960af15315320f8a628a53',
        repo: 'pytorch-doc-zh',
        owner: 'apachecn',
        admin: ['jiangzhonglian', 'wizardforcel'],
        id: md5(location.pathname),
        distractionFreeMode: false
        })
        gitalk.render('gitalk-container')
    </script>
</div>

<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2020-03-04 13:35:32
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                
                <a href="75.html" class="navigation navigation-next navigation-unique" aria-label="Next page: torch.nn">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"torch","level":"1.17.1","depth":2,"next":{"title":"torch.nn","level":"1.17.2","depth":2,"path":"75.md","ref":"75.md","articles":[]},"previous":{"title":"Python API","level":"1.17","depth":1,"ref":"","articles":[{"title":"torch","level":"1.17.1","depth":2,"path":"74.md","ref":"74.md","articles":[]},{"title":"torch.nn","level":"1.17.2","depth":2,"path":"75.md","ref":"75.md","articles":[]},{"title":"torch功能","level":"1.17.3","depth":2,"path":"76.md","ref":"76.md","articles":[]},{"title":"torch张量","level":"1.17.4","depth":2,"path":"77.md","ref":"77.md","articles":[]},{"title":"张量属性","level":"1.17.5","depth":2,"path":"78.md","ref":"78.md","articles":[]},{"title":"自动差分包-Torch.Autograd","level":"1.17.6","depth":2,"path":"79.md","ref":"79.md","articles":[]},{"title":"torch.cuda","level":"1.17.7","depth":2,"path":"80.md","ref":"80.md","articles":[]},{"title":"分布式通讯包-Torch.Distributed","level":"1.17.8","depth":2,"path":"81.md","ref":"81.md","articles":[]},{"title":"概率分布-torch分布","level":"1.17.9","depth":2,"path":"82.md","ref":"82.md","articles":[]},{"title":"torch.hub","level":"1.17.10","depth":2,"path":"83.md","ref":"83.md","articles":[]},{"title":"torch脚本","level":"1.17.11","depth":2,"path":"84.md","ref":"84.md","articles":[]},{"title":"torch.nn.init","level":"1.17.12","depth":2,"path":"85.md","ref":"85.md","articles":[]},{"title":"torch.onnx","level":"1.17.13","depth":2,"path":"86.md","ref":"86.md","articles":[]},{"title":"torch.optim","level":"1.17.14","depth":2,"path":"87.md","ref":"87.md","articles":[]},{"title":"量化","level":"1.17.15","depth":2,"path":"88.md","ref":"88.md","articles":[]},{"title":"分布式 RPC 框架","level":"1.17.16","depth":2,"path":"89.md","ref":"89.md","articles":[]},{"title":"torch随机","level":"1.17.17","depth":2,"path":"90.md","ref":"90.md","articles":[]},{"title":"torch稀疏","level":"1.17.18","depth":2,"path":"91.md","ref":"91.md","articles":[]},{"title":"torch存储","level":"1.17.19","depth":2,"path":"92.md","ref":"92.md","articles":[]},{"title":"torch.utils.bottleneck","level":"1.17.20","depth":2,"path":"93.md","ref":"93.md","articles":[]},{"title":"torch.utils.checkpoint","level":"1.17.21","depth":2,"path":"94.md","ref":"94.md","articles":[]},{"title":"torch.utils.cpp_extension","level":"1.17.22","depth":2,"path":"95.md","ref":"95.md","articles":[]},{"title":"torch.utils.data","level":"1.17.23","depth":2,"path":"96.md","ref":"96.md","articles":[]},{"title":"torch.utils.dlpack","level":"1.17.24","depth":2,"path":"97.md","ref":"97.md","articles":[]},{"title":"torch.utils.model_zoo","level":"1.17.25","depth":2,"path":"98.md","ref":"98.md","articles":[]},{"title":"torch.utils.tensorboard","level":"1.17.26","depth":2,"path":"99.md","ref":"99.md","articles":[]},{"title":"类型信息","level":"1.17.27","depth":2,"path":"100.md","ref":"100.md","articles":[]},{"title":"命名张量","level":"1.17.28","depth":2,"path":"101.md","ref":"101.md","articles":[]},{"title":"命名为 Tensors 操作员范围","level":"1.17.29","depth":2,"path":"102.md","ref":"102.md","articles":[]},{"title":"糟糕！","level":"1.17.30","depth":2,"path":"103.md","ref":"103.md","articles":[]}]},"dir":"ltr"},"config":{"plugins":["github","github-buttons","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","tbfed-pagefooter","sitemap","advanced-emoji","expandable-chapters","splitter","search-pro"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn/pytorch-doc-zh"},"splitter":{},"search-pro":{},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"sitemap":{"hostname":"http://pytorch.apachecn.org"},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"advanced-emoji":{"embedEmojis":false},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.4"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"},"expandable-chapters":{}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["donate","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","ga","baidu","sitemap","tbfed-pagefooter","advanced-emoji","sectionx","page-treeview","simple-page-toc","ancre-navigation","theme-apachecn@git+https://github.com/apachecn/theme-apachecn#HEAD","pagefooter-apachecn@git+https://github.com/apachecn/gitbook-plugin-pagefooter-apachecn#HEAD"],"my_pluginsConfig":{"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"github-buttons":{"buttons":[{"user":"apachecn","repo":"pytorch-doc-zh","type":"star","count":true,"size":"small"},{"user":"apachecn","width":"160","type":"follow","count":true,"size":"small"}]},"ga":{"token":"UA-102475051-10"},"baidu":{"token":"75439e2cbd22bdd813226000e9dcc12f"},"pagefooter-apachecn":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"74.md","mtime":"2020-03-04T05:35:32.395Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-03-30T18:46:55.556Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

